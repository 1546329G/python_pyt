{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsl0NCqtwHMIgAUN8swAfk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1546329G/python_pyt/blob/main/Copia_de_Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Estructura del archivo CSV\n",
        "El archivo CSV tendrá las siguientes columnas:\n",
        "•\tAño: Desde 2001 hasta 2024.\n",
        "•\tCódigo_Taxista: Identificador del taxista (ejemplo: AS-001).\n",
        "•\tNúmero_Carrera: Número de carrera.\n",
        "•\tFecha_Carrera: Fecha de la carrera.\n",
        "•\tDuración_Carrera: Duración en minutos.\n"
      ],
      "metadata": {
        "id": "K77knYXUaWJb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuUFPTsDaQdN",
        "outputId": "23dac04a-e381-4ce7-fd71-9ffcc5e9fd25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Codigo_Taxista  Numero_Carrera Fecha_Carrera  Duracion_Carrera\n",
            "0         AS-001               1    2010-09-19                33\n",
            "1         AS-001               2    2012-08-23                35\n",
            "2         AS-001               3    2013-11-11                58\n",
            "3         AS-001               4    2021-02-12                29\n",
            "4         AS-001               5    2005-01-08                 5\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Generación de datos\n",
        "def generar_datos_radio_taxi():\n",
        "    data = []\n",
        "    codigos_taxistas = [f'AS-{str(i).zfill(3)}' for i in range(1, 51)]  # 50 taxistas\n",
        "    inicio_fecha = datetime(2001, 1, 1)\n",
        "\n",
        "    # Generamos carreras aleatorias para cada taxista\n",
        "    for taxista in codigos_taxistas:\n",
        "        num_carreras = random.randint(500, 1500)  # Cada taxista tiene entre 500 y 1500 carreras\n",
        "        for i in range(num_carreras):\n",
        "            fecha_carrera = inicio_fecha + timedelta(days=random.randint(0, (datetime(2024, 12, 31) - inicio_fecha).days))\n",
        "            duracion_carrera = random.randint(5, 60)  # Duración de la carrera entre 5 y 60 minutos\n",
        "            data.append([taxista, i + 1, fecha_carrera.strftime('%Y-%m-%d'), duracion_carrera])\n",
        "\n",
        "    # Crear DataFrame y guardarlo como CSV\n",
        "    df = pd.DataFrame(data, columns=['Codigo_Taxista', 'Numero_Carrera', 'Fecha_Carrera', 'Duracion_Carrera'])\n",
        "    df.to_csv('radio_taxi_carreras.csv', index=False)\n",
        "    return df\n",
        "\n",
        "# Generar y visualizar los primeros datos\n",
        "df = generar_datos_radio_taxi()\n",
        "print(df.head())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#Modelo de Deep Learning para identificar los taxistas con más carreras"
      ],
      "metadata": {
        "id": "-kdnASsAbeiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "\n",
        "# Contar el número de carreras por taxista\n",
        "carreras_por_taxista = df.groupby('Taxista_Label').size().reset_index(name='Total_Carreras')\n",
        "\n",
        "# Obtener los 3 taxistas con más carreras\n",
        "top_3_taxistas = carreras_por_taxista.nlargest(3, 'Total_Carreras')\n",
        "\n",
        "# Datos de entrada (Taxista_Label) y de salida (Total_Carreras)\n",
        "X = carreras_por_taxista[['Taxista_Label']].values\n",
        "y = carreras_por_taxista['Total_Carreras'].values\n",
        "\n",
        "# Dividimos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "tf.keras.layers.Dense(32, activation='relu'),\n",
        "tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100,\n",
        "validation_data=(X_test, y_test))\n",
        "\n",
        "\n",
        "# Predecir las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "\n",
        "# Mostrar los 3 taxistas con más carreras\n",
        "print(\"Top 3 taxistas con más carreras:\")\n",
        "print(top_3_taxistas)\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "309awvokbbeO",
        "outputId": "01aa4dcf-6edb-4c6a-ac7b-73994811d226"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 202ms/step - loss: 1150197.5000 - mae: 1028.0199 - val_loss: 1044296.5000 - val_mae: 975.6819\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1146080.8750 - mae: 1025.3450 - val_loss: 1042419.8125 - val_mae: 974.7212\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1140508.8750 - mae: 1025.6804 - val_loss: 1040553.6250 - val_mae: 973.7645\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1126094.5000 - mae: 1016.6066 - val_loss: 1038693.8750 - val_mae: 972.8104\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1130335.1250 - mae: 1021.5234 - val_loss: 1036932.0000 - val_mae: 971.9053\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1105179.0000 - mae: 1007.2900 - val_loss: 1035371.1250 - val_mae: 971.1027\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1116258.1250 - mae: 1013.9395 - val_loss: 1033794.3125 - val_mae: 970.2911\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1093584.5000 - mae: 1001.8448 - val_loss: 1032212.6250 - val_mae: 969.4764\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1101986.7500 - mae: 1007.5544 - val_loss: 1030725.1250 - val_mae: 968.7092\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1069956.7500 - mae: 989.9473 - val_loss: 1029272.1250 - val_mae: 967.9593\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1135651.5000 - mae: 1021.6149 - val_loss: 1027768.0000 - val_mae: 967.1823\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1138981.8750 - mae: 1022.8599 - val_loss: 1026284.1250 - val_mae: 966.4150\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step - loss: 1105310.2500 - mae: 1007.2110 - val_loss: 1024827.1250 - val_mae: 965.6610\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step - loss: 1120655.1250 - mae: 1015.3306 - val_loss: 1023393.1875 - val_mae: 964.9183\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step - loss: 1086310.2500 - mae: 998.5992 - val_loss: 1021933.1875 - val_mae: 964.1613\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 1106050.0000 - mae: 1006.9047 - val_loss: 1020412.6875 - val_mae: 963.3723\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1100874.6250 - mae: 1004.2778 - val_loss: 1018844.6875 - val_mae: 962.5578\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1107324.1250 - mae: 1006.9606 - val_loss: 1017022.1875 - val_mae: 961.6105\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1082688.6250 - mae: 995.9315 - val_loss: 1015048.8750 - val_mae: 960.5831\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1125119.2500 - mae: 1018.8858 - val_loss: 1012972.6875 - val_mae: 959.5010\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1101603.1250 - mae: 1005.1306 - val_loss: 1010822.0000 - val_mae: 958.3786\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1093196.5000 - mae: 1003.2309 - val_loss: 1008579.3125 - val_mae: 957.2065\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1080271.3750 - mae: 995.4410 - val_loss: 1006259.6250 - val_mae: 955.9924\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1085903.1250 - mae: 997.5937 - val_loss: 1003856.5000 - val_mae: 954.7329\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1092434.0000 - mae: 1001.9230 - val_loss: 1001383.6875 - val_mae: 953.4349\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1099381.5000 - mae: 1004.5626 - val_loss: 998834.5000 - val_mae: 952.0945\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1066179.2500 - mae: 986.9639 - val_loss: 996182.3125 - val_mae: 950.6979\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1096277.1250 - mae: 1001.6656 - val_loss: 993451.5000 - val_mae: 949.2573\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1079375.8750 - mae: 992.5256 - val_loss: 990659.1875 - val_mae: 947.7819\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1109581.0000 - mae: 1009.0074 - val_loss: 987771.8125 - val_mae: 946.2532\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1095970.6250 - mae: 1003.5587 - val_loss: 984832.0000 - val_mae: 944.6940\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 1044335.9375 - mae: 977.8931 - val_loss: 981821.8125 - val_mae: 943.0944\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1066352.0000 - mae: 987.3822 - val_loss: 978737.8750 - val_mae: 941.4523\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1104677.2500 - mae: 1007.3334 - val_loss: 975580.5000 - val_mae: 939.7678\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1083595.2500 - mae: 998.1428 - val_loss: 972331.5000 - val_mae: 938.0306\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1058753.7500 - mae: 983.2731 - val_loss: 968933.6875 - val_mae: 936.2101\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1040224.1250 - mae: 975.4856 - val_loss: 965388.6250 - val_mae: 934.3063\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1087953.8750 - mae: 998.6237 - val_loss: 961737.6875 - val_mae: 932.3410\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1016598.0625 - mae: 963.1905 - val_loss: 957972.0000 - val_mae: 930.3090\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1065363.1250 - mae: 987.7203 - val_loss: 954049.3750 - val_mae: 928.1870\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1062957.3750 - mae: 984.3854 - val_loss: 949986.6250 - val_mae: 925.9830\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1031829.8125 - mae: 969.4655 - val_loss: 945756.3750 - val_mae: 923.6820\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1043548.1250 - mae: 978.3249 - val_loss: 941346.8125 - val_mae: 921.2764\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1025476.9375 - mae: 965.9673 - val_loss: 936824.1250 - val_mae: 918.8016\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1032049.1250 - mae: 968.8889 - val_loss: 932179.1250 - val_mae: 916.2518\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1049465.7500 - mae: 976.1021 - val_loss: 927328.8125 - val_mae: 913.5807\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1020755.0000 - mae: 962.7206 - val_loss: 922236.6250 - val_mae: 910.7665\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1024891.3125 - mae: 967.8070 - val_loss: 916917.5000 - val_mae: 907.8164\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 994895.4375 - mae: 950.3695 - val_loss: 911391.0000 - val_mae: 904.7394\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1026088.1875 - mae: 965.5024 - val_loss: 905684.1875 - val_mae: 901.5495\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1006548.5000 - mae: 955.6706 - val_loss: 899793.0000 - val_mae: 898.2429\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1008312.9375 - mae: 957.0416 - val_loss: 893691.3750 - val_mae: 894.8032\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 994675.1250 - mae: 951.2021 - val_loss: 887476.3750 - val_mae: 891.2840\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 960327.7500 - mae: 932.4184 - val_loss: 881131.0000 - val_mae: 887.6743\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 992955.1875 - mae: 947.9207 - val_loss: 874621.0000 - val_mae: 883.9533\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1000783.0625 - mae: 953.9381 - val_loss: 867935.3125 - val_mae: 880.1131\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 979627.8125 - mae: 942.5217 - val_loss: 861072.8125 - val_mae: 876.1510\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1008891.4375 - mae: 957.6434 - val_loss: 854016.5000 - val_mae: 872.0554\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 961300.4375 - mae: 930.9747 - val_loss: 846741.8125 - val_mae: 867.8097\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 952521.6875 - mae: 925.7004 - val_loss: 839233.6875 - val_mae: 863.4023\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 945989.1875 - mae: 924.7985 - val_loss: 831433.5000 - val_mae: 858.7958\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 923345.3125 - mae: 912.0496 - val_loss: 823416.9375 - val_mae: 854.0314\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step - loss: 952774.0625 - mae: 926.8095 - val_loss: 815326.1875 - val_mae: 849.1918\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step - loss: 949149.9375 - mae: 924.3800 - val_loss: 807145.6875 - val_mae: 844.2654\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - loss: 918282.5625 - mae: 906.2415 - val_loss: 798787.6875 - val_mae: 839.1976\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step - loss: 938401.0625 - mae: 916.8760 - val_loss: 790065.0000 - val_mae: 833.8705\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 903353.9375 - mae: 897.6525 - val_loss: 780988.1250 - val_mae: 828.2847\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - loss: 910106.2500 - mae: 900.7252 - val_loss: 771599.5625 - val_mae: 822.4604\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 890811.5625 - mae: 892.3326 - val_loss: 762089.6250 - val_mae: 816.5116\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 901779.7500 - mae: 894.5436 - val_loss: 752563.6250 - val_mae: 810.5015\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 880551.1875 - mae: 883.0306 - val_loss: 742682.2500 - val_mae: 804.2117\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 867420.1875 - mae: 874.6815 - val_loss: 732527.8125 - val_mae: 797.6876\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 864601.3125 - mae: 874.9128 - val_loss: 722277.6250 - val_mae: 791.0379\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 857819.3750 - mae: 871.1646 - val_loss: 711984.5625 - val_mae: 784.2938\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 854082.4375 - mae: 865.7411 - val_loss: 701512.8750 - val_mae: 777.3619\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 841650.3750 - mae: 864.0403 - val_loss: 690881.7500 - val_mae: 770.2494\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 829806.0000 - mae: 851.4258 - val_loss: 680107.8125 - val_mae: 762.9613\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 809206.4375 - mae: 841.1888 - val_loss: 669150.6250 - val_mae: 755.4642\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 811704.8750 - mae: 843.0704 - val_loss: 658082.6875 - val_mae: 747.8008\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 804676.1875 - mae: 837.0866 - val_loss: 646953.6250 - val_mae: 740.0002\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 794396.5000 - mae: 829.5553 - val_loss: 635654.6875 - val_mae: 731.9793\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 788105.9375 - mae: 822.0527 - val_loss: 624248.8750 - val_mae: 723.7751\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 776732.6250 - mae: 816.7948 - val_loss: 612721.3750 - val_mae: 715.3691\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 760821.6875 - mae: 808.5643 - val_loss: 600998.7500 - val_mae: 706.6981\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 746853.6250 - mae: 797.1608 - val_loss: 589265.3125 - val_mae: 697.8896\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 751058.5625 - mae: 799.5042 - val_loss: 577453.0625 - val_mae: 688.8850\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 759922.0000 - mae: 806.6357 - val_loss: 565645.2500 - val_mae: 679.7399\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 709628.9375 - mae: 776.1795 - val_loss: 554104.4375 - val_mae: 670.6559\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 729128.5000 - mae: 782.0264 - val_loss: 542589.9375 - val_mae: 661.4418\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 666714.9375 - mae: 745.2585 - val_loss: 530822.8750 - val_mae: 651.8617\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 711131.1875 - mae: 768.0444 - val_loss: 519119.9375 - val_mae: 642.1608\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 690845.3750 - mae: 756.1273 - val_loss: 507546.0625 - val_mae: 632.3875\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 684960.0000 - mae: 746.4088 - val_loss: 496078.9062 - val_mae: 622.5184\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 654763.1875 - mae: 728.4439 - val_loss: 484600.6562 - val_mae: 612.4430\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 629365.0000 - mae: 715.3392 - val_loss: 473185.4375 - val_mae: 602.2160\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 657310.0625 - mae: 727.1655 - val_loss: 461814.9062 - val_mae: 591.8104\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 624251.5000 - mae: 705.4832 - val_loss: 450433.8438 - val_mae: 581.1619\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 627888.6250 - mae: 702.2553 - val_loss: 439207.8125 - val_mae: 570.4139\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 612392.6250 - mae: 698.0844 - val_loss: 428248.3438 - val_mae: 559.6700\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 609492.1875 - mae: 685.2623 - val_loss: 417653.5938 - val_mae: 549.0305\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "Top 3 taxistas con más carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "39             39            1488\n",
            "0               0            1482\n",
            "3               3            1437\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 417653.5938 - mae: 549.0305\n",
            "Pérdida: 417653.59375, MAE: 549.0304565429688\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicio propuesto: Trabajo de 2 máximo intergantes ."
      ],
      "metadata": {
        "id": "oznu_GgVlkZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1:    10 peores"
      ],
      "metadata": {
        "id": "uO1cnkf8gUyM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "\n",
        "# Contar el número de carreras por taxista\n",
        "carreras_por_taxista = df.groupby('Taxista_Label').size().reset_index(name='Total_Carreras')\n",
        "\n",
        "# Obtener los 10 taxistas con menos carreras\n",
        "peores_10_taxistas = carreras_por_taxista.nsmallest(10, 'Total_Carreras')\n",
        "\n",
        "# Datos de entrada (Taxista_Label) y de salida (Total_Carreras)\n",
        "X = carreras_por_taxista[['Taxista_Label']].values\n",
        "y = carreras_por_taxista['Total_Carreras'].values\n",
        "\n",
        "# Dividimos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predecir las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "\n",
        "# Mostrar los 10 taxistas con menos carreras\n",
        "print(\"Peores 10 taxistas con menos carreras:\")\n",
        "print(peores_10_taxistas)\n",
        "\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md_0-o41eMxF",
        "outputId": "c88e97b9-9bb8-4dd2-bdb2-1d9bdcab12f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - loss: 1136784.6250 - mae: 1022.8373 - val_loss: 1043216.6875 - val_mae: 975.1292\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1134237.7500 - mae: 1022.5142 - val_loss: 1041273.3750 - val_mae: 974.1338\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1115159.3750 - mae: 1012.2360 - val_loss: 1039337.8125 - val_mae: 973.1408\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1130151.1250 - mae: 1020.6683 - val_loss: 1037490.3750 - val_mae: 972.1922\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1118008.7500 - mae: 1014.3963 - val_loss: 1035738.1250 - val_mae: 971.2914\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1108441.0000 - mae: 1008.5718 - val_loss: 1033990.1250 - val_mae: 970.3919\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1109961.3750 - mae: 1009.7159 - val_loss: 1032346.6250 - val_mae: 969.5453\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1104975.8750 - mae: 1007.1061 - val_loss: 1030570.8750 - val_mae: 968.6296\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1102065.5000 - mae: 1006.6125 - val_loss: 1028768.0000 - val_mae: 967.6990\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1113635.2500 - mae: 1012.1223 - val_loss: 1027157.8125 - val_mae: 966.8669\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1144823.8750 - mae: 1026.7356 - val_loss: 1025418.6875 - val_mae: 965.9672\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1117742.2500 - mae: 1012.2550 - val_loss: 1023697.1875 - val_mae: 965.0758\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1104570.2500 - mae: 1004.9833 - val_loss: 1021900.6875 - val_mae: 964.1444\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1119952.0000 - mae: 1013.8140 - val_loss: 1019919.8750 - val_mae: 963.1164\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 1131732.8750 - mae: 1021.7039 - val_loss: 1017807.1875 - val_mae: 962.0187\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 1137914.5000 - mae: 1023.4733 - val_loss: 1015620.0000 - val_mae: 960.8805\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1078210.6250 - mae: 993.7317 - val_loss: 1013297.6250 - val_mae: 959.6705\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 1097959.7500 - mae: 1004.1356 - val_loss: 1010857.8125 - val_mae: 958.3973\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1101364.8750 - mae: 1006.0543 - val_loss: 1008307.3125 - val_mae: 957.0641\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 1078249.5000 - mae: 992.2330 - val_loss: 1005652.6250 - val_mae: 955.6745\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 1093550.1250 - mae: 1002.3121 - val_loss: 1002907.6875 - val_mae: 954.2352\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1093733.7500 - mae: 1001.4341 - val_loss: 1000092.6250 - val_mae: 952.7563\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1072402.0000 - mae: 990.8018 - val_loss: 997189.5000 - val_mae: 951.2287\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1124486.2500 - mae: 1015.4713 - val_loss: 994231.5000 - val_mae: 949.6691\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1079823.1250 - mae: 994.1310 - val_loss: 991170.1875 - val_mae: 948.0521\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1075556.2500 - mae: 991.3726 - val_loss: 987974.3750 - val_mae: 946.3605\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1069204.8750 - mae: 988.4503 - val_loss: 984675.8125 - val_mae: 944.6110\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1087436.6250 - mae: 997.4772 - val_loss: 981240.0000 - val_mae: 942.7848\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1049131.8750 - mae: 978.2755 - val_loss: 977682.1250 - val_mae: 940.8895\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1079936.7500 - mae: 994.6510 - val_loss: 973989.3750 - val_mae: 938.9176\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1076917.5000 - mae: 994.2983 - val_loss: 970132.6250 - val_mae: 936.8531\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1068010.3750 - mae: 989.0571 - val_loss: 966047.6875 - val_mae: 934.6608\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - loss: 1052452.7500 - mae: 981.3635 - val_loss: 961744.6250 - val_mae: 932.3449\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step - loss: 1076654.3750 - mae: 994.9797 - val_loss: 957273.3750 - val_mae: 929.9316\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 1064280.3750 - mae: 985.3705 - val_loss: 952647.6875 - val_mae: 927.4276\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1057290.6250 - mae: 983.2318 - val_loss: 947857.6875 - val_mae: 924.8260\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1076560.6250 - mae: 992.4930 - val_loss: 942953.1250 - val_mae: 922.1538\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1008589.9375 - mae: 958.0703 - val_loss: 937847.6250 - val_mae: 919.3625\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1063011.5000 - mae: 984.5434 - val_loss: 932593.6875 - val_mae: 916.4800\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1042656.9375 - mae: 975.1190 - val_loss: 927221.6875 - val_mae: 913.5219\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1048855.5000 - mae: 977.8111 - val_loss: 921686.6875 - val_mae: 910.4625\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1055769.7500 - mae: 981.0555 - val_loss: 915905.5000 - val_mae: 907.2543\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1009859.5625 - mae: 958.2581 - val_loss: 909910.6250 - val_mae: 903.9138\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1021800.6875 - mae: 964.9515 - val_loss: 903699.3750 - val_mae: 900.4375\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 997947.1875 - mae: 951.6195 - val_loss: 897218.5000 - val_mae: 896.7939\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1026422.4375 - mae: 967.4510 - val_loss: 890494.8125 - val_mae: 892.9958\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1003996.6250 - mae: 953.6255 - val_loss: 883436.6875 - val_mae: 888.9886\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 976223.2500 - mae: 940.4003 - val_loss: 876103.3750 - val_mae: 884.8030\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1003117.6875 - mae: 953.3444 - val_loss: 868633.3750 - val_mae: 880.5158\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 967710.6250 - mae: 934.8333 - val_loss: 861013.8125 - val_mae: 876.1179\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 939833.7500 - mae: 921.7072 - val_loss: 853079.8750 - val_mae: 871.5111\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 932706.8125 - mae: 916.7914 - val_loss: 844960.8125 - val_mae: 866.7678\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 958033.5000 - mae: 929.2468 - val_loss: 836589.4375 - val_mae: 861.8453\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 947118.6250 - mae: 922.9984 - val_loss: 827859.3750 - val_mae: 856.6769\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 966228.0625 - mae: 933.8377 - val_loss: 818841.1250 - val_mae: 851.2999\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 954295.6250 - mae: 928.9708 - val_loss: 809548.4375 - val_mae: 845.7176\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 941832.4375 - mae: 920.7360 - val_loss: 799982.4375 - val_mae: 839.9261\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 933309.3125 - mae: 916.8106 - val_loss: 790307.8750 - val_mae: 834.0214\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 896751.2500 - mae: 894.8380 - val_loss: 780516.8750 - val_mae: 827.9957\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 927729.3750 - mae: 910.9332 - val_loss: 770445.3750 - val_mae: 821.7435\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 885894.4375 - mae: 889.2273 - val_loss: 760224.8750 - val_mae: 815.3417\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 893894.9375 - mae: 893.1573 - val_loss: 749944.0000 - val_mae: 808.8424\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 870823.6250 - mae: 880.6205 - val_loss: 739592.8750 - val_mae: 802.2365\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 876709.0625 - mae: 881.0736 - val_loss: 729207.5625 - val_mae: 795.5442\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 869658.0625 - mae: 876.4529 - val_loss: 718579.8750 - val_mae: 788.6266\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 837411.9375 - mae: 859.2717 - val_loss: 707715.1875 - val_mae: 781.4805\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 847372.0625 - mae: 862.7349 - val_loss: 696733.0625 - val_mae: 774.1782\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 867603.8750 - mae: 873.6636 - val_loss: 685666.1250 - val_mae: 766.7365\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 816361.9375 - mae: 845.4091 - val_loss: 674603.5625 - val_mae: 759.2117\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 832492.3125 - mae: 854.8199 - val_loss: 663501.6250 - val_mae: 751.5706\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 785117.0625 - mae: 824.6866 - val_loss: 652481.9375 - val_mae: 743.8942\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 824189.8125 - mae: 848.1996 - val_loss: 641502.6875 - val_mae: 736.1513\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 782035.5000 - mae: 824.1058 - val_loss: 630535.9375 - val_mae: 728.3196\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 769108.3125 - mae: 812.6476 - val_loss: 619281.6250 - val_mae: 720.1769\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 753070.5625 - mae: 806.8162 - val_loss: 607780.1250 - val_mae: 711.7401\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 746660.8750 - mae: 798.4180 - val_loss: 596268.0625 - val_mae: 703.1743\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 768227.4375 - mae: 808.7162 - val_loss: 584671.7500 - val_mae: 694.4175\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 765090.5625 - mae: 808.4116 - val_loss: 573021.4375 - val_mae: 685.4843\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 736956.0625 - mae: 788.9951 - val_loss: 561288.9375 - val_mae: 676.3439\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 703853.9375 - mae: 767.7038 - val_loss: 549530.1250 - val_mae: 667.0309\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 710970.6875 - mae: 770.6141 - val_loss: 537762.0000 - val_mae: 657.5500\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 701859.6875 - mae: 763.4567 - val_loss: 526020.6250 - val_mae: 647.9222\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 696066.6250 - mae: 760.1456 - val_loss: 514193.9062 - val_mae: 638.0450\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 676729.8750 - mae: 749.6284 - val_loss: 502390.3125 - val_mae: 627.9969\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 659556.9375 - mae: 734.0707 - val_loss: 490769.5625 - val_mae: 617.9081\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 660589.5625 - mae: 731.6843 - val_loss: 479100.5938 - val_mae: 607.5690\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 660260.9375 - mae: 732.5724 - val_loss: 467420.0000 - val_mae: 596.9967\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 620873.5000 - mae: 702.8614 - val_loss: 455985.4062 - val_mae: 586.4172\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 638749.0000 - mae: 710.8539 - val_loss: 444608.9062 - val_mae: 575.6497\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 623350.4375 - mae: 697.6647 - val_loss: 433054.7500 - val_mae: 564.4490\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 589414.8125 - mae: 677.4744 - val_loss: 421443.2500 - val_mae: 552.9032\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 586901.5625 - mae: 668.8557 - val_loss: 410059.8438 - val_mae: 541.2802\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 604249.1875 - mae: 684.4587 - val_loss: 398943.4688 - val_mae: 529.6146\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 569000.6875 - mae: 654.1450 - val_loss: 388248.3125 - val_mae: 518.0719\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 581506.6250 - mae: 660.2820 - val_loss: 378001.8438 - val_mae: 506.6942\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 587313.6250 - mae: 665.3729 - val_loss: 368060.0625 - val_mae: 495.3286\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 531016.1250 - mae: 621.1730 - val_loss: 358340.5625 - val_mae: 483.8777\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 546848.3125 - mae: 633.0068 - val_loss: 348791.9062 - val_mae: 472.2701\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 533384.0000 - mae: 626.6572 - val_loss: 339427.5625 - val_mae: 460.5065\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 532268.0625 - mae: 614.4476 - val_loss: 330377.7812 - val_mae: 449.7115\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n",
            "Peores 10 taxistas con menos carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "28             28             504\n",
            "12             12             537\n",
            "2               2             540\n",
            "38             38             585\n",
            "23             23             623\n",
            "29             29             644\n",
            "37             37             657\n",
            "30             30             679\n",
            "13             13             691\n",
            "47             47             705\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 330377.7812 - mae: 449.7115\n",
            "Pérdida: 330377.78125, MAE: 449.71148681640625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2   2.\tFiltrar por año"
      ],
      "metadata": {
        "id": "MCcST_JAm08A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "# Generación de datos\n",
        "def generar_datos_radio_taxi():\n",
        " data = []\n",
        " codigos_taxistas = [f'AS-{str(i).zfill(3)}' for i in range(1, 51)] # 50 taxistas\n",
        " inicio_fecha = datetime(2001, 1, 1)\n",
        " # Generamos carreras aleatorias para cada taxista\n",
        " for taxista in codigos_taxistas:\n",
        "  num_carreras = random.randint(500, 1500) # Cada taxista tiene entre 500 y 1500 carreras\n",
        "  for i in range(num_carreras):\n",
        "   fecha_carrera = inicio_fecha + timedelta(days=random.randint(0, (datetime(2024, 12, 31) - inicio_fecha).days))\n",
        "   duracion_carrera = random.randint(5, 60) # Duración de la carrera entre 5 y 60 minutos\n",
        "   data.append([taxista, i + 1, fecha_carrera.strftime('%Y-%m-%d'), duracion_carrera])\n",
        " # Crear DataFrame y guardarlo como CSV\n",
        " df = pd.DataFrame(data, columns=['Codigo_Taxista', 'Numero_Carrera', 'Fecha_Carrera', 'Duracion_Carrera'])\n",
        " # Asegurarnos que 'Fecha_Carrera' sea de tipo datetime\n",
        "\n",
        " df['Fecha_Carrera'] = pd.to_datetime(df['Fecha_Carrera'])\n",
        " # Añadir la columna \"Año de la Carrera\"\n",
        "\n",
        " df['Año_Carrera'] = df['Fecha_Carrera'].dt.year\n",
        " # Filtrar los datos para encontrar los taxistas con más carreras en 2020\n",
        " carreras_2020 = df[df['Año_Carrera'] == 2020]\n",
        " # Contar el número de carreras por taxista en 2020\n",
        " carreras_por_taxista_2020 = carreras_2020.groupby('Codigo_Taxista').size().reset_index(name='Total_Carreras')\n",
        "# Obtener los taxistas con más carreras en 2020\n",
        " top_taxistas_2020 = carreras_por_taxista_2020.nlargest(3, 'Total_Carreras')\n",
        "# Guardar el DataFrame original en un archivo CSV\n",
        "\n",
        " df.to_csv('radio_taxi_carreras.csv', index=False)\n",
        "\n",
        "# Retornar el DataFrame\n",
        " return df, top_taxistas_2020\n",
        " # Generar y visualizar los primeros datos\n",
        "df, top_taxistas_2020 = generar_datos_radio_taxi()\n",
        "print(df.head())\n",
        "print(top_taxistas_2020)\n",
        "# Generar y visualizar los primeros datos\n",
        "df = generar_datos_radio_taxi()\n",
        "print(df)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#**********\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "# Contar el número de carreras por taxista\n",
        "carreras_por_taxista = df.groupby('Taxista_Label').size().reset_index(name='Total_Carreras')\n",
        "# Obtener los 3 taxistas con más carreras\n",
        "top_3_taxistas = carreras_por_taxista.nlargest(3, 'Total_Carreras')\n",
        "# Obtener los 10 taxistas con menos carreras\n",
        "peores_10_taxistas = carreras_por_taxista.nsmallest(10, 'Total_Carreras')\n",
        "# Datos de entrada (Taxista_Label) y de salida (Total_Carreras)\n",
        "X = carreras_por_taxista[['Taxista_Label']].values\n",
        "y = carreras_por_taxista['Total_Carreras'].values\n",
        "# Dividimos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "tf.keras.layers.Dense(32, activation='relu'),\n",
        "tf.keras.layers.Dense(1)\n",
        "\n",
        "])\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "# Predecir las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "# Mostrar los 3 taxistas con más carreras\n",
        "print(\"Top 3 taxistas con más carreras:\")\n",
        "print(top_3_taxistas)\n",
        "# Mostrar los 3 taxistas con más carreras\n",
        "print(\"Los 10 taxistas con peores carreras:\")\n",
        "print(peores_10_taxistas)\n",
        "#mostrar taxistas con mas carreras en 2020\n",
        "print(\"los taxistas con mas carreras del 2020\")\n",
        "print(top_taxistas_2020)\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXrrEfAJm0d5",
        "outputId": "2392e82e-1d30-441a-c6e4-f4fad6cde580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Codigo_Taxista  Numero_Carrera Fecha_Carrera  Duracion_Carrera  Año_Carrera\n",
            "0         AS-001               1    2004-12-07                40         2004\n",
            "1         AS-001               2    2024-12-20                25         2024\n",
            "2         AS-001               3    2002-08-17                47         2002\n",
            "3         AS-001               4    2012-08-30                59         2012\n",
            "4         AS-001               5    2023-03-18                41         2023\n",
            "   Codigo_Taxista  Total_Carreras\n",
            "33         AS-034              76\n",
            "5          AS-006              71\n",
            "38         AS-039              62\n",
            "(      Codigo_Taxista  Numero_Carrera Fecha_Carrera  Duracion_Carrera  \\\n",
            "0             AS-001               1    2017-03-27                15   \n",
            "1             AS-001               2    2008-10-04                 5   \n",
            "2             AS-001               3    2019-07-21                32   \n",
            "3             AS-001               4    2011-12-29                50   \n",
            "4             AS-001               5    2007-02-24                60   \n",
            "...              ...             ...           ...               ...   \n",
            "49286         AS-050             750    2020-07-15                40   \n",
            "49287         AS-050             751    2006-06-10                10   \n",
            "49288         AS-050             752    2014-10-19                38   \n",
            "49289         AS-050             753    2018-01-27                13   \n",
            "49290         AS-050             754    2019-09-06                23   \n",
            "\n",
            "       Año_Carrera  \n",
            "0             2017  \n",
            "1             2008  \n",
            "2             2019  \n",
            "3             2011  \n",
            "4             2007  \n",
            "...            ...  \n",
            "49286         2020  \n",
            "49287         2006  \n",
            "49288         2014  \n",
            "49289         2018  \n",
            "49290         2019  \n",
            "\n",
            "[49291 rows x 5 columns],    Codigo_Taxista  Total_Carreras\n",
            "21         AS-022              70\n",
            "2          AS-003              63\n",
            "31         AS-032              61)\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 190ms/step - loss: 1099950.8750 - mae: 999.6252 - val_loss: 900142.0000 - val_mae: 927.5695\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1083539.1250 - mae: 993.3302 - val_loss: 898423.1250 - val_mae: 926.6414\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1097991.3750 - mae: 998.9790 - val_loss: 896695.6250 - val_mae: 925.7076\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1105123.1250 - mae: 1003.7708 - val_loss: 895111.5000 - val_mae: 924.8505\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 1098130.3750 - mae: 997.2120 - val_loss: 893434.8125 - val_mae: 923.9423\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1093020.8750 - mae: 995.8071 - val_loss: 891711.6250 - val_mae: 923.0077\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1086205.1250 - mae: 994.3054 - val_loss: 889971.0000 - val_mae: 922.0627\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1103677.3750 - mae: 1002.0034 - val_loss: 888234.3125 - val_mae: 921.1187\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1060907.8750 - mae: 979.1080 - val_loss: 886720.8125 - val_mae: 920.2949\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1087803.2500 - mae: 996.0592 - val_loss: 885228.6875 - val_mae: 919.4821\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1104807.0000 - mae: 1003.0615 - val_loss: 883920.6250 - val_mae: 918.7687\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1049327.7500 - mae: 975.3373 - val_loss: 882604.6250 - val_mae: 918.0506\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1064043.2500 - mae: 983.8876 - val_loss: 881240.0000 - val_mae: 917.3053\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1077773.6250 - mae: 990.6938 - val_loss: 879829.3750 - val_mae: 916.5340\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1079411.3750 - mae: 989.2918 - val_loss: 878372.3750 - val_mae: 915.7366\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1094083.1250 - mae: 997.3146 - val_loss: 876858.3750 - val_mae: 914.9072\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1060807.6250 - mae: 981.7056 - val_loss: 875296.5000 - val_mae: 914.0506\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1080749.0000 - mae: 989.7946 - val_loss: 873693.3750 - val_mae: 913.1705\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1073382.0000 - mae: 985.7770 - val_loss: 872046.8125 - val_mae: 912.2654\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1056468.1250 - mae: 978.6302 - val_loss: 870343.1875 - val_mae: 911.3280\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1029666.4375 - mae: 965.9536 - val_loss: 868589.0000 - val_mae: 910.3615\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1045281.0625 - mae: 974.5173 - val_loss: 866772.1250 - val_mae: 909.3594\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1089234.2500 - mae: 996.3073 - val_loss: 864746.0000 - val_mae: 908.2404\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 1074506.1250 - mae: 987.2083 - val_loss: 862584.0000 - val_mae: 907.0446\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1059272.5000 - mae: 979.1214 - val_loss: 860301.8125 - val_mae: 905.7802\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1044752.0625 - mae: 970.4443 - val_loss: 857849.5000 - val_mae: 904.4193\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1067910.8750 - mae: 981.9404 - val_loss: 855231.0000 - val_mae: 902.9637\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1086217.7500 - mae: 993.6616 - val_loss: 852483.1875 - val_mae: 901.4333\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1067460.8750 - mae: 984.1957 - val_loss: 849584.3125 - val_mae: 899.8156\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1054273.1250 - mae: 976.4190 - val_loss: 846532.6875 - val_mae: 898.1089\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1030840.4375 - mae: 964.1969 - val_loss: 843373.8125 - val_mae: 896.3383\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 1042736.6875 - mae: 968.8394 - val_loss: 840123.0000 - val_mae: 894.5121\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1048530.5625 - mae: 972.8817 - val_loss: 836723.6250 - val_mae: 892.5977\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1023123.3125 - mae: 961.1098 - val_loss: 833146.1250 - val_mae: 890.5779\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1023464.1250 - mae: 960.8766 - val_loss: 829403.9375 - val_mae: 888.4596\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1044480.8125 - mae: 971.2889 - val_loss: 825562.8125 - val_mae: 886.2792\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1021685.0000 - mae: 959.3583 - val_loss: 821660.8750 - val_mae: 884.0581\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1027257.1875 - mae: 962.2416 - val_loss: 817658.0000 - val_mae: 881.7726\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1016438.0000 - mae: 958.1689 - val_loss: 813510.6250 - val_mae: 879.3975\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1006928.1250 - mae: 950.0970 - val_loss: 809194.1250 - val_mae: 876.9178\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1009650.0625 - mae: 954.1107 - val_loss: 804716.6250 - val_mae: 874.3369\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1000988.4375 - mae: 947.3516 - val_loss: 800068.1875 - val_mae: 871.6483\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1021438.2500 - mae: 958.8997 - val_loss: 795237.8750 - val_mae: 868.8444\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 1020481.8750 - mae: 958.0477 - val_loss: 790305.6875 - val_mae: 865.9705\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 998071.1250 - mae: 946.0776 - val_loss: 785300.1875 - val_mae: 863.0427\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 975829.6250 - mae: 937.5055 - val_loss: 780228.2500 - val_mae: 860.0643\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1031958.8125 - mae: 963.0838 - val_loss: 775099.1250 - val_mae: 857.0402\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1003757.4375 - mae: 946.9282 - val_loss: 769869.8750 - val_mae: 853.9443\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 995099.1875 - mae: 944.3408 - val_loss: 764431.5625 - val_mae: 850.7109\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 977702.1875 - mae: 934.7861 - val_loss: 758782.3125 - val_mae: 847.3369\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 942377.0000 - mae: 915.5045 - val_loss: 752951.4375 - val_mae: 843.8383\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 966428.7500 - mae: 930.5274 - val_loss: 746947.8750 - val_mae: 840.2184\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 963614.2500 - mae: 926.7383 - val_loss: 740880.3125 - val_mae: 836.5414\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 964649.3750 - mae: 926.3610 - val_loss: 734598.6250 - val_mae: 832.7148\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 928652.1875 - mae: 907.9670 - val_loss: 728067.0000 - val_mae: 828.7145\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 944957.0625 - mae: 920.9772 - val_loss: 721414.0000 - val_mae: 824.6165\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 945334.3125 - mae: 913.5732 - val_loss: 714643.8125 - val_mae: 820.4221\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 915787.4375 - mae: 901.0380 - val_loss: 707658.6875 - val_mae: 816.0682\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 941399.3750 - mae: 914.4521 - val_loss: 700474.5000 - val_mae: 811.5619\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 918826.9375 - mae: 902.8871 - val_loss: 693144.8125 - val_mae: 806.9342\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 943402.9375 - mae: 913.5817 - val_loss: 685681.1875 - val_mae: 802.1900\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 907350.3125 - mae: 895.1694 - val_loss: 678070.6875 - val_mae: 797.3185\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 925940.4375 - mae: 904.8345 - val_loss: 670216.6250 - val_mae: 792.2546\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 909883.6875 - mae: 891.8867 - val_loss: 662234.3125 - val_mae: 787.0689\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 883999.6875 - mae: 880.8400 - val_loss: 654097.1875 - val_mae: 781.7412\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 883160.1250 - mae: 877.9599 - val_loss: 645779.3750 - val_mae: 776.2512\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 866505.6875 - mae: 874.2804 - val_loss: 637322.8750 - val_mae: 770.6224\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 869243.9375 - mae: 868.4305 - val_loss: 628795.2500 - val_mae: 764.8972\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 891554.8125 - mae: 880.9151 - val_loss: 620161.3750 - val_mae: 759.0490\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 849005.0000 - mae: 857.7213 - val_loss: 611520.1250 - val_mae: 753.1423\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 832004.4375 - mae: 847.8210 - val_loss: 602706.8750 - val_mae: 747.0614\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 817710.9375 - mae: 839.0807 - val_loss: 593733.0000 - val_mae: 740.8092\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 831542.0625 - mae: 846.7084 - val_loss: 584508.3125 - val_mae: 734.3165\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 840285.5625 - mae: 847.9007 - val_loss: 575068.4375 - val_mae: 727.6014\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 790277.6875 - mae: 822.4879 - val_loss: 565512.6250 - val_mae: 720.7283\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 818049.7500 - mae: 834.3826 - val_loss: 555922.7500 - val_mae: 713.7518\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 811501.8750 - mae: 834.4581 - val_loss: 546298.1875 - val_mae: 706.6675\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 803476.7500 - mae: 825.9639 - val_loss: 536637.1250 - val_mae: 699.4703\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 786165.2500 - mae: 814.0694 - val_loss: 526962.1250 - val_mae: 692.1732\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 762882.2500 - mae: 803.6848 - val_loss: 517279.5625 - val_mae: 684.7772\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 773457.2500 - mae: 804.1542 - val_loss: 507468.0938 - val_mae: 677.1838\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 752574.3125 - mae: 791.7101 - val_loss: 497444.6562 - val_mae: 669.3194\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 756057.3125 - mae: 796.3397 - val_loss: 487468.0625 - val_mae: 661.3798\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 746380.4375 - mae: 788.0043 - val_loss: 477750.8125 - val_mae: 653.5347\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 740053.0000 - mae: 780.3382 - val_loss: 468148.5938 - val_mae: 645.6691\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 717231.8750 - mae: 764.3744 - val_loss: 458660.0938 - val_mae: 637.7810\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 730066.0625 - mae: 772.0139 - val_loss: 449361.0938 - val_mae: 629.9341\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 713205.9375 - mae: 764.8871 - val_loss: 440003.5938 - val_mae: 621.9155\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 684428.6875 - mae: 742.2575 - val_loss: 430546.5000 - val_mae: 613.6813\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 691434.8750 - mae: 746.3268 - val_loss: 420993.9375 - val_mae: 605.2242\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 708174.5625 - mae: 754.5777 - val_loss: 411383.6875 - val_mae: 596.5668\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 672587.6250 - mae: 730.2941 - val_loss: 401733.9375 - val_mae: 587.7149\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 673056.5625 - mae: 727.6262 - val_loss: 392123.4375 - val_mae: 578.7320\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 620884.5000 - mae: 694.8130 - val_loss: 382373.9375 - val_mae: 569.4385\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 649630.3750 - mae: 706.5502 - val_loss: 372533.1250 - val_mae: 559.8622\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 653924.3750 - mae: 717.1656 - val_loss: 362869.0938 - val_mae: 550.2542\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 614500.1875 - mae: 682.8707 - val_loss: 353464.7500 - val_mae: 540.6974\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 645968.5000 - mae: 699.7382 - val_loss: 344079.5000 - val_mae: 530.9424\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 608790.8125 - mae: 674.2421 - val_loss: 334850.7500 - val_mae: 521.1230\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 626736.4375 - mae: 683.5992 - val_loss: 325865.0000 - val_mae: 511.3301\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "Top 3 taxistas con más carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "0               0            1480\n",
            "2               2            1467\n",
            "10             10            1448\n",
            "Los 10 taxistas con peores carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "16             16             512\n",
            "40             40             512\n",
            "11             11             529\n",
            "7               7             572\n",
            "38             38             577\n",
            "42             42             581\n",
            "29             29             592\n",
            "4               4             616\n",
            "28             28             657\n",
            "33             33             663\n",
            "los taxistas con mas carreras del 2020\n",
            "   Codigo_Taxista  Total_Carreras\n",
            "33         AS-034              76\n",
            "5          AS-006              71\n",
            "38         AS-039              62\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 325865.0000 - mae: 511.3301\n",
            "Pérdida: 325865.0, MAE: 511.33013916015625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3\tDuración promedio de carreras"
      ],
      "metadata": {
        "id": "CBA7DH8tgffr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "\n",
        "# Calcular la duración promedio de las carreras para cada taxista\n",
        "duracion_promedio_por_taxista = df.groupby('Taxista_Label')['Duracion_Carrera'].mean().reset_index(name='Promedio_Duracion')\n",
        "\n",
        "# Obtener los 5 taxistas con las carreras más cortas en promedio\n",
        "peores_5_taxistas = duracion_promedio_por_taxista.nsmallest(5, 'Promedio_Duracion')\n",
        "\n",
        "# Datos de entrada (Taxista_Label) y de salida (Promedio_Duracion)\n",
        "X = duracion_promedio_por_taxista[['Taxista_Label']].values\n",
        "y = duracion_promedio_por_taxista['Promedio_Duracion'].values\n",
        "\n",
        "# Dividimos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predecir la duración promedio de las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "\n",
        "# Mostrar los 5 taxistas con las carreras más cortas en promedio\n",
        "print(\"Peores 5 taxistas con carreras más cortas en promedio:\")\n",
        "print(peores_5_taxistas)\n",
        "\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuyRb2bDgd0T",
        "outputId": "7eb72b20-56f2-47d8-8893-886749d3d204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 184ms/step - loss: 1156.7583 - mae: 33.9945 - val_loss: 1084.0039 - val_mae: 32.9209\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1093.8915 - mae: 33.0685 - val_loss: 1010.7375 - val_mae: 31.7880\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1033.2540 - mae: 32.1396 - val_loss: 940.0018 - val_mae: 30.6486\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 977.2973 - mae: 31.2472 - val_loss: 872.0497 - val_mae: 29.5060\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 922.3602 - mae: 30.3343 - val_loss: 807.0128 - val_mae: 28.3624\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 865.1486 - mae: 29.3469 - val_loss: 745.1880 - val_mae: 27.2230\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 823.0508 - mae: 28.5710 - val_loss: 686.4825 - val_mae: 26.0867\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 769.0529 - mae: 27.5532 - val_loss: 633.4751 - val_mae: 25.0076\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 735.4867 - mae: 26.8662 - val_loss: 585.3431 - val_mae: 23.9774\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 693.2365 - mae: 25.9938 - val_loss: 539.1488 - val_mae: 22.9363\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 655.8951 - mae: 25.1701 - val_loss: 495.5333 - val_mae: 21.8975\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 625.3218 - mae: 24.4433 - val_loss: 454.2270 - val_mae: 20.8550\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 586.7655 - mae: 23.5427 - val_loss: 415.0435 - val_mae: 19.8027\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 562.0968 - mae: 22.8589 - val_loss: 378.1977 - val_mae: 18.7448\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 524.3981 - mae: 21.8846 - val_loss: 343.7172 - val_mae: 17.6809\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 497.7489 - mae: 21.0901 - val_loss: 311.8523 - val_mae: 16.6183\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 457.2624 - mae: 19.9086 - val_loss: 284.2147 - val_mae: 15.6137\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 456.1677 - mae: 19.6704 - val_loss: 259.7995 - val_mae: 14.6520\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 420.7080 - mae: 18.5457 - val_loss: 237.4194 - val_mae: 13.6856\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 409.5571 - mae: 17.9270 - val_loss: 217.7426 - val_mae: 12.7469\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 394.5937 - mae: 17.2870 - val_loss: 200.3149 - val_mae: 11.9812\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 370.4497 - mae: 16.4033 - val_loss: 184.7670 - val_mae: 11.4705\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 349.8618 - mae: 15.6903 - val_loss: 171.2747 - val_mae: 11.1293\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 339.5529 - mae: 15.2751 - val_loss: 159.1723 - val_mae: 10.7707\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 328.7701 - mae: 14.9204 - val_loss: 148.8260 - val_mae: 10.3997\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 324.2055 - mae: 14.6978 - val_loss: 140.7536 - val_mae: 10.1153\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 314.6242 - mae: 14.4846 - val_loss: 135.0388 - val_mae: 10.0098\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 315.0239 - mae: 14.5213 - val_loss: 131.5688 - val_mae: 9.9085\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 308.0690 - mae: 14.4198 - val_loss: 130.0882 - val_mae: 9.8129\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 300.5137 - mae: 14.2863 - val_loss: 130.2480 - val_mae: 9.7248\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 304.1461 - mae: 14.5359 - val_loss: 131.4872 - val_mae: 9.7025\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 295.4944 - mae: 14.3656 - val_loss: 133.2622 - val_mae: 9.7606\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 284.7241 - mae: 14.2960 - val_loss: 135.3196 - val_mae: 9.8097\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 288.1935 - mae: 14.4484 - val_loss: 137.1722 - val_mae: 9.8453\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 292.8922 - mae: 14.5521 - val_loss: 138.5788 - val_mae: 9.9056\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 313.0529 - mae: 15.0837 - val_loss: 139.6188 - val_mae: 9.9583\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 296.6380 - mae: 14.6555 - val_loss: 139.4309 - val_mae: 9.9513\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 297.2789 - mae: 14.7016 - val_loss: 138.2951 - val_mae: 9.9000\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 290.9476 - mae: 14.4591 - val_loss: 136.6951 - val_mae: 9.8221\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 288.6659 - mae: 14.2915 - val_loss: 134.4805 - val_mae: 9.7776\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 287.6977 - mae: 14.3459 - val_loss: 132.7870 - val_mae: 9.7368\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 302.2583 - mae: 14.6671 - val_loss: 131.3334 - val_mae: 9.6925\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 294.8238 - mae: 14.3300 - val_loss: 130.3544 - val_mae: 9.6608\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 302.5705 - mae: 14.5280 - val_loss: 129.4960 - val_mae: 9.6285\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 283.2311 - mae: 14.0155 - val_loss: 128.9637 - val_mae: 9.6076\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 293.3188 - mae: 14.4283 - val_loss: 128.2013 - val_mae: 9.5854\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 278.2263 - mae: 13.8955 - val_loss: 127.7822 - val_mae: 9.5930\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 295.3989 - mae: 14.3617 - val_loss: 127.6916 - val_mae: 9.5817\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 296.9673 - mae: 14.3641 - val_loss: 127.7724 - val_mae: 9.5617\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 285.9580 - mae: 14.2529 - val_loss: 128.0732 - val_mae: 9.5738\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 286.0773 - mae: 14.0273 - val_loss: 128.3047 - val_mae: 9.5837\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 286.2972 - mae: 14.1444 - val_loss: 128.1791 - val_mae: 9.5790\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 290.4824 - mae: 14.2117 - val_loss: 127.6163 - val_mae: 9.5569\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 284.5959 - mae: 14.1334 - val_loss: 127.1779 - val_mae: 9.5386\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 292.3062 - mae: 14.2457 - val_loss: 127.0194 - val_mae: 9.5328\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 288.7414 - mae: 14.0837 - val_loss: 126.8934 - val_mae: 9.5284\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 289.0293 - mae: 14.1906 - val_loss: 126.5741 - val_mae: 9.5151\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 281.5069 - mae: 13.9803 - val_loss: 126.1504 - val_mae: 9.5028\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 276.8942 - mae: 13.8390 - val_loss: 125.7936 - val_mae: 9.5051\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 281.0555 - mae: 13.9367 - val_loss: 125.4132 - val_mae: 9.5097\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - loss: 271.8911 - mae: 13.7862 - val_loss: 125.0968 - val_mae: 9.5112\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 226ms/step - loss: 287.3084 - mae: 14.1459 - val_loss: 125.0012 - val_mae: 9.4965\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 300.6173 - mae: 14.5478 - val_loss: 125.0218 - val_mae: 9.4759\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - loss: 278.2451 - mae: 13.9232 - val_loss: 124.8194 - val_mae: 9.4697\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 283.6752 - mae: 14.0347 - val_loss: 124.6198 - val_mae: 9.4630\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 162ms/step - loss: 289.9997 - mae: 14.1356 - val_loss: 124.4809 - val_mae: 9.4527\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 292.7046 - mae: 14.3734 - val_loss: 124.2580 - val_mae: 9.4475\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 277.4452 - mae: 13.9228 - val_loss: 123.9530 - val_mae: 9.4477\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 276.8940 - mae: 13.7998 - val_loss: 123.8536 - val_mae: 9.4344\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 270.5295 - mae: 13.6435 - val_loss: 123.9901 - val_mae: 9.4152\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 286.4763 - mae: 14.1764 - val_loss: 124.2943 - val_mae: 9.4308\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 270.1811 - mae: 13.6150 - val_loss: 124.9040 - val_mae: 9.4534\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 283.7269 - mae: 14.1300 - val_loss: 125.9856 - val_mae: 9.4843\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 290.5431 - mae: 14.3438 - val_loss: 126.8007 - val_mae: 9.5024\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 283.4735 - mae: 14.3276 - val_loss: 127.6968 - val_mae: 9.5196\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 290.8814 - mae: 14.3102 - val_loss: 127.8815 - val_mae: 9.5201\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 284.4754 - mae: 14.3406 - val_loss: 126.6814 - val_mae: 9.4908\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 289.8973 - mae: 14.4157 - val_loss: 125.2718 - val_mae: 9.4537\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 292.8950 - mae: 14.5217 - val_loss: 123.7709 - val_mae: 9.4086\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 291.7807 - mae: 14.3922 - val_loss: 122.3944 - val_mae: 9.3571\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 274.1110 - mae: 13.8856 - val_loss: 121.5140 - val_mae: 9.3431\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 282.2019 - mae: 13.9605 - val_loss: 121.0183 - val_mae: 9.3694\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 262.9081 - mae: 13.3833 - val_loss: 120.8112 - val_mae: 9.4017\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 278.1391 - mae: 13.7337 - val_loss: 120.2833 - val_mae: 9.3880\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 282.0657 - mae: 13.9615 - val_loss: 120.1478 - val_mae: 9.3636\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 275.0324 - mae: 13.8751 - val_loss: 120.1361 - val_mae: 9.3314\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 272.7975 - mae: 13.7311 - val_loss: 120.3598 - val_mae: 9.2909\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 272.2076 - mae: 13.8922 - val_loss: 121.0577 - val_mae: 9.3064\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 287.3860 - mae: 14.3595 - val_loss: 121.9490 - val_mae: 9.3367\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 270.0226 - mae: 13.8320 - val_loss: 122.7411 - val_mae: 9.3570\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 272.8187 - mae: 13.9304 - val_loss: 123.1989 - val_mae: 9.3656\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 276.5070 - mae: 13.9925 - val_loss: 122.2564 - val_mae: 9.3387\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 280.3820 - mae: 14.0903 - val_loss: 120.1745 - val_mae: 9.2720\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 284.5472 - mae: 14.2258 - val_loss: 118.7471 - val_mae: 9.2366\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 265.1559 - mae: 13.5744 - val_loss: 118.1640 - val_mae: 9.2578\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 265.2298 - mae: 13.6233 - val_loss: 117.8931 - val_mae: 9.2543\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 267.9902 - mae: 13.4987 - val_loss: 117.7125 - val_mae: 9.2395\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 261.5801 - mae: 13.4403 - val_loss: 117.5108 - val_mae: 9.2264\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 279.7264 - mae: 14.0503 - val_loss: 117.3326 - val_mae: 9.2110\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 268.6804 - mae: 13.6662 - val_loss: 117.0808 - val_mae: 9.2025\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step\n",
            "Peores 5 taxistas con carreras más cortas en promedio:\n",
            "    Taxista_Label  Promedio_Duracion\n",
            "28             28          31.455099\n",
            "38             38          31.476603\n",
            "30             30          31.523190\n",
            "11             11          31.699433\n",
            "32             32          31.748330\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 117.0808 - mae: 9.2025\n",
            "Pérdida: 117.08077239990234, MAE: 9.20254898071289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.\tCarreras en un rango de fechas:"
      ],
      "metadata": {
        "id": "9lmsBimdivja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "\n",
        "# Convertimos la columna de fecha a datetime\n",
        "df['Fecha_Carrera'] = pd.to_datetime(df['Fecha_Carrera']) # Changed 'Fecha' to 'Fecha_Carrera'\n",
        "\n",
        "# Filtrar las carreras entre 2010 y 2015\n",
        "df_filtered = df[(df['Fecha_Carrera'].dt.year >= 2010) & (df['Fecha_Carrera'].dt.year <= 2015)]\n",
        "\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df_filtered['Taxista_Label'] = label_encoder.fit_transform(df_filtered['Codigo_Taxista'])\n",
        "\n",
        "# Contar el número de carreras por taxista\n",
        "carreras_por_taxista = df_filtered.groupby('Taxista_Label').size().reset_index(name='Total_Carreras')\n",
        "\n",
        "# Obtener los 10 taxistas con menos carreras\n",
        "peores_10_taxistas = carreras_por_taxista.nsmallest(10, 'Total_Carreras')\n",
        "\n",
        "# Datos de entrada (Taxista_Label) y de salida (Total_Carreras)\n",
        "X = carreras_por_taxista[['Taxista_Label']].values\n",
        "y = carreras_por_taxista['Total_Carreras'].values\n",
        "\n",
        "# Dividimos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predecir las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "\n",
        "# Mostrar los 10 taxistas con menos carreras\n",
        "print(\"Peores 10 taxistas con menos carreras:\")\n",
        "print(peores_10_taxistas)\n",
        "\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')\n",
        "\n",
        "# Encontrar el taxista que hizo más carreras en ese período\n",
        "mejor_taxista = carreras_por_taxista.loc[carreras_por_taxista['Total_Carreras'].idxmax()]\n",
        "\n",
        "print(\"El taxista que hizo más carreras entre 2010 y 2015 es:\", mejor_taxista['Taxista_Label'])\n",
        "print(\"Número de carreras:\", mejor_taxista['Total_Carreras'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsMKOIztixFy",
        "outputId": "7da9a4b7-42db-488f-e5fd-fff4a9a4ac18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-2ef27742d570>:18: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_filtered['Taxista_Label'] = label_encoder.fit_transform(df_filtered['Codigo_Taxista'])\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 196ms/step - loss: 65397.3867 - mae: 244.5272 - val_loss: 54200.2617 - val_mae: 228.0643\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 62621.1914 - mae: 239.0536 - val_loss: 53486.1250 - val_mae: 226.4781\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 63052.6836 - mae: 239.6353 - val_loss: 52770.5508 - val_mae: 224.8758\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 61697.4766 - mae: 236.4462 - val_loss: 52087.3945 - val_mae: 223.3336\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 61196.7773 - mae: 235.9245 - val_loss: 51497.8320 - val_mae: 221.9927\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 62203.8281 - mae: 237.5238 - val_loss: 50937.3633 - val_mae: 220.7093\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 62091.0781 - mae: 237.0024 - val_loss: 50371.6758 - val_mae: 219.4052\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 62424.9883 - mae: 237.9444 - val_loss: 49818.3438 - val_mae: 218.1208\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 62211.4961 - mae: 237.7825 - val_loss: 49281.9180 - val_mae: 216.8671\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 61268.4453 - mae: 235.3657 - val_loss: 48740.8438 - val_mae: 215.5941\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 59257.2891 - mae: 230.8738 - val_loss: 48192.9375 - val_mae: 214.2961\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 59080.0859 - mae: 230.9495 - val_loss: 47634.2930 - val_mae: 212.9632\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 61569.3945 - mae: 236.4002 - val_loss: 47069.0195 - val_mae: 211.6044\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 59522.6602 - mae: 231.2624 - val_loss: 46507.6641 - val_mae: 210.2450\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 58383.0273 - mae: 228.6304 - val_loss: 45937.6758 - val_mae: 208.8542\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 58007.6602 - mae: 228.0489 - val_loss: 45358.9766 - val_mae: 207.4310\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 56619.2852 - mae: 225.8322 - val_loss: 44768.4180 - val_mae: 205.9668\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 57309.0039 - mae: 226.8570 - val_loss: 44175.8203 - val_mae: 204.4850\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 57010.2188 - mae: 226.1947 - val_loss: 43580.6836 - val_mae: 202.9844\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 55879.5898 - mae: 223.6281 - val_loss: 42978.2578 - val_mae: 201.4520\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 56118.7969 - mae: 223.4497 - val_loss: 42363.7734 - val_mae: 199.8747\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 55599.0703 - mae: 222.1946 - val_loss: 41731.1016 - val_mae: 198.2353\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 54208.5078 - mae: 218.9225 - val_loss: 41073.0508 - val_mae: 196.5130\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 54479.4961 - mae: 219.4668 - val_loss: 40387.8477 - val_mae: 194.7007\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 54536.0586 - mae: 219.7530 - val_loss: 39686.1992 - val_mae: 192.8240\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 52407.4570 - mae: 215.2572 - val_loss: 38983.5664 - val_mae: 190.9228\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 51860.2344 - mae: 212.9671 - val_loss: 38273.1875 - val_mae: 188.9776\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 52016.1172 - mae: 213.4778 - val_loss: 37535.5703 - val_mae: 186.9324\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 52607.6016 - mae: 214.9401 - val_loss: 36775.4766 - val_mae: 184.7966\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 48204.1016 - mae: 204.7848 - val_loss: 36001.8945 - val_mae: 182.5924\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - loss: 49836.4023 - mae: 207.9635 - val_loss: 35224.0703 - val_mae: 180.3436\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 48852.7344 - mae: 205.2807 - val_loss: 34437.7188 - val_mae: 178.0356\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 49289.3320 - mae: 206.2034 - val_loss: 33630.8984 - val_mae: 175.6296\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 47748.1758 - mae: 202.2740 - val_loss: 32810.3203 - val_mae: 173.1413\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 45147.7734 - mae: 196.4310 - val_loss: 31990.1719 - val_mae: 170.6104\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 45989.7656 - mae: 197.4034 - val_loss: 31162.9629 - val_mae: 168.0110\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 43200.2188 - mae: 190.0982 - val_loss: 30304.8867 - val_mae: 165.2620\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 44618.3750 - mae: 192.6158 - val_loss: 29434.9414 - val_mae: 162.4169\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step - loss: 43182.0352 - mae: 189.5360 - val_loss: 28564.8789 - val_mae: 159.5093\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 44336.9336 - mae: 191.5102 - val_loss: 27686.8906 - val_mae: 156.5077\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 42475.5391 - mae: 186.7720 - val_loss: 26813.8633 - val_mae: 153.4510\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 40101.2539 - mae: 181.3811 - val_loss: 25951.7969 - val_mae: 150.3570\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 39317.8203 - mae: 177.6871 - val_loss: 25087.9570 - val_mae: 147.1751\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 39111.8516 - mae: 177.5428 - val_loss: 24212.5078 - val_mae: 143.8603\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 37697.2305 - mae: 173.3481 - val_loss: 23346.0684 - val_mae: 140.4821\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 38094.8086 - mae: 171.7793 - val_loss: 22493.9277 - val_mae: 137.0565\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 36944.0586 - mae: 169.2914 - val_loss: 21652.0273 - val_mae: 133.5615\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 36065.2070 - mae: 166.4430 - val_loss: 20834.1133 - val_mae: 130.0500\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 37458.9648 - mae: 168.3385 - val_loss: 20042.9414 - val_mae: 126.5328\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 34542.5625 - mae: 159.4576 - val_loss: 19274.9941 - val_mae: 122.9925\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 35654.3242 - mae: 161.4769 - val_loss: 18518.6035 - val_mae: 119.3695\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 32406.5117 - mae: 153.6528 - val_loss: 17775.3652 - val_mae: 115.6731\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 33582.7656 - mae: 156.3734 - val_loss: 17051.0664 - val_mae: 113.0445\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 33181.8281 - mae: 156.1057 - val_loss: 16358.5234 - val_mae: 110.4135\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 30106.2500 - mae: 146.9767 - val_loss: 15703.1045 - val_mae: 108.2095\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 31661.4277 - mae: 149.5948 - val_loss: 15074.0020 - val_mae: 106.7963\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 30548.1348 - mae: 147.0199 - val_loss: 14471.3496 - val_mae: 105.3681\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 30165.3281 - mae: 145.1369 - val_loss: 13896.2031 - val_mae: 103.9250\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 27942.4297 - mae: 139.3230 - val_loss: 13347.0908 - val_mae: 102.4601\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 29855.6211 - mae: 143.8898 - val_loss: 12830.5449 - val_mae: 100.9879\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 29240.5371 - mae: 142.8260 - val_loss: 12352.2588 - val_mae: 99.5251\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 28197.1328 - mae: 140.2019 - val_loss: 11921.1416 - val_mae: 98.1045\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 27625.6641 - mae: 137.5601 - val_loss: 11531.8154 - val_mae: 96.7171\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 28576.2891 - mae: 140.0968 - val_loss: 11179.2686 - val_mae: 95.3520\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 26835.1738 - mae: 135.3830 - val_loss: 10868.8105 - val_mae: 94.0392\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 26099.3691 - mae: 132.8478 - val_loss: 10597.1982 - val_mae: 92.7795\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 26807.9941 - mae: 134.5027 - val_loss: 10350.6211 - val_mae: 91.5160\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 27260.4102 - mae: 136.2105 - val_loss: 10137.7969 - val_mae: 90.3006\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 26812.9258 - mae: 134.5848 - val_loss: 9958.9453 - val_mae: 89.6836\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 24970.8320 - mae: 130.6172 - val_loss: 9802.2939 - val_mae: 89.3446\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 25570.8496 - mae: 130.6473 - val_loss: 9662.9062 - val_mae: 88.9956\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 24139.3496 - mae: 128.2220 - val_loss: 9543.1562 - val_mae: 88.6379\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 24955.2637 - mae: 130.9763 - val_loss: 9449.9395 - val_mae: 88.2934\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 25208.5820 - mae: 131.8353 - val_loss: 9382.4766 - val_mae: 87.9725\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 25466.7109 - mae: 131.7374 - val_loss: 9337.5586 - val_mae: 87.6842\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 24807.1953 - mae: 130.4078 - val_loss: 9309.5312 - val_mae: 87.4235\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 24329.9160 - mae: 129.6642 - val_loss: 9293.5732 - val_mae: 87.1679\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 25007.7520 - mae: 131.6646 - val_loss: 9288.8232 - val_mae: 86.9248\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 25233.5391 - mae: 130.3084 - val_loss: 9292.4883 - val_mae: 86.7159\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 24997.8789 - mae: 130.4759 - val_loss: 9302.6816 - val_mae: 86.5173\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 23733.0508 - mae: 126.3828 - val_loss: 9319.0537 - val_mae: 86.3219\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 25107.6074 - mae: 130.2738 - val_loss: 9338.9248 - val_mae: 86.1445\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 23353.2949 - mae: 124.7203 - val_loss: 9360.8984 - val_mae: 85.9833\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 24810.2500 - mae: 128.6819 - val_loss: 9382.4922 - val_mae: 85.8430\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 24018.9238 - mae: 126.6264 - val_loss: 9404.0801 - val_mae: 85.7160\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 24641.1250 - mae: 127.9133 - val_loss: 9424.5098 - val_mae: 85.6030\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 24042.5059 - mae: 126.6261 - val_loss: 9440.0225 - val_mae: 85.5157\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 24028.3262 - mae: 126.6192 - val_loss: 9455.5488 - val_mae: 85.4316\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 23247.7070 - mae: 123.9523 - val_loss: 9474.3379 - val_mae: 85.3389\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 23844.6250 - mae: 124.7793 - val_loss: 9488.5146 - val_mae: 85.2648\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 23632.9199 - mae: 125.5002 - val_loss: 9496.9326 - val_mae: 85.2111\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 22534.3555 - mae: 122.5148 - val_loss: 9496.4658 - val_mae: 85.1863\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 24718.7676 - mae: 129.4667 - val_loss: 9495.0166 - val_mae: 85.1644\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 24775.9531 - mae: 128.5517 - val_loss: 9491.5410 - val_mae: 85.1498\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 24495.4043 - mae: 127.3126 - val_loss: 9485.8076 - val_mae: 85.1430\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 25420.3926 - mae: 131.5951 - val_loss: 9487.0918 - val_mae: 85.1139\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 22807.9414 - mae: 124.2956 - val_loss: 9491.1514 - val_mae: 85.0764\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 23636.3770 - mae: 124.9119 - val_loss: 9498.4561 - val_mae: 85.0279\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 22628.9844 - mae: 122.8158 - val_loss: 9509.8359 - val_mae: 84.9668\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 22629.2871 - mae: 124.8723 - val_loss: 9522.8545 - val_mae: 84.9012\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f0a45679900> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "Peores 10 taxistas con menos carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "11             11             124\n",
            "16             16             124\n",
            "38             38             139\n",
            "7               7             140\n",
            "40             40             140\n",
            "4               4             155\n",
            "29             29             155\n",
            "42             42             158\n",
            "28             28             169\n",
            "33             33             172\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 9522.8545 - mae: 84.9012\n",
            "Pérdida: 9522.8544921875, MAE: 84.90115356445312\n",
            "El taxista que hizo más carreras entre 2010 y 2015 es: 0\n",
            "Número de carreras: 373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.\tTaxistas más consistentes:"
      ],
      "metadata": {
        "id": "o4kt8x39sZ4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Generación de datos\n",
        "def generar_datos_radio_taxi():\n",
        "    data = []\n",
        "    codigos_taxistas = [f'AS-{str(i).zfill(3)}' for i in range(1, 51)]  # 50 taxistas\n",
        "    inicio_fecha = datetime(2001, 1, 1)\n",
        "\n",
        "    for taxista in codigos_taxistas:\n",
        "        num_carreras = random.randint(500, 1500)  # Cada taxista tiene entre 500 y 1500 carreras\n",
        "\n",
        "        for i in range(num_carreras):\n",
        "            fecha_carrera = inicio_fecha + timedelta(days=random.randint(0, (datetime(2024, 12, 31) - inicio_fecha).days))\n",
        "            duracion_carrera = random.randint(5, 60)  # Duración de la carrera entre 5 y 60 minutos\n",
        "            data.append([taxista, i + 1, fecha_carrera.strftime('%Y-%m-%d'), duracion_carrera])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['Codigo_Taxista', 'Numero_Carrera', 'Fecha_Carrera', 'Duracion_Carrera'])\n",
        "    df['Fecha_Carrera'] = pd.to_datetime(df['Fecha_Carrera'])\n",
        "    df['Año_Carrera'] = df['Fecha_Carrera'].dt.year\n",
        "\n",
        "    # Carreras en 2020\n",
        "    carreras_2020 = df[df['Año_Carrera'] == 2020]\n",
        "    carreras_por_taxista_2020 = carreras_2020.groupby('Codigo_Taxista').size().reset_index(name='Total_Carreras')\n",
        "    top_taxistas_2020 = carreras_por_taxista_2020.nlargest(3, 'Total_Carreras')\n",
        "\n",
        "    df.to_csv('radio_taxi_carreras.csv', index=False)\n",
        "    return df, top_taxistas_2020\n",
        "\n",
        "# Generar y visualizar los primeros datos\n",
        "df, top_taxistas_2020 = generar_datos_radio_taxi()\n",
        "print(df.head())\n",
        "print(top_taxistas_2020)\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "\n",
        "# Contar el número de carreras por taxista\n",
        "carreras_por_taxista = df.groupby('Taxista_Label').size().reset_index(name='Total_Carreras')\n",
        "top_3_taxistas = carreras_por_taxista.nlargest(3, 'Total_Carreras')\n",
        "peores_10_taxistas = carreras_por_taxista.nsmallest(10, 'Total_Carreras')\n",
        "\n",
        "# Datos de entrada (Taxista_Label) y de salida (Total_Carreras)\n",
        "X = carreras_por_taxista[['Taxista_Label']].values\n",
        "y = carreras_por_taxista['Total_Carreras'].values\n",
        "\n",
        "# Dividimos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predecir las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "\n",
        "# Mostrar los 3 taxistas con más carreras\n",
        "print(\"Top 3 taxistas con más carreras:\")\n",
        "print(top_3_taxistas)\n",
        "\n",
        "# Mostrar los 10 taxistas con menos carreras\n",
        "print(\"Los 10 taxistas con peores carreras:\")\n",
        "print(peores_10_taxistas)\n",
        "\n",
        "# Mostrar taxistas con más carreras en 2020\n",
        "print(\"Los taxistas con más carreras del 2020:\")\n",
        "print(top_taxistas_2020)\n",
        "\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')\n",
        "\n",
        "# Encontrar a los taxistas más consistentes\n",
        "consistencia_taxistas = df.groupby('Codigo_Taxista')['Duracion_Carrera'].agg(['mean', 'std']).reset_index()\n",
        "consistencia_taxistas = consistencia_taxistas.sort_values(by='std')\n",
        "taxistas_consistentes = consistencia_taxistas.head(3)\n",
        "\n",
        "# Mostrar los taxistas más consistentes\n",
        "print(\"Los taxistas más consistentes en la duración de sus carreras:\")\n",
        "print(taxistas_consistentes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKYOQErUsWZ_",
        "outputId": "e270a765-2b63-4f31-d7b0-d195e45ebd0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Codigo_Taxista  Numero_Carrera Fecha_Carrera  Duracion_Carrera  Año_Carrera\n",
            "0         AS-001               1    2008-02-18                38         2008\n",
            "1         AS-001               2    2003-10-30                53         2003\n",
            "2         AS-001               3    2022-10-20                24         2022\n",
            "3         AS-001               4    2010-08-20                30         2010\n",
            "4         AS-001               5    2008-01-19                44         2008\n",
            "   Codigo_Taxista  Total_Carreras\n",
            "34         AS-035              70\n",
            "33         AS-034              68\n",
            "21         AS-022              65\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - loss: 1065200.1250 - mae: 989.5840 - val_loss: 1028612.6875 - val_mae: 992.9603\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1056712.0000 - mae: 984.8294 - val_loss: 1026127.3125 - val_mae: 991.7416\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1039679.8750 - mae: 974.3996 - val_loss: 1023606.8125 - val_mae: 990.5041\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1027144.3125 - mae: 971.1680 - val_loss: 1021113.0000 - val_mae: 989.2781\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 1008318.8125 - mae: 960.1010 - val_loss: 1018714.8125 - val_mae: 988.0972\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1035688.5625 - mae: 972.9694 - val_loss: 1016239.6875 - val_mae: 986.8770\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1028106.1250 - mae: 970.3125 - val_loss: 1013936.8125 - val_mae: 985.7402\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1030295.7500 - mae: 972.0890 - val_loss: 1011666.8125 - val_mae: 984.6180\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1038802.1250 - mae: 976.7194 - val_loss: 1009587.1875 - val_mae: 983.5892\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1007772.0000 - mae: 960.9119 - val_loss: 1007525.5000 - val_mae: 982.5677\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1042172.8125 - mae: 976.8910 - val_loss: 1005391.1875 - val_mae: 981.5090\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1031867.8125 - mae: 971.4326 - val_loss: 1003208.1875 - val_mae: 980.4246\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1015065.6875 - mae: 964.3242 - val_loss: 1000997.6250 - val_mae: 979.3253\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1028352.1875 - mae: 969.8446 - val_loss: 998781.1875 - val_mae: 978.2219\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 999808.6875 - mae: 956.7958 - val_loss: 996473.6250 - val_mae: 977.0713\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 998551.8750 - mae: 955.9994 - val_loss: 994072.6250 - val_mae: 975.8727\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 995552.8125 - mae: 953.8245 - val_loss: 991588.3750 - val_mae: 974.6307\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1006807.2500 - mae: 959.4157 - val_loss: 989110.6250 - val_mae: 973.3906\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1005688.1250 - mae: 960.9110 - val_loss: 986721.1875 - val_mae: 972.1926\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1020242.8750 - mae: 966.6804 - val_loss: 984272.8125 - val_mae: 970.9634\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1005797.3125 - mae: 959.1862 - val_loss: 981666.3125 - val_mae: 969.6528\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1001508.9375 - mae: 957.1118 - val_loss: 978863.8125 - val_mae: 968.2413\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1033001.6875 - mae: 971.8552 - val_loss: 975871.0000 - val_mae: 966.7316\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1005132.6875 - mae: 957.1116 - val_loss: 972707.3125 - val_mae: 965.1330\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 965748.4375 - mae: 937.9810 - val_loss: 969341.5000 - val_mae: 963.4289\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 977748.5625 - mae: 943.0792 - val_loss: 965770.5000 - val_mae: 961.6172\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 991208.8125 - mae: 950.9773 - val_loss: 962038.1875 - val_mae: 959.7196\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1002640.2500 - mae: 957.0333 - val_loss: 958200.1875 - val_mae: 957.7640\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 974729.6875 - mae: 942.3237 - val_loss: 954285.6875 - val_mae: 955.7646\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 997073.6875 - mae: 953.6526 - val_loss: 950227.0000 - val_mae: 953.6869\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 977660.4375 - mae: 942.6176 - val_loss: 945993.3125 - val_mae: 951.5139\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 963087.4375 - mae: 938.0203 - val_loss: 941615.8750 - val_mae: 949.2614\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 987665.1875 - mae: 948.5859 - val_loss: 937129.8125 - val_mae: 946.9469\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 988200.1875 - mae: 949.0410 - val_loss: 932482.3750 - val_mae: 944.5424\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 983517.0000 - mae: 947.2499 - val_loss: 927655.8125 - val_mae: 942.0379\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 972972.0625 - mae: 939.7886 - val_loss: 922626.1875 - val_mae: 939.4201\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 934618.6875 - mae: 921.6157 - val_loss: 917400.8125 - val_mae: 936.6918\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 975262.0625 - mae: 943.0096 - val_loss: 911962.8125 - val_mae: 933.8431\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 966648.2500 - mae: 935.3721 - val_loss: 906355.6250 - val_mae: 930.8954\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 940726.8125 - mae: 924.8879 - val_loss: 900633.8125 - val_mae: 927.8768\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 928012.8125 - mae: 917.6734 - val_loss: 894784.3125 - val_mae: 924.7793\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 925465.0625 - mae: 916.3460 - val_loss: 888725.8125 - val_mae: 921.5588\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 949599.8750 - mae: 928.5250 - val_loss: 882417.8750 - val_mae: 918.1925\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 930995.1250 - mae: 918.3923 - val_loss: 875935.5000 - val_mae: 914.7186\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 896994.3125 - mae: 901.9196 - val_loss: 869247.3750 - val_mae: 911.1187\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 927115.3750 - mae: 915.6743 - val_loss: 862231.0000 - val_mae: 907.3251\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 940771.9375 - mae: 922.4005 - val_loss: 855027.8125 - val_mae: 903.4118\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 883252.8750 - mae: 893.0372 - val_loss: 847648.3750 - val_mae: 899.3830\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 885195.6250 - mae: 894.0968 - val_loss: 839972.3125 - val_mae: 895.1707\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 903830.3750 - mae: 903.8942 - val_loss: 832097.6250 - val_mae: 890.8260\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 893164.1875 - mae: 897.5011 - val_loss: 824056.0000 - val_mae: 886.3647\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 863466.7500 - mae: 882.6257 - val_loss: 815960.0000 - val_mae: 881.8477\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 841405.3125 - mae: 870.2567 - val_loss: 807699.3750 - val_mae: 877.2116\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 831302.2500 - mae: 864.1002 - val_loss: 799070.3750 - val_mae: 872.3392\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 849669.3125 - mae: 873.6984 - val_loss: 790136.7500 - val_mae: 867.2624\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 818957.1250 - mae: 858.6354 - val_loss: 781001.4375 - val_mae: 862.0361\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 851490.0625 - mae: 874.1596 - val_loss: 771738.5000 - val_mae: 856.7002\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 826194.3125 - mae: 859.2173 - val_loss: 762401.0000 - val_mae: 851.2829\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 839549.6875 - mae: 865.6180 - val_loss: 752944.6875 - val_mae: 845.7570\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 827755.5000 - mae: 859.3955 - val_loss: 743430.1250 - val_mae: 840.1553\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 820931.3125 - mae: 858.2994 - val_loss: 733806.3125 - val_mae: 834.4463\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - loss: 786788.4375 - mae: 836.3220 - val_loss: 723970.1250 - val_mae: 828.5651\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 810001.1875 - mae: 847.7691 - val_loss: 713921.1250 - val_mae: 822.5075\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 782733.5000 - mae: 832.1235 - val_loss: 703807.0000 - val_mae: 816.3592\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 767694.0625 - mae: 822.9131 - val_loss: 693488.9375 - val_mae: 810.0323\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 803623.9375 - mae: 842.8256 - val_loss: 682889.3750 - val_mae: 803.4738\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 767118.6250 - mae: 822.0150 - val_loss: 672101.6250 - val_mae: 796.7357\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 757767.3750 - mae: 815.2833 - val_loss: 661056.3750 - val_mae: 789.7690\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 741103.9375 - mae: 806.3237 - val_loss: 649812.1875 - val_mae: 782.6041\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 755312.8750 - mae: 815.3994 - val_loss: 638350.0625 - val_mae: 775.2225\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 741327.4375 - mae: 805.1902 - val_loss: 626719.1875 - val_mae: 767.6493\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 743020.7500 - mae: 807.2081 - val_loss: 615074.0625 - val_mae: 759.9802\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 698867.1250 - mae: 781.2074 - val_loss: 603442.1875 - val_mae: 752.2303\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 703209.0000 - mae: 782.4322 - val_loss: 591728.6250 - val_mae: 744.3323\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 716861.5625 - mae: 787.5718 - val_loss: 579935.1250 - val_mae: 736.2819\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 715069.6250 - mae: 784.0646 - val_loss: 568079.3125 - val_mae: 728.0856\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 701398.8750 - mae: 774.1279 - val_loss: 556068.1875 - val_mae: 719.6719\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 692153.1875 - mae: 770.9786 - val_loss: 543998.8125 - val_mae: 711.1014\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 684671.0000 - mae: 767.1400 - val_loss: 531962.3750 - val_mae: 702.4337\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 655166.4375 - mae: 750.9588 - val_loss: 520057.5938 - val_mae: 693.7371\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 623411.7500 - mae: 723.2116 - val_loss: 508276.0625 - val_mae: 685.0041\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 649913.1875 - mae: 734.9732 - val_loss: 496409.9062 - val_mae: 676.0755\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 658409.2500 - mae: 742.5942 - val_loss: 484579.4062 - val_mae: 667.0347\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 593465.1250 - mae: 700.8848 - val_loss: 472730.3438 - val_mae: 657.8337\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 582534.7500 - mae: 694.1770 - val_loss: 460661.5000 - val_mae: 648.3046\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 610863.5625 - mae: 709.1590 - val_loss: 448541.9375 - val_mae: 638.5670\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 568171.2500 - mae: 678.3248 - val_loss: 436511.0000 - val_mae: 628.7245\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 590172.7500 - mae: 691.5319 - val_loss: 424366.0938 - val_mae: 618.6007\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 562673.5625 - mae: 670.8900 - val_loss: 412166.1875 - val_mae: 608.2296\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 548291.6875 - mae: 665.2421 - val_loss: 400101.3125 - val_mae: 597.7624\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 553759.0000 - mae: 665.5436 - val_loss: 388340.4375 - val_mae: 587.3445\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 565108.1875 - mae: 670.8697 - val_loss: 376642.4688 - val_mae: 576.7582\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 541010.6875 - mae: 647.7129 - val_loss: 365125.2812 - val_mae: 566.1027\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 521485.6875 - mae: 637.1848 - val_loss: 353888.3125 - val_mae: 555.4683\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 516282.1562 - mae: 633.1794 - val_loss: 342886.9062 - val_mae: 544.8131\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 501444.0938 - mae: 621.9370 - val_loss: 332094.2500 - val_mae: 534.1083\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 499640.0312 - mae: 619.1365 - val_loss: 321289.2812 - val_mae: 523.1224\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 471243.1250 - mae: 596.5710 - val_loss: 310493.1562 - val_mae: 511.8557\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 481777.5625 - mae: 604.1889 - val_loss: 299891.6875 - val_mae: 500.4870\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 469033.0938 - mae: 591.4817 - val_loss: 289729.0625 - val_mae: 489.2808\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x7f0a456fd630> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "Top 3 taxistas con más carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "21             21            1495\n",
            "34             34            1476\n",
            "5               5            1464\n",
            "Los 10 taxistas con peores carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "7               7             556\n",
            "46             46             574\n",
            "4               4             595\n",
            "2               2             605\n",
            "20             20             633\n",
            "36             36             637\n",
            "26             26             656\n",
            "43             43             671\n",
            "8               8             674\n",
            "14             14             701\n",
            "Los taxistas con más carreras del 2020:\n",
            "   Codigo_Taxista  Total_Carreras\n",
            "34         AS-035              70\n",
            "33         AS-034              68\n",
            "21         AS-022              65\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 289729.0625 - mae: 489.2808\n",
            "Pérdida: 289729.0625, MAE: 489.28076171875\n",
            "Los taxistas más consistentes en la duración de sus carreras:\n",
            "   Codigo_Taxista       mean        std\n",
            "24         AS-025  32.125329  15.512730\n",
            "6          AS-007  32.571819  15.533304\n",
            "16         AS-017  32.341991  15.715459\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  6.\tAgregar datos de propinas"
      ],
      "metadata": {
        "id": "15nOe4-KtZ2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Generación de datos\n",
        "def generar_datos_radio_taxi():\n",
        "    data = []\n",
        "    codigos_taxistas = [f'AS-{str(i).zfill(3)}' for i in range(1, 51)]  # 50 taxistas\n",
        "    inicio_fecha = datetime(2001, 1, 1)\n",
        "\n",
        "    for taxista in codigos_taxistas:\n",
        "        num_carreras = random.randint(500, 1500)  # Cada taxista tiene entre 500 y 1500 carreras\n",
        "\n",
        "        for i in range(num_carreras):\n",
        "            fecha_carrera = inicio_fecha + timedelta(days=random.randint(0, (datetime(2024, 12, 31) - inicio_fecha).days))\n",
        "            duracion_carrera = random.randint(5, 60)  # Duración de la carrera entre 5 y 60 minutos\n",
        "            data.append([taxista, i + 1, fecha_carrera.strftime('%Y-%m-%d'), duracion_carrera])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['Codigo_Taxista', 'Numero_Carrera', 'Fecha_Carrera', 'Duracion_Carrera'])\n",
        "    df['Fecha_Carrera'] = pd.to_datetime(df['Fecha_Carrera'])\n",
        "    df['Año_Carrera'] = df['Fecha_Carrera'].dt.year\n",
        "\n",
        "    carreras_2020 = df[df['Año_Carrera'] == 2020]\n",
        "    carreras_por_taxista_2020 = carreras_2020.groupby('Codigo_Taxista').size().reset_index(name='Total_Carreras')\n",
        "    top_taxistas_2020 = carreras_por_taxista_2020.nlargest(3, 'Total_Carreras')\n",
        "\n",
        "    df.to_csv('radio_taxi_carreras.csv', index=False)\n",
        "    return df, top_taxistas_2020\n",
        "\n",
        "# Generar y visualizar los primeros datos\n",
        "df, top_taxistas_2020 = generar_datos_radio_taxi()\n",
        "print(df.head())\n",
        "print(top_taxistas_2020)\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "\n",
        "# Agregar columna de propinas aleatorias entre 0 y 10 soles\n",
        "df['Propina'] = np.random.uniform(0, 10, size=len(df))\n",
        "\n",
        "# Calcular la suma total de propinas por taxista\n",
        "propinas_por_taxista = df.groupby('Codigo_Taxista')['Propina'].sum().reset_index(name='Total_Propinas')\n",
        "\n",
        "# Identificar los taxistas con las mayores propinas\n",
        "top_taxistas_propinas = propinas_por_taxista.nlargest(3, 'Total_Propinas')\n",
        "\n",
        "# Mostrar los taxistas con mayores propinas\n",
        "print(\"Los taxistas con mayor cantidad de propinas:\")\n",
        "print(top_taxistas_propinas)\n",
        "\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "\n",
        "# Contar el número de carreras por taxista\n",
        "carreras_por_taxista = df.groupby('Taxista_Label').size().reset_index(name='Total_Carreras')\n",
        "top_3_taxistas = carreras_por_taxista.nlargest(3, 'Total_Carreras')\n",
        "peores_10_taxistas = carreras_por_taxista.nsmallest(10, 'Total_Carreras')\n",
        "\n",
        "# Datos de entrada (Taxista_Label) y de salida (Total_Carreras)\n",
        "X = carreras_por_taxista[['Taxista_Label']].values\n",
        "y = carreras_por_taxista['Total_Carreras'].values\n",
        "\n",
        "# Dividimos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predecir las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "\n",
        "# Mostrar los 3 taxistas con más carreras\n",
        "print(\"Top 3 taxistas con más carreras:\")\n",
        "print(top_3_taxistas)\n",
        "\n",
        "# Mostrar los 10 taxistas con menos carreras\n",
        "print(\"Los 10 taxistas con peores carreras:\")\n",
        "print(peores_10_taxistas)\n",
        "\n",
        "# Mostrar taxistas con más carreras en 2020\n",
        "print(\"Los taxistas con más carreras del 2020:\")\n",
        "print(top_taxistas_2020)\n",
        "\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')\n",
        "\n",
        "# Encontrar a los taxistas más consistentes\n",
        "consistencia_taxistas = df.groupby('Codigo_Taxista')['Duracion_Carrera'].agg(['mean', 'std']).reset_index()\n",
        "consistencia_taxistas = consistencia_taxistas.sort_values(by='std')\n",
        "taxistas_consistentes = consistencia_taxistas.head(3)\n",
        "\n",
        "# Mostrar los taxistas más consistentes\n",
        "print(\"Los taxistas más consistentes en la duración de sus carreras:\")\n",
        "print(taxistas_consistentes)\n"
      ],
      "metadata": {
        "id": "Cn-nQ_-RtYXx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f819939-12a2-482f-aee2-4553c06d3ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Codigo_Taxista  Numero_Carrera Fecha_Carrera  Duracion_Carrera  Año_Carrera\n",
            "0         AS-001               1    2009-08-18                14         2009\n",
            "1         AS-001               2    2014-01-28                21         2014\n",
            "2         AS-001               3    2017-04-23                53         2017\n",
            "3         AS-001               4    2006-06-25                19         2006\n",
            "4         AS-001               5    2023-12-17                16         2023\n",
            "   Codigo_Taxista  Total_Carreras\n",
            "47         AS-048              74\n",
            "8          AS-009              68\n",
            "24         AS-025              68\n",
            "Los taxistas con mayor cantidad de propinas:\n",
            "   Codigo_Taxista  Total_Propinas\n",
            "13         AS-014     7469.176673\n",
            "41         AS-042     7201.541912\n",
            "40         AS-041     7062.147451\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 189ms/step - loss: 1169893.6250 - mae: 1046.6531 - val_loss: 1245336.2500 - val_mae: 1089.9281\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1146496.5000 - mae: 1034.9723 - val_loss: 1242318.0000 - val_mae: 1088.5164\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1153961.0000 - mae: 1037.1383 - val_loss: 1239834.2500 - val_mae: 1087.3528\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1140090.0000 - mae: 1031.5046 - val_loss: 1237499.6250 - val_mae: 1086.2578\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1160062.3750 - mae: 1042.0824 - val_loss: 1235329.7500 - val_mae: 1085.2390\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1139081.7500 - mae: 1030.5081 - val_loss: 1233181.3750 - val_mae: 1084.2292\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1140734.2500 - mae: 1032.7634 - val_loss: 1231030.7500 - val_mae: 1083.2174\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1160921.3750 - mae: 1041.8801 - val_loss: 1228873.2500 - val_mae: 1082.2010\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1128712.7500 - mae: 1025.6852 - val_loss: 1226716.1250 - val_mae: 1081.1838\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1150580.1250 - mae: 1037.0211 - val_loss: 1224991.1250 - val_mae: 1080.3688\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1167319.5000 - mae: 1045.4974 - val_loss: 1223634.7500 - val_mae: 1079.7278\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1123662.1250 - mae: 1023.0115 - val_loss: 1222282.7500 - val_mae: 1079.0887\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1147497.7500 - mae: 1033.9867 - val_loss: 1220879.8750 - val_mae: 1078.4250\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1135395.6250 - mae: 1028.1382 - val_loss: 1219433.3750 - val_mae: 1077.7404\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1127111.3750 - mae: 1025.1974 - val_loss: 1217953.0000 - val_mae: 1077.0388\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1138911.3750 - mae: 1031.4973 - val_loss: 1216428.3750 - val_mae: 1076.3162\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1141226.8750 - mae: 1033.1980 - val_loss: 1214756.0000 - val_mae: 1075.5231\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1127870.1250 - mae: 1026.5675 - val_loss: 1212937.0000 - val_mae: 1074.6592\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1133401.2500 - mae: 1029.1027 - val_loss: 1211011.2500 - val_mae: 1073.7438\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1157090.8750 - mae: 1039.7183 - val_loss: 1209105.5000 - val_mae: 1072.8367\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1114673.8750 - mae: 1020.3619 - val_loss: 1207149.2500 - val_mae: 1071.9050\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1148863.7500 - mae: 1035.7200 - val_loss: 1205205.7500 - val_mae: 1070.9778\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1134189.3750 - mae: 1030.1179 - val_loss: 1203311.7500 - val_mae: 1070.0739\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 1107061.5000 - mae: 1016.4559 - val_loss: 1201313.0000 - val_mae: 1069.1189\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1107098.3750 - mae: 1016.3319 - val_loss: 1199207.7500 - val_mae: 1068.1122\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 1104062.6250 - mae: 1014.1536 - val_loss: 1197028.0000 - val_mae: 1067.0684\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1135949.8750 - mae: 1031.3823 - val_loss: 1194759.0000 - val_mae: 1065.9808\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1130544.6250 - mae: 1027.4662 - val_loss: 1192408.2500 - val_mae: 1064.8525\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1122426.1250 - mae: 1022.9956 - val_loss: 1189992.2500 - val_mae: 1063.6917\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1111181.0000 - mae: 1018.1323 - val_loss: 1187490.3750 - val_mae: 1062.4879\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1104983.8750 - mae: 1015.3154 - val_loss: 1184888.7500 - val_mae: 1061.2346\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1098849.8750 - mae: 1012.5364 - val_loss: 1182151.0000 - val_mae: 1059.9138\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1105872.3750 - mae: 1014.2179 - val_loss: 1179281.0000 - val_mae: 1058.5271\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1105112.8750 - mae: 1014.8738 - val_loss: 1176302.3750 - val_mae: 1057.0857\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1103728.2500 - mae: 1014.3495 - val_loss: 1173201.2500 - val_mae: 1055.5824\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1073822.5000 - mae: 999.4373 - val_loss: 1169965.2500 - val_mae: 1054.0114\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1114292.3750 - mae: 1019.7031 - val_loss: 1166596.3750 - val_mae: 1052.3728\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1100879.6250 - mae: 1012.3049 - val_loss: 1163075.0000 - val_mae: 1050.6567\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1088187.7500 - mae: 1007.0810 - val_loss: 1159404.3750 - val_mae: 1048.8645\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1064666.7500 - mae: 994.4438 - val_loss: 1155571.6250 - val_mae: 1046.9895\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1097260.8750 - mae: 1010.9178 - val_loss: 1151549.8750 - val_mae: 1045.0176\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1072778.6250 - mae: 1000.5085 - val_loss: 1147376.2500 - val_mae: 1042.9667\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1098499.3750 - mae: 1012.5583 - val_loss: 1143032.6250 - val_mae: 1040.8271\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1075304.6250 - mae: 1001.4219 - val_loss: 1138531.2500 - val_mae: 1038.6047\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1091015.0000 - mae: 1009.0381 - val_loss: 1133912.2500 - val_mae: 1036.3181\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1074910.6250 - mae: 999.9004 - val_loss: 1129181.5000 - val_mae: 1033.9703\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1043281.8750 - mae: 984.5956 - val_loss: 1124237.5000 - val_mae: 1031.5101\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1039949.3750 - mae: 984.1508 - val_loss: 1119032.2500 - val_mae: 1028.9121\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1068011.5000 - mae: 997.0168 - val_loss: 1113649.3750 - val_mae: 1026.2177\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1065784.3750 - mae: 997.9191 - val_loss: 1108056.2500 - val_mae: 1023.4091\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1046998.7500 - mae: 985.6518 - val_loss: 1102235.7500 - val_mae: 1020.4767\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1050795.1250 - mae: 989.0023 - val_loss: 1096340.3750 - val_mae: 1017.4966\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1026019.1875 - mae: 977.8560 - val_loss: 1090267.0000 - val_mae: 1014.4157\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1043700.7500 - mae: 986.2185 - val_loss: 1083904.6250 - val_mae: 1011.1764\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 1029753.8750 - mae: 978.0381 - val_loss: 1077276.1250 - val_mae: 1007.7885\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1022184.7500 - mae: 975.0932 - val_loss: 1070384.6250 - val_mae: 1004.2520\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 1009583.7500 - mae: 968.2743 - val_loss: 1063257.0000 - val_mae: 1000.5789\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - loss: 1010930.1875 - mae: 970.0120 - val_loss: 1055867.7500 - val_mae: 996.7543\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - loss: 981644.6875 - mae: 953.2616 - val_loss: 1048300.1875 - val_mae: 992.8192\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 1008233.2500 - mae: 968.2610 - val_loss: 1040461.1875 - val_mae: 988.7238\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 1016032.9375 - mae: 972.6955 - val_loss: 1032513.1875 - val_mae: 984.5507\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 980957.0625 - mae: 954.8681 - val_loss: 1024358.1875 - val_mae: 980.2472\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 961932.7500 - mae: 942.2184 - val_loss: 1015850.3750 - val_mae: 975.7338\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 989723.3750 - mae: 958.4125 - val_loss: 1007042.1250 - val_mae: 971.0347\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 979091.2500 - mae: 953.0517 - val_loss: 998065.6875 - val_mae: 966.2181\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step - loss: 965861.9375 - mae: 946.2861 - val_loss: 989035.1875 - val_mae: 961.3438\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 963702.5625 - mae: 945.2100 - val_loss: 979796.5000 - val_mae: 956.3265\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 958021.8750 - mae: 941.9073 - val_loss: 970182.3750 - val_mae: 951.0721\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 950985.6875 - mae: 938.3263 - val_loss: 960219.8750 - val_mae: 945.5906\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 213ms/step - loss: 923927.5000 - mae: 922.2385 - val_loss: 950057.8125 - val_mae: 939.9601\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - loss: 919641.7500 - mae: 919.0956 - val_loss: 939790.3750 - val_mae: 934.2305\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 172ms/step - loss: 917266.5625 - mae: 918.2235 - val_loss: 929330.8125 - val_mae: 928.3502\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 913783.3125 - mae: 919.4028 - val_loss: 918582.1875 - val_mae: 922.2609\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 891726.2500 - mae: 905.6913 - val_loss: 907631.1250 - val_mae: 916.0071\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - loss: 876072.4375 - mae: 896.7988 - val_loss: 896420.0000 - val_mae: 909.5516\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 875310.6250 - mae: 895.1713 - val_loss: 884972.6250 - val_mae: 902.9030\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 847630.4375 - mae: 879.4816 - val_loss: 873306.0000 - val_mae: 896.0661\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 856384.0625 - mae: 884.8804 - val_loss: 861251.0000 - val_mae: 888.9354\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 858110.1875 - mae: 886.7025 - val_loss: 848945.8750 - val_mae: 881.5848\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 855628.8750 - mae: 887.0373 - val_loss: 836543.8125 - val_mae: 874.1011\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 840591.8125 - mae: 875.4727 - val_loss: 824116.6875 - val_mae: 866.5241\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 826129.9375 - mae: 866.0622 - val_loss: 811595.8750 - val_mae: 858.8080\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 805464.6875 - mae: 853.4916 - val_loss: 798946.3125 - val_mae: 850.9266\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 786579.6250 - mae: 841.2872 - val_loss: 786188.8125 - val_mae: 842.8872\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 774613.8750 - mae: 836.3177 - val_loss: 773315.2500 - val_mae: 834.6794\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 782444.0625 - mae: 839.5216 - val_loss: 760394.9375 - val_mae: 826.3418\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 783694.8750 - mae: 843.4011 - val_loss: 747286.8750 - val_mae: 817.7767\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 758761.8750 - mae: 826.1671 - val_loss: 734138.9375 - val_mae: 809.0739\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 741937.7500 - mae: 813.2617 - val_loss: 721069.4375 - val_mae: 800.3080\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 723562.4375 - mae: 801.0067 - val_loss: 707844.1250 - val_mae: 791.3157\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 714981.6250 - mae: 796.9741 - val_loss: 694448.0000 - val_mae: 782.0772\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 711405.0625 - mae: 792.3714 - val_loss: 681068.8125 - val_mae: 772.7140\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 691337.3750 - mae: 780.2309 - val_loss: 667704.3750 - val_mae: 763.2186\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 668987.7500 - mae: 767.8829 - val_loss: 654392.5625 - val_mae: 753.6127\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 671110.3750 - mae: 764.3984 - val_loss: 641103.1250 - val_mae: 743.8683\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 686945.1875 - mae: 777.2120 - val_loss: 627698.5000 - val_mae: 733.8749\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 647517.1875 - mae: 749.3940 - val_loss: 614130.6250 - val_mae: 723.5826\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 657387.5000 - mae: 756.2833 - val_loss: 600444.6250 - val_mae: 713.0106\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 636354.1875 - mae: 739.3307 - val_loss: 586779.1250 - val_mae: 702.2526\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 620468.5625 - mae: 728.9991 - val_loss: 573127.6250 - val_mae: 691.2926\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step\n",
            "Top 3 taxistas con más carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "13             13            1465\n",
            "41             41            1433\n",
            "38             38            1402\n",
            "Los 10 taxistas con peores carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "31             31             514\n",
            "3               3             528\n",
            "23             23             554\n",
            "16             16             579\n",
            "46             46             622\n",
            "7               7             696\n",
            "29             29             706\n",
            "0               0             715\n",
            "1               1             715\n",
            "25             25             720\n",
            "Los taxistas con más carreras del 2020:\n",
            "   Codigo_Taxista  Total_Carreras\n",
            "47         AS-048              74\n",
            "8          AS-009              68\n",
            "24         AS-025              68\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 573127.6250 - mae: 691.2926\n",
            "Pérdida: 573127.625, MAE: 691.2926025390625\n",
            "Los taxistas más consistentes en la duración de sus carreras:\n",
            "   Codigo_Taxista       mean        std\n",
            "23         AS-024  34.355596  15.617544\n",
            "25         AS-026  32.804167  15.676377\n",
            "39         AS-040  32.626028  15.717782\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7 7.\tTaxista más activo por año"
      ],
      "metadata": {
        "id": "lCnID5xRtdy1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "from datetime import datetime, timedelta\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Generación de datos\n",
        "def generar_datos_radio_taxi():\n",
        "    data = []\n",
        "    codigos_taxistas = [f'AS-{str(i).zfill(3)}' for i in range(1, 51)]  # 50 taxistas\n",
        "    inicio_fecha = datetime(2001, 1, 1)\n",
        "\n",
        "    for taxista in codigos_taxistas:\n",
        "        num_carreras = random.randint(500, 1500)  # Cada taxista tiene entre 500 y 1500 carreras\n",
        "\n",
        "        for i in range(num_carreras):\n",
        "            fecha_carrera = inicio_fecha + timedelta(days=random.randint(0, (datetime(2024, 12, 31) - inicio_fecha).days))\n",
        "            duracion_carrera = random.randint(5, 60)  # Duración de la carrera entre 5 y 60 minutos\n",
        "            data.append([taxista, i + 1, fecha_carrera.strftime('%Y-%m-%d'), duracion_carrera])\n",
        "\n",
        "    df = pd.DataFrame(data, columns=['Codigo_Taxista', 'Numero_Carrera', 'Fecha_Carrera', 'Duracion_Carrera'])\n",
        "    df['Fecha_Carrera'] = pd.to_datetime(df['Fecha_Carrera'])\n",
        "    df['Año_Carrera'] = df['Fecha_Carrera'].dt.year\n",
        "    df.to_csv('radio_taxi_carreras.csv', index=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Generar datos\n",
        "df = generar_datos_radio_taxi()\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "\n",
        "# Agregar columna de propinas aleatorias entre 0 y 10 soles\n",
        "df['Propina'] = np.random.uniform(0, 10, size=len(df))\n",
        "\n",
        "# Calcular la suma total de propinas por taxista\n",
        "propinas_por_taxista = df.groupby('Codigo_Taxista')['Propina'].sum().reset_index(name='Total_Propinas')\n",
        "\n",
        "# Identificar los taxistas con las mayores propinas\n",
        "top_taxistas_propinas = propinas_por_taxista.nlargest(3, 'Total_Propinas')\n",
        "\n",
        "# **Identificar el taxista más activo por año**\n",
        "taxista_activo_por_año = df.groupby(['Año_Carrera', 'Codigo_Taxista']).size().reset_index(name='Total_Carreras')\n",
        "taxista_mas_activo = taxista_activo_por_año.loc[taxista_activo_por_año.groupby('Año_Carrera')['Total_Carreras'].idxmax()]\n",
        "\n",
        "# Mostrar el taxista más activo por año\n",
        "print(\"Taxista más activo por año (2001-2024):\")\n",
        "print(taxista_mas_activo)\n",
        "\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "\n",
        "# Contar el número de carreras por taxista\n",
        "carreras_por_taxista = df.groupby('Taxista_Label').size().reset_index(name='Total_Carreras')\n",
        "top_3_taxistas = carreras_por_taxista.nlargest(3, 'Total_Carreras')\n",
        "peores_10_taxistas = carreras_por_taxista.nsmallest(10, 'Total_Carreras')\n",
        "\n",
        "# Datos de entrada (Taxista_Label) y de salida (Total_Carreras)\n",
        "X = carreras_por_taxista[['Taxista_Label']].values\n",
        "y = carreras_por_taxista['Total_Carreras'].values\n",
        "\n",
        "# Dividimos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predecir las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "\n",
        "# Mostrar los 3 taxistas con más carreras\n",
        "print(\"Top 3 taxistas con más carreras:\")\n",
        "print(top_3_taxistas)\n",
        "\n",
        "# Mostrar los 10 taxistas con menos carreras\n",
        "print(\"Los 10 taxistas con peores carreras:\")\n",
        "print(peores_10_taxistas)\n",
        "\n",
        "# Mostrar taxistas con más carreras en 2020\n",
        "top_taxistas_2020 = df[df['Año_Carrera'] == 2020].groupby('Codigo_Taxista').size().reset_index(name='Total_Carreras').nlargest(3, 'Total_Carreras')\n",
        "print(\"Los taxistas con más carreras del 2020:\")\n",
        "print(top_taxistas_2020)\n",
        "\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')\n",
        "\n",
        "# **Encontrar a los taxistas más consistentes**\n",
        "consistencia_taxistas = df.groupby('Codigo_Taxista')['Duracion_Carrera'].agg(['mean', 'std']).reset_index()\n",
        "consistencia_taxistas = consistencia_taxistas.sort_values(by='std')\n",
        "taxistas_consistentes = consistencia_taxistas.head(3)\n",
        "\n",
        "# Mostrar los taxistas más consistentes\n",
        "print(\"Los taxistas más consistentes en la duración de sus carreras:\")\n",
        "print(taxistas_consistentes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQqtqiDftvzw",
        "outputId": "4d76e526-86bc-4bcd-b99b-d28902f0d5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taxista más activo por año (2001-2024):\n",
            "      Año_Carrera Codigo_Taxista  Total_Carreras\n",
            "1            2001         AS-002              65\n",
            "73           2002         AS-024              66\n",
            "123          2003         AS-024              61\n",
            "151          2004         AS-002              69\n",
            "233          2005         AS-034              72\n",
            "267          2006         AS-018              72\n",
            "317          2007         AS-018              77\n",
            "350          2008         AS-001              71\n",
            "400          2009         AS-001              69\n",
            "467          2010         AS-018              65\n",
            "523          2011         AS-024              68\n",
            "567          2012         AS-018              67\n",
            "623          2013         AS-024              75\n",
            "675          2014         AS-026              88\n",
            "701          2015         AS-002              74\n",
            "756          2016         AS-007              71\n",
            "800          2017         AS-001              66\n",
            "873          2018         AS-024              67\n",
            "900          2019         AS-001              65\n",
            "973          2020         AS-024              60\n",
            "1017         2021         AS-018              79\n",
            "1050         2022         AS-001              68\n",
            "1100         2023         AS-001              74\n",
            "1173         2024         AS-024              74\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 209ms/step - loss: 877469.7500 - mae: 892.2827 - val_loss: 1099266.0000 - val_mae: 1007.9510\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 883253.0625 - mae: 893.7464 - val_loss: 1096355.1250 - val_mae: 1006.4114\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 837672.8125 - mae: 869.8307 - val_loss: 1093444.7500 - val_mae: 1004.8695\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 883262.4375 - mae: 894.2242 - val_loss: 1090527.7500 - val_mae: 1003.3211\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 864582.4375 - mae: 884.8526 - val_loss: 1087917.5000 - val_mae: 1001.9324\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 866069.0625 - mae: 887.0281 - val_loss: 1085335.2500 - val_mae: 1000.5570\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 837597.4375 - mae: 870.5803 - val_loss: 1082780.2500 - val_mae: 999.1932\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 852549.1875 - mae: 879.7068 - val_loss: 1080581.3750 - val_mae: 998.0173\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 885815.1875 - mae: 894.5611 - val_loss: 1078365.3750 - val_mae: 996.8314\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 852246.3125 - mae: 878.0403 - val_loss: 1076098.1250 - val_mae: 995.6163\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 828237.4375 - mae: 866.6642 - val_loss: 1073762.2500 - val_mae: 994.3628\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 865381.5625 - mae: 884.6974 - val_loss: 1071356.6250 - val_mae: 993.0695\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 833806.4375 - mae: 866.9981 - val_loss: 1068907.0000 - val_mae: 991.7507\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 833686.0625 - mae: 865.4037 - val_loss: 1066502.2500 - val_mae: 990.4528\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 863100.3750 - mae: 882.6666 - val_loss: 1064108.7500 - val_mae: 989.1603\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 857550.4375 - mae: 877.6221 - val_loss: 1061636.7500 - val_mae: 987.8231\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 818518.7500 - mae: 861.1028 - val_loss: 1059093.8750 - val_mae: 986.4451\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 827418.3750 - mae: 863.5886 - val_loss: 1056500.7500 - val_mae: 985.0376\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 829453.3125 - mae: 863.4467 - val_loss: 1053838.8750 - val_mae: 983.5903\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - loss: 833916.6875 - mae: 867.1682 - val_loss: 1051070.7500 - val_mae: 982.0825\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 840962.0000 - mae: 869.1542 - val_loss: 1048175.1250 - val_mae: 980.5021\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 804803.4375 - mae: 850.5585 - val_loss: 1045126.3750 - val_mae: 978.8350\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 818948.0625 - mae: 857.7656 - val_loss: 1041964.3750 - val_mae: 977.1022\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 846899.9375 - mae: 870.4218 - val_loss: 1038714.6250 - val_mae: 975.3156\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 865459.0000 - mae: 882.9153 - val_loss: 1035474.0000 - val_mae: 973.5319\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 839084.7500 - mae: 868.5825 - val_loss: 1032143.6250 - val_mae: 971.6945\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 842227.0000 - mae: 868.8206 - val_loss: 1028717.8125 - val_mae: 969.8001\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - loss: 843790.2500 - mae: 868.8626 - val_loss: 1025163.6875 - val_mae: 967.8299\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 838113.0000 - mae: 865.9481 - val_loss: 1021473.8750 - val_mae: 965.7794\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 827295.6250 - mae: 861.7485 - val_loss: 1017700.5000 - val_mae: 963.6771\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 805995.0625 - mae: 848.8012 - val_loss: 1013855.8125 - val_mae: 961.5290\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - loss: 786118.5625 - mae: 838.7780 - val_loss: 1009943.8125 - val_mae: 959.3375\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 807499.3750 - mae: 849.2497 - val_loss: 1005975.1250 - val_mae: 957.1080\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 815838.6250 - mae: 853.0588 - val_loss: 1001924.6875 - val_mae: 954.8262\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 832491.1875 - mae: 862.5724 - val_loss: 997738.1875 - val_mae: 952.4606\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 778847.8125 - mae: 832.0465 - val_loss: 993359.1875 - val_mae: 949.9784\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 806682.0000 - mae: 845.6371 - val_loss: 988761.1250 - val_mae: 947.3635\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 818149.9375 - mae: 854.2007 - val_loss: 984010.0000 - val_mae: 944.6522\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 792237.4375 - mae: 838.3465 - val_loss: 979063.8125 - val_mae: 941.8193\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 782681.5625 - mae: 835.1198 - val_loss: 973927.6250 - val_mae: 938.8668\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 789393.0625 - mae: 836.6202 - val_loss: 968616.3750 - val_mae: 935.8013\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 781437.3125 - mae: 830.9350 - val_loss: 963058.6250 - val_mae: 932.5804\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 793762.8750 - mae: 836.2800 - val_loss: 957283.0000 - val_mae: 929.2184\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 768430.8750 - mae: 824.2830 - val_loss: 951384.6875 - val_mae: 925.7697\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 778800.4375 - mae: 829.5438 - val_loss: 945437.5000 - val_mae: 922.2762\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 764931.8750 - mae: 821.0242 - val_loss: 939408.3125 - val_mae: 918.7181\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 774516.4375 - mae: 826.2576 - val_loss: 933235.3125 - val_mae: 915.0573\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 732051.6875 - mae: 802.6700 - val_loss: 926917.6250 - val_mae: 911.2918\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 777638.3125 - mae: 825.2739 - val_loss: 920445.8125 - val_mae: 907.4144\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 747281.4375 - mae: 810.0067 - val_loss: 913750.3750 - val_mae: 903.3815\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 728485.5625 - mae: 800.9995 - val_loss: 906851.1875 - val_mae: 899.2021\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 759543.5625 - mae: 813.9152 - val_loss: 899797.8125 - val_mae: 894.9045\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 741837.5625 - mae: 803.9165 - val_loss: 892534.3750 - val_mae: 890.4518\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 723566.5000 - mae: 792.6005 - val_loss: 885125.8750 - val_mae: 885.8813\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 730500.1875 - mae: 796.6750 - val_loss: 877529.5000 - val_mae: 881.1644\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 712715.3125 - mae: 785.9509 - val_loss: 869702.3750 - val_mae: 876.2710\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 709983.1250 - mae: 782.1478 - val_loss: 861465.6875 - val_mae: 871.0842\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 709893.3125 - mae: 781.4166 - val_loss: 852849.6875 - val_mae: 865.6170\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 681936.5625 - mae: 768.1823 - val_loss: 844118.8125 - val_mae: 860.0323\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 694448.7500 - mae: 770.0236 - val_loss: 835388.3125 - val_mae: 854.4022\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 706283.1250 - mae: 776.3516 - val_loss: 826468.9375 - val_mae: 848.6017\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 682464.9375 - mae: 763.0829 - val_loss: 817345.5000 - val_mae: 842.6164\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 643145.1875 - mae: 737.1578 - val_loss: 807937.3125 - val_mae: 836.3875\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 649170.1875 - mae: 741.4182 - val_loss: 798279.8750 - val_mae: 829.9318\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 645189.7500 - mae: 736.7232 - val_loss: 788547.8750 - val_mae: 823.3615\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 677265.8125 - mae: 753.8377 - val_loss: 778620.2500 - val_mae: 816.5898\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 654875.4375 - mae: 739.0033 - val_loss: 768529.8125 - val_mae: 809.6328\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 640192.4375 - mae: 730.4662 - val_loss: 758362.1875 - val_mae: 802.5447\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 650497.5625 - mae: 735.1575 - val_loss: 748126.1250 - val_mae: 795.3270\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 638531.8750 - mae: 726.6154 - val_loss: 737850.6250 - val_mae: 787.9960\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 608913.8750 - mae: 705.6018 - val_loss: 727515.1250 - val_mae: 780.5327\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 598102.8750 - mae: 696.1599 - val_loss: 717104.5000 - val_mae: 772.9213\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 604155.6875 - mae: 696.7933 - val_loss: 706557.6250 - val_mae: 765.1100\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 607138.6875 - mae: 699.5292 - val_loss: 695908.3750 - val_mae: 757.1168\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 589866.1875 - mae: 684.2243 - val_loss: 685143.6250 - val_mae: 748.9235\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 597230.5000 - mae: 687.4689 - val_loss: 674368.3750 - val_mae: 740.6034\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 562073.5625 - mae: 663.8553 - val_loss: 663746.1250 - val_mae: 732.2804\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 576523.0625 - mae: 668.5034 - val_loss: 653187.8750 - val_mae: 723.8826\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 567640.4375 - mae: 662.5109 - val_loss: 642602.6250 - val_mae: 715.3323\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 567675.0625 - mae: 663.2857 - val_loss: 632080.1250 - val_mae: 706.6967\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 528510.3125 - mae: 635.0905 - val_loss: 621566.8750 - val_mae: 697.9261\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 530003.1250 - mae: 637.4008 - val_loss: 611044.8125 - val_mae: 688.9987\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 546160.3125 - mae: 642.2770 - val_loss: 600663.1250 - val_mae: 680.0359\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 546904.0625 - mae: 638.8863 - val_loss: 590220.8125 - val_mae: 670.8571\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 538853.0625 - mae: 627.4803 - val_loss: 579695.6250 - val_mae: 661.4296\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 522161.0312 - mae: 620.0730 - val_loss: 569286.5625 - val_mae: 651.9230\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 506964.7500 - mae: 605.1483 - val_loss: 559011.7500 - val_mae: 642.3496\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 490658.3750 - mae: 591.5858 - val_loss: 548746.5625 - val_mae: 632.5852\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 518279.1875 - mae: 603.4929 - val_loss: 538542.1250 - val_mae: 622.6676\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 495485.8125 - mae: 589.8687 - val_loss: 528544.1250 - val_mae: 612.7329\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 485558.3125 - mae: 584.3220 - val_loss: 518886.3438 - val_mae: 602.9182\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 473677.5938 - mae: 573.4716 - val_loss: 509583.3125 - val_mae: 593.2469\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 459319.3125 - mae: 560.7131 - val_loss: 500526.7500 - val_mae: 585.8745\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 444264.6562 - mae: 544.5630 - val_loss: 491645.2500 - val_mae: 579.3266\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 478930.8750 - mae: 567.9607 - val_loss: 482887.0000 - val_mae: 572.7103\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 469762.5000 - mae: 558.9695 - val_loss: 474324.8438 - val_mae: 568.4075\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 441043.6875 - mae: 538.3984 - val_loss: 465880.9062 - val_mae: 564.3073\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 447657.7188 - mae: 546.2447 - val_loss: 457662.7500 - val_mae: 560.2080\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 447818.8438 - mae: 538.1010 - val_loss: 449815.1875 - val_mae: 556.1852\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 446216.6562 - mae: 539.3920 - val_loss: 442273.1875 - val_mae: 552.2105\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step\n",
            "Top 3 taxistas con más carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "17             17            1500\n",
            "0               0            1490\n",
            "23             23            1448\n",
            "Los 10 taxistas con peores carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "31             31             518\n",
            "3               3             532\n",
            "10             10             541\n",
            "14             14             547\n",
            "9               9             552\n",
            "46             46             564\n",
            "39             39             592\n",
            "47             47             598\n",
            "42             42             606\n",
            "18             18             613\n",
            "Los taxistas con más carreras del 2020:\n",
            "   Codigo_Taxista  Total_Carreras\n",
            "23         AS-024              60\n",
            "0          AS-001              59\n",
            "6          AS-007              56\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 442273.1875 - mae: 552.2105\n",
            "Pérdida: 442273.1875, MAE: 552.2105102539062\n",
            "Los taxistas más consistentes en la duración de sus carreras:\n",
            "   Codigo_Taxista       mean        std\n",
            "7          AS-008  32.790468  15.764445\n",
            "19         AS-020  32.352222  15.778548\n",
            "32         AS-033  33.012308  15.843801\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#99.\tTaxista con más carreras largas: Modifica el código para encontrar al taxista que ha realizado más carreras con una duración mayor a 45 minutos."
      ],
      "metadata": {
        "id": "mhQCopmTuEWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "\n",
        "# Contar el número de carreras por taxista\n",
        "carreras_por_taxista = df.groupby('Taxista_Label').size().reset_index(name='Total_Carreras')\n",
        "\n",
        "# Obtener los 10 taxistas con menos carreras\n",
        "X = carreras_por_taxista[['Taxista_Label']].values\n",
        "y = carreras_por_taxista['Total_Carreras'].values\n",
        "\n",
        "# Dividimos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')\n",
        "\n",
        "carreras_largas = df[df['Duracion_Carrera'] > 45]\n",
        "carreras_largas_por_taxista = carreras_largas.groupby('Taxista_Label').size().reset_index(name='Total_Carreras_Largas')\n",
        "\n",
        "# Encontrar al taxista con más carreras largas\n",
        "taxista_mas_carreras_largas = carreras_largas_por_taxista.nlargest(1, 'Total_Carreras_Largas')\n",
        "taxista_nombre = label_encoder.inverse_transform(taxista_mas_carreras_largas['Taxista_Label'])\n",
        "# empezar a lerer\n",
        "print(\"Taxista con más carreras largas (duración > 45 minutos):\")\n",
        "print(taxista_nombre[0], \"con\", taxista_mas_carreras_largas['Total_Carreras_Largas'].values[0], \"carreras.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Yd_aOhuC2P",
        "outputId": "395dcd69-a794-4639-e291-4f926d8b7f6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 284ms/step - loss: 873226.2500 - mae: 889.0989 - val_loss: 1109383.3750 - val_mae: 1013.2839\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 862369.3750 - mae: 886.6358 - val_loss: 1106160.6250 - val_mae: 1011.5903\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 918157.1875 - mae: 914.9334 - val_loss: 1103148.1250 - val_mae: 1010.0039\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 860533.0000 - mae: 883.6623 - val_loss: 1100311.5000 - val_mae: 1008.5071\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - loss: 859637.5000 - mae: 884.3795 - val_loss: 1097563.7500 - val_mae: 1007.0551\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 879220.1875 - mae: 894.0210 - val_loss: 1094798.7500 - val_mae: 1005.5913\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 896044.9375 - mae: 900.0035 - val_loss: 1091996.7500 - val_mae: 1004.1053\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 858565.5000 - mae: 882.0479 - val_loss: 1089301.2500 - val_mae: 1002.6722\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 847152.9375 - mae: 874.7233 - val_loss: 1086812.7500 - val_mae: 1001.3475\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 863442.4375 - mae: 885.4911 - val_loss: 1084290.0000 - val_mae: 1000.0027\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step - loss: 878439.4375 - mae: 891.2271 - val_loss: 1081723.5000 - val_mae: 998.6325\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 848147.3750 - mae: 876.0695 - val_loss: 1079143.1250 - val_mae: 997.2526\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 854095.5625 - mae: 880.7963 - val_loss: 1076555.2500 - val_mae: 995.8663\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 844064.0000 - mae: 871.6646 - val_loss: 1073932.0000 - val_mae: 994.4588\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 865521.5625 - mae: 883.6667 - val_loss: 1071254.7500 - val_mae: 993.0199\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 861715.6875 - mae: 880.7603 - val_loss: 1068507.5000 - val_mae: 991.5406\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 852278.1875 - mae: 874.3114 - val_loss: 1065683.7500 - val_mae: 990.0175\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 872842.6875 - mae: 887.6984 - val_loss: 1062781.2500 - val_mae: 988.4487\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 827618.8750 - mae: 862.6166 - val_loss: 1059828.7500 - val_mae: 986.8496\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 853791.9375 - mae: 875.9991 - val_loss: 1056863.7500 - val_mae: 985.2399\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 807086.1250 - mae: 852.0244 - val_loss: 1053945.3750 - val_mae: 983.6531\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 841202.1250 - mae: 868.7039 - val_loss: 1050931.7500 - val_mae: 982.0117\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 839347.1875 - mae: 866.7239 - val_loss: 1047804.3125 - val_mae: 980.3047\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 844395.4375 - mae: 871.5990 - val_loss: 1044539.8750 - val_mae: 978.5193\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 841533.8125 - mae: 867.4581 - val_loss: 1040885.3125 - val_mae: 976.5189\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 810367.5625 - mae: 856.2181 - val_loss: 1036921.3750 - val_mae: 974.3389\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 819731.2500 - mae: 857.2505 - val_loss: 1032750.3125 - val_mae: 972.0394\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 828981.0000 - mae: 863.7274 - val_loss: 1028391.3750 - val_mae: 969.6298\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 837144.0625 - mae: 867.1171 - val_loss: 1023973.5000 - val_mae: 967.1768\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 829436.9375 - mae: 863.6480 - val_loss: 1019403.8125 - val_mae: 964.6362\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 839469.0625 - mae: 868.1729 - val_loss: 1014507.3750 - val_mae: 961.9045\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 808728.2500 - mae: 848.9022 - val_loss: 1009403.1250 - val_mae: 959.0456\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 817204.5625 - mae: 855.5499 - val_loss: 1004002.6875 - val_mae: 956.0094\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 830935.4375 - mae: 861.0667 - val_loss: 998309.0000 - val_mae: 952.7953\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 808498.6875 - mae: 847.4379 - val_loss: 992325.1875 - val_mae: 949.4033\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 766780.5000 - mae: 825.6235 - val_loss: 986077.6250 - val_mae: 945.8457\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 798457.2500 - mae: 842.6708 - val_loss: 979642.8125 - val_mae: 942.1644\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 795759.8125 - mae: 838.7332 - val_loss: 973054.8750 - val_mae: 938.3773\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 806180.8125 - mae: 844.7484 - val_loss: 966215.6875 - val_mae: 934.4256\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 783089.0625 - mae: 831.1978 - val_loss: 959166.6875 - val_mae: 930.3311\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 762669.0625 - mae: 820.7347 - val_loss: 951890.0000 - val_mae: 926.0811\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 788880.4375 - mae: 833.4451 - val_loss: 944334.8750 - val_mae: 921.6428\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 769476.0000 - mae: 822.2967 - val_loss: 936468.8750 - val_mae: 916.9935\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 776009.6875 - mae: 824.7365 - val_loss: 928338.6875 - val_mae: 912.1575\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 764856.7500 - mae: 818.5914 - val_loss: 919905.6875 - val_mae: 907.1074\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 736712.5000 - mae: 801.1459 - val_loss: 911211.6875 - val_mae: 901.8644\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 754523.7500 - mae: 813.8277 - val_loss: 902307.6250 - val_mae: 896.4553\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 726062.4375 - mae: 793.2836 - val_loss: 893201.8125 - val_mae: 890.8815\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 730548.1250 - mae: 797.0638 - val_loss: 883922.1875 - val_mae: 885.1562\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 710375.5625 - mae: 786.2383 - val_loss: 874387.0000 - val_mae: 879.2251\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 679684.6875 - mae: 765.6110 - val_loss: 864612.8125 - val_mae: 873.0928\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 687894.7500 - mae: 770.1012 - val_loss: 854760.0000 - val_mae: 866.8566\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 685851.4375 - mae: 768.3427 - val_loss: 844753.3750 - val_mae: 860.4650\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 682613.3125 - mae: 764.2421 - val_loss: 834570.1250 - val_mae: 853.8990\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 684111.6875 - mae: 764.6475 - val_loss: 824217.0625 - val_mae: 847.1577\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 662163.8125 - mae: 752.9315 - val_loss: 813643.3125 - val_mae: 840.2023\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 685303.8750 - mae: 760.5997 - val_loss: 802913.2500 - val_mae: 833.0690\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 671105.0625 - mae: 749.4395 - val_loss: 791885.8750 - val_mae: 825.6564\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 655732.9375 - mae: 740.8032 - val_loss: 780666.1250 - val_mae: 818.0267\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 651968.8750 - mae: 738.6950 - val_loss: 769410.7500 - val_mae: 810.2809\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 651225.5000 - mae: 735.3427 - val_loss: 758185.3750 - val_mae: 802.4608\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 648241.0625 - mae: 733.1524 - val_loss: 746828.9375 - val_mae: 794.4490\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 625367.3125 - mae: 714.9471 - val_loss: 735116.0000 - val_mae: 786.0752\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 635912.3125 - mae: 720.9442 - val_loss: 723238.1250 - val_mae: 777.4647\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 642108.0000 - mae: 723.4484 - val_loss: 711313.0000 - val_mae: 768.6943\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 628147.3125 - mae: 712.3350 - val_loss: 699313.8125 - val_mae: 759.7366\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 595413.7500 - mae: 691.5543 - val_loss: 687317.6250 - val_mae: 750.6418\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 553078.4375 - mae: 662.8597 - val_loss: 675440.0625 - val_mae: 741.4938\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 575196.8750 - mae: 671.9131 - val_loss: 663497.6875 - val_mae: 732.1448\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 590766.0000 - mae: 678.5582 - val_loss: 651403.1250 - val_mae: 722.5140\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 594805.6250 - mae: 681.0214 - val_loss: 639264.9375 - val_mae: 712.6747\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 572298.0625 - mae: 661.4600 - val_loss: 627083.5000 - val_mae: 702.6157\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 548389.5625 - mae: 647.8918 - val_loss: 614953.0000 - val_mae: 692.4047\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 510587.5938 - mae: 621.6744 - val_loss: 602920.3125 - val_mae: 682.0732\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 552389.6250 - mae: 641.1230 - val_loss: 590960.1875 - val_mae: 671.5918\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 539611.5625 - mae: 630.8373 - val_loss: 579127.6250 - val_mae: 660.9996\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 540111.0625 - mae: 626.9719 - val_loss: 567407.0000 - val_mae: 650.2750\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 495085.1562 - mae: 592.4931 - val_loss: 555707.3125 - val_mae: 639.3226\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 488428.5938 - mae: 594.4468 - val_loss: 544075.3125 - val_mae: 628.1718\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 479484.9062 - mae: 581.5941 - val_loss: 532850.8750 - val_mae: 617.1465\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 469544.3438 - mae: 570.2794 - val_loss: 521925.1562 - val_mae: 606.1453\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 498512.2812 - mae: 591.9764 - val_loss: 511216.1562 - val_mae: 595.0833\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 472404.8750 - mae: 568.9492 - val_loss: 500799.4375 - val_mae: 586.0986\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 449687.0938 - mae: 550.6233 - val_loss: 490705.9062 - val_mae: 578.6544\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 464561.9688 - mae: 555.2868 - val_loss: 481051.7500 - val_mae: 571.5604\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 451140.2188 - mae: 546.6333 - val_loss: 471644.7500 - val_mae: 567.0808\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 423819.4062 - mae: 525.9600 - val_loss: 462454.5000 - val_mae: 562.5764\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 455234.1562 - mae: 542.4480 - val_loss: 453547.5938 - val_mae: 558.0787\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 442622.9062 - mae: 539.9291 - val_loss: 444831.5938 - val_mae: 553.5403\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 443011.0312 - mae: 531.9417 - val_loss: 436465.3438 - val_mae: 549.0436\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 437664.7812 - mae: 529.3676 - val_loss: 428471.6562 - val_mae: 544.6056\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 427797.7188 - mae: 520.8278 - val_loss: 420945.8125 - val_mae: 540.2880\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 421942.5312 - mae: 512.2442 - val_loss: 413642.8750 - val_mae: 535.9559\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 427138.6250 - mae: 523.0704 - val_loss: 406680.3438 - val_mae: 531.6808\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 420194.3438 - mae: 510.6506 - val_loss: 400077.6875 - val_mae: 527.4821\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 398745.0312 - mae: 490.7307 - val_loss: 393812.6562 - val_mae: 523.3531\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 411335.0000 - mae: 497.0034 - val_loss: 387781.1875 - val_mae: 519.2297\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 400628.2500 - mae: 492.5600 - val_loss: 381796.9375 - val_mae: 514.9774\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 396835.8438 - mae: 489.8171 - val_loss: 375976.1875 - val_mae: 510.6676\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 403460.4688 - mae: 486.5647 - val_loss: 370494.0938 - val_mae: 508.3095\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 370494.0938 - mae: 508.3095\n",
            "Pérdida: 370494.09375, MAE: 508.30950927734375\n",
            "Taxista con más carreras largas (duración > 45 minutos):\n",
            "AS-018 con 397 carreras.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#10\tEliminar registros duplicados: Simula que algunos registros de carreras están duplicados. Agrega código para eliminar los registros duplicados antes de realizar los análisis."
      ],
      "metadata": {
        "id": "a14AC6THvD9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "\n",
        "# Verificar valores nulos y tipos de datos\n",
        "print(df.info())\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Simular registros duplicados\n",
        "df = pd.concat([df, df.sample(frac=0.1)])  # Duplica el 10% de los registros\n",
        "\n",
        "# Eliminar registros duplicados\n",
        "df = df.drop_duplicates()\n",
        "\n",
        "# Convertir el código del taxista a un valor numérico usando LabelEncoder\n",
        "if 'Codigo_Taxista' in df.columns and df['Codigo_Taxista'].notnull().all():\n",
        "    label_encoder = LabelEncoder()\n",
        "    df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "else:\n",
        "    print(\"Error: 'Codigo_Taxista' tiene valores nulos o no existe en el DataFrame.\")\n",
        "\n",
        "# Contar el número de carreras por taxista\n",
        "carreras_por_taxista = df.groupby('Taxista_Label').size().reset_index(name='Total_Carreras')\n",
        "\n",
        "# Obtener los 10 taxistas con menos carreras\n",
        "peores_10_taxistas = carreras_por_taxista.nsmallest(10, 'Total_Carreras')\n",
        "\n",
        "# Datos de entrada (Taxista_Label) y de salida (Total_Carreras)\n",
        "X = carreras_por_taxista[['Taxista_Label']].values\n",
        "y = carreras_por_taxista['Total_Carreras'].values\n",
        "\n",
        "# Dividir en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)  # No activación, ya que es una predicción continua\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predecir las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss:.4f}, MAE: {mae:.4f}')\n",
        "\n",
        "# Encontrar el taxista con más carreras largas (duración > 45 minutos)\n",
        "if 'Duracion_Carrera' in df.columns:\n",
        "    carreras_largas = df[df['Duracion_Carrera'] > 45]\n",
        "    carreras_largas_por_taxista = carreras_largas.groupby('Taxista_Label').size().reset_index(name='Total_Carreras_Largas')\n",
        "\n",
        "    # Encontrar al taxista con más carreras largas\n",
        "    taxista_mas_carreras_largas = carreras_largas_por_taxista.nlargest(1, 'Total_Carreras_Largas')\n",
        "\n",
        "    # Mostrar el resultado\n",
        "    taxista_nombre = label_encoder.inverse_transform(taxista_mas_carreras_largas['Taxista_Label'])\n",
        "    print(\"Taxista con más carreras largas (duración > 45 minutos):\")\n",
        "    print(taxista_nombre[0], \"con\", taxista_mas_carreras_largas['Total_Carreras_Largas'].values[0], \"carreras.\")\n",
        "else:\n",
        "    print(\"Error: 'Duracion_Carrera' no existe en el DataFrame.\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SORwFGwNvFvY",
        "outputId": "3b4ce4f3-d12c-480b-fb7e-f388cb3163d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 45830 entries, 0 to 45829\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   Codigo_Taxista    45830 non-null  object\n",
            " 1   Numero_Carrera    45830 non-null  int64 \n",
            " 2   Fecha_Carrera     45830 non-null  object\n",
            " 3   Duracion_Carrera  45830 non-null  int64 \n",
            " 4   Año_Carrera       45830 non-null  int64 \n",
            "dtypes: int64(3), object(2)\n",
            "memory usage: 1.7+ MB\n",
            "None\n",
            "Codigo_Taxista      0\n",
            "Numero_Carrera      0\n",
            "Fecha_Carrera       0\n",
            "Duracion_Carrera    0\n",
            "Año_Carrera         0\n",
            "dtype: int64\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-c0277665654d>:23: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 298ms/step - loss: 891134.9375 - mae: 897.9958 - val_loss: 1097566.0000 - val_mae: 1007.0511\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 869694.3125 - mae: 888.2153 - val_loss: 1095896.0000 - val_mae: 1006.1675\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 876184.0625 - mae: 891.8415 - val_loss: 1094247.5000 - val_mae: 1005.2941\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 881313.5625 - mae: 893.2110 - val_loss: 1092614.7500 - val_mae: 1004.4283\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 859372.3750 - mae: 881.5594 - val_loss: 1090939.8750 - val_mae: 1003.5394\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 869316.4375 - mae: 887.3487 - val_loss: 1089193.3750 - val_mae: 1002.6111\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 855277.3750 - mae: 880.2546 - val_loss: 1087364.3750 - val_mae: 1001.6381\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 848250.7500 - mae: 876.3430 - val_loss: 1085467.3750 - val_mae: 1000.6277\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 836876.1875 - mae: 872.4462 - val_loss: 1083512.2500 - val_mae: 999.5848\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 855154.6875 - mae: 879.8077 - val_loss: 1081511.0000 - val_mae: 998.5158\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 874513.7500 - mae: 889.2197 - val_loss: 1079522.5000 - val_mae: 997.4528\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 846491.8125 - mae: 875.3842 - val_loss: 1077461.7500 - val_mae: 996.3494\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 872809.2500 - mae: 887.8436 - val_loss: 1075326.0000 - val_mae: 995.2043\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 876562.1875 - mae: 890.8419 - val_loss: 1073100.2500 - val_mae: 994.0095\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 866346.8125 - mae: 883.9354 - val_loss: 1070787.6250 - val_mae: 992.7660\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 854553.4375 - mae: 879.4392 - val_loss: 1068382.7500 - val_mae: 991.4709\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 863629.5000 - mae: 884.6912 - val_loss: 1065879.5000 - val_mae: 990.1208\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 845935.5000 - mae: 873.1718 - val_loss: 1063246.8750 - val_mae: 988.6984\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 852034.1250 - mae: 876.2729 - val_loss: 1060515.5000 - val_mae: 987.2199\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 855546.0625 - mae: 877.0038 - val_loss: 1057691.6250 - val_mae: 985.6886\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 843624.6875 - mae: 871.1142 - val_loss: 1054741.6250 - val_mae: 984.0858\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 832093.1875 - mae: 863.9794 - val_loss: 1051695.2500 - val_mae: 982.4276\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 841749.8750 - mae: 870.7127 - val_loss: 1048550.1875 - val_mae: 980.7117\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 817758.6250 - mae: 857.2656 - val_loss: 1045333.6250 - val_mae: 978.9533\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 837344.3125 - mae: 866.1517 - val_loss: 1042016.5000 - val_mae: 977.1359\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 845208.6250 - mae: 869.1813 - val_loss: 1038532.0000 - val_mae: 975.2224\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 808812.2500 - mae: 853.6967 - val_loss: 1034889.8750 - val_mae: 973.2174\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 828205.3750 - mae: 860.6010 - val_loss: 1031102.1250 - val_mae: 971.1268\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 841207.9375 - mae: 868.6534 - val_loss: 1027153.1250 - val_mae: 968.9413\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 829362.5625 - mae: 861.4074 - val_loss: 1023079.8750 - val_mae: 966.6810\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 800756.5000 - mae: 847.9897 - val_loss: 1018862.5000 - val_mae: 964.3336\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 801237.6875 - mae: 845.6872 - val_loss: 1014471.8125 - val_mae: 961.8824\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 811788.7500 - mae: 849.8615 - val_loss: 1009869.3125 - val_mae: 959.3049\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 822180.3125 - mae: 857.4765 - val_loss: 1005064.0000 - val_mae: 956.6045\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 822903.1250 - mae: 857.9984 - val_loss: 1000150.8750 - val_mae: 953.8341\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 815022.4375 - mae: 853.8117 - val_loss: 995182.8125 - val_mae: 951.0226\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 808929.1875 - mae: 847.1810 - val_loss: 990048.6875 - val_mae: 948.1063\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 790408.7500 - mae: 837.2603 - val_loss: 984701.8125 - val_mae: 945.0575\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 808060.1875 - mae: 847.9863 - val_loss: 979186.5000 - val_mae: 941.9000\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - loss: 797268.1875 - mae: 841.4147 - val_loss: 973536.1250 - val_mae: 938.6517\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - loss: 772052.0000 - mae: 827.6552 - val_loss: 967722.0000 - val_mae: 935.2947\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - loss: 765174.3750 - mae: 823.7006 - val_loss: 961705.5000 - val_mae: 931.8051\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - loss: 800113.6875 - mae: 841.6039 - val_loss: 955471.5000 - val_mae: 928.1725\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 121ms/step - loss: 794128.9375 - mae: 837.1306 - val_loss: 949010.0000 - val_mae: 924.3887\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 761910.5000 - mae: 817.9764 - val_loss: 942317.1250 - val_mae: 920.4490\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 778389.6875 - mae: 830.2129 - val_loss: 935465.8750 - val_mae: 916.3945\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 754243.5000 - mae: 813.1119 - val_loss: 928440.1250 - val_mae: 912.2135\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 746233.8125 - mae: 805.9691 - val_loss: 921158.1250 - val_mae: 907.8550\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 756882.5625 - mae: 814.0624 - val_loss: 913638.3125 - val_mae: 903.3265\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - loss: 756857.7500 - mae: 812.7415 - val_loss: 905947.6875 - val_mae: 898.6661\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step - loss: 713188.9375 - mae: 790.6753 - val_loss: 898170.1875 - val_mae: 893.9225\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 731099.9375 - mae: 796.9088 - val_loss: 890317.0000 - val_mae: 889.1006\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 712441.0000 - mae: 787.9001 - val_loss: 882288.5000 - val_mae: 884.1373\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step - loss: 738066.1875 - mae: 801.6548 - val_loss: 874183.5000 - val_mae: 879.0913\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - loss: 721640.4375 - mae: 787.1938 - val_loss: 865920.1875 - val_mae: 873.9093\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 708404.0625 - mae: 780.8934 - val_loss: 857323.3125 - val_mae: 868.4769\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 709090.0625 - mae: 779.8605 - val_loss: 848487.3750 - val_mae: 862.8485\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 671988.3125 - mae: 758.3425 - val_loss: 839594.1250 - val_mae: 857.1370\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 699327.3750 - mae: 772.1929 - val_loss: 830432.8750 - val_mae: 851.2029\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 700642.9375 - mae: 773.1178 - val_loss: 821015.3750 - val_mae: 845.0479\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 690271.3750 - mae: 766.1431 - val_loss: 811381.8125 - val_mae: 838.6925\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 692585.8750 - mae: 766.2268 - val_loss: 801639.8750 - val_mae: 832.2031\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 675020.3125 - mae: 756.5621 - val_loss: 791893.6875 - val_mae: 825.6462\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 674945.3125 - mae: 751.7928 - val_loss: 782089.4375 - val_mae: 818.9828\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 648464.5000 - mae: 737.5967 - val_loss: 772071.5625 - val_mae: 812.1024\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 625420.5625 - mae: 723.4801 - val_loss: 761900.3750 - val_mae: 805.0398\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 633699.3750 - mae: 724.4866 - val_loss: 751575.6250 - val_mae: 797.7886\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 622583.8125 - mae: 713.7576 - val_loss: 741178.7500 - val_mae: 790.4006\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 643776.6250 - mae: 730.2546 - val_loss: 730797.6250 - val_mae: 782.9344\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 614122.6875 - mae: 705.7615 - val_loss: 720389.6250 - val_mae: 775.3556\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 640755.0000 - mae: 722.1924 - val_loss: 709887.3125 - val_mae: 767.6100\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 605552.1250 - mae: 699.9173 - val_loss: 699320.9375 - val_mae: 759.7133\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 592819.1250 - mae: 691.2858 - val_loss: 688729.5625 - val_mae: 751.6896\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 621329.4375 - mae: 709.4149 - val_loss: 678228.6875 - val_mae: 743.6228\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 583164.4375 - mae: 677.2717 - val_loss: 667772.3750 - val_mae: 735.4748\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 552016.7500 - mae: 658.9562 - val_loss: 657258.5000 - val_mae: 727.1610\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 558708.4375 - mae: 659.9193 - val_loss: 646690.8750 - val_mae: 718.6769\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 553819.0625 - mae: 653.4689 - val_loss: 636139.3125 - val_mae: 710.0717\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 530321.3750 - mae: 640.0624 - val_loss: 625639.6875 - val_mae: 701.3694\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 548478.7500 - mae: 645.9550 - val_loss: 615184.3750 - val_mae: 692.5588\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 556027.5625 - mae: 650.4232 - val_loss: 604766.1875 - val_mae: 683.6276\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 530928.6250 - mae: 633.9073 - val_loss: 594448.8750 - val_mae: 674.6256\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step - loss: 545990.1250 - mae: 633.7748 - val_loss: 584215.5625 - val_mae: 665.5337\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 519926.7500 - mae: 615.6843 - val_loss: 573847.9375 - val_mae: 656.1469\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 493745.7188 - mae: 592.8521 - val_loss: 563491.1875 - val_mae: 646.5837\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 501130.0625 - mae: 601.2579 - val_loss: 553269.5000 - val_mae: 636.9517\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 496089.8750 - mae: 595.5611 - val_loss: 543096.1250 - val_mae: 627.1615\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 473810.7500 - mae: 578.0466 - val_loss: 533196.6875 - val_mae: 617.4276\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 487960.6875 - mae: 583.3473 - val_loss: 523550.8125 - val_mae: 607.7334\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 481110.1875 - mae: 574.7136 - val_loss: 513956.3438 - val_mae: 597.8702\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 497155.1562 - mae: 586.9498 - val_loss: 504476.4062 - val_mae: 588.7468\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 479044.4062 - mae: 567.6727 - val_loss: 495271.5938 - val_mae: 582.0298\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 455255.0938 - mae: 554.6621 - val_loss: 486333.3125 - val_mae: 575.3451\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 467068.2812 - mae: 562.5871 - val_loss: 477778.8125 - val_mae: 570.0406\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 438248.1562 - mae: 533.8219 - val_loss: 469398.1875 - val_mae: 566.0157\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 471297.5312 - mae: 558.5790 - val_loss: 461001.0000 - val_mae: 561.8749\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 405011.8125 - mae: 515.0809 - val_loss: 452888.9375 - val_mae: 557.7634\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 429084.4062 - mae: 522.3177 - val_loss: 444968.0000 - val_mae: 553.6339\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 446469.1250 - mae: 534.0151 - val_loss: 437351.5625 - val_mae: 549.5464\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 439179.9688 - mae: 531.3282 - val_loss: 430000.0625 - val_mae: 545.4833\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 430000.0625 - mae: 545.4833\n",
            "Pérdida: 430000.0625, MAE: 545.4833\n",
            "Taxista con más carreras largas (duración > 45 minutos):\n",
            "AS-018 con 397 carreras.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#11. Top 3 por año: Modifica el código para encontrar los 3 taxistas con más carreras para cada año, no solo en el total de la empresa.\n"
      ],
      "metadata": {
        "id": "Ck6fNPVm_6zU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "\n",
        "# Contar el número de carreras por taxista\n",
        "carreras_por_taxista = df.groupby('Taxista_Label').size().reset_index(name='Total_Carreras')\n",
        "\n",
        "# Obtener los 3 taxistas con más carreras\n",
        "top_3_taxistas = carreras_por_taxista.nlargest(3, 'Total_Carreras')\n",
        "\n",
        "# Datos de entrada (Taxista_Label) y de salida (Total_Carreras)\n",
        "X = carreras_por_taxista[['Taxista_Label']].values\n",
        "y = carreras_por_taxista['Total_Carreras'].values\n",
        "\n",
        "# Dividimos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predecir las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "\n",
        "# Mostrar los 3 taxistas con más carreras\n",
        "print(\"Top 3 taxistas con más carreras:\")\n",
        "print(top_3_taxistas)\n",
        "\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFOFfDpu_83D",
        "outputId": "78629fd9-94dd-4c51-ac41-f776c0b5585f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 193ms/step - loss: 875804.7500 - mae: 891.0908 - val_loss: 1107240.2500 - val_mae: 1012.1566\n",
            "Epoch 2/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 899561.3750 - mae: 903.4962 - val_loss: 1105317.8750 - val_mae: 1011.1453\n",
            "Epoch 3/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 887125.2500 - mae: 896.2962 - val_loss: 1103723.5000 - val_mae: 1010.3057\n",
            "Epoch 4/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step - loss: 844010.7500 - mae: 874.2355 - val_loss: 1102170.6250 - val_mae: 1009.4871\n",
            "Epoch 5/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 836552.8750 - mae: 870.1315 - val_loss: 1100775.2500 - val_mae: 1008.7505\n",
            "Epoch 6/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 854825.8750 - mae: 880.9349 - val_loss: 1099431.6250 - val_mae: 1008.0409\n",
            "Epoch 7/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 881601.8125 - mae: 894.1415 - val_loss: 1098122.7500 - val_mae: 1007.3491\n",
            "Epoch 8/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 870533.9375 - mae: 888.1526 - val_loss: 1096804.2500 - val_mae: 1006.6516\n",
            "Epoch 9/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 865650.8125 - mae: 885.2356 - val_loss: 1095625.1250 - val_mae: 1006.0269\n",
            "Epoch 10/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 868945.2500 - mae: 888.6945 - val_loss: 1094437.5000 - val_mae: 1005.3981\n",
            "Epoch 11/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 880056.2500 - mae: 891.7105 - val_loss: 1093230.7500 - val_mae: 1004.7584\n",
            "Epoch 12/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 887767.4375 - mae: 895.8979 - val_loss: 1091913.2500 - val_mae: 1004.0593\n",
            "Epoch 13/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 884897.9375 - mae: 894.2819 - val_loss: 1090521.0000 - val_mae: 1003.3199\n",
            "Epoch 14/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 868712.6250 - mae: 886.1827 - val_loss: 1089058.7500 - val_mae: 1002.5436\n",
            "Epoch 15/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 879480.2500 - mae: 891.5182 - val_loss: 1087411.8750 - val_mae: 1001.6676\n",
            "Epoch 16/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 879180.8125 - mae: 892.6158 - val_loss: 1085663.3750 - val_mae: 1000.7362\n",
            "Epoch 17/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 850956.8750 - mae: 876.1401 - val_loss: 1083840.8750 - val_mae: 999.7645\n",
            "Epoch 18/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 842792.3125 - mae: 874.4374 - val_loss: 1081939.2500 - val_mae: 998.7492\n",
            "Epoch 19/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 878032.1875 - mae: 889.4395 - val_loss: 1079966.8750 - val_mae: 997.6951\n",
            "Epoch 20/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 855574.5000 - mae: 879.3009 - val_loss: 1077918.5000 - val_mae: 996.5988\n",
            "Epoch 21/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 852096.1875 - mae: 877.3532 - val_loss: 1075808.3750 - val_mae: 995.4678\n",
            "Epoch 22/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 877753.4375 - mae: 889.7988 - val_loss: 1073615.7500 - val_mae: 994.2913\n",
            "Epoch 23/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 856531.3750 - mae: 878.6245 - val_loss: 1071284.6250 - val_mae: 993.0383\n",
            "Epoch 24/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 852205.9375 - mae: 877.0833 - val_loss: 1068828.2500 - val_mae: 991.7159\n",
            "Epoch 25/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 861619.1875 - mae: 883.2906 - val_loss: 1066315.2500 - val_mae: 990.3610\n",
            "Epoch 26/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 863625.0000 - mae: 881.4482 - val_loss: 1063788.0000 - val_mae: 988.9959\n",
            "Epoch 27/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 823999.3125 - mae: 861.3254 - val_loss: 1061201.7500 - val_mae: 987.5967\n",
            "Epoch 28/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 851794.1250 - mae: 875.9902 - val_loss: 1058535.1250 - val_mae: 986.1516\n",
            "Epoch 29/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 857607.0625 - mae: 878.5129 - val_loss: 1055741.6250 - val_mae: 984.6349\n",
            "Epoch 30/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 851898.8750 - mae: 875.1046 - val_loss: 1052844.8750 - val_mae: 983.0590\n",
            "Epoch 31/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 825402.8125 - mae: 863.2681 - val_loss: 1049826.8750 - val_mae: 981.4142\n",
            "Epoch 32/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 850169.9375 - mae: 874.2600 - val_loss: 1046717.0000 - val_mae: 979.7156\n",
            "Epoch 33/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 835663.1250 - mae: 865.8484 - val_loss: 1043494.3125 - val_mae: 977.9515\n",
            "Epoch 34/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 853599.6875 - mae: 877.3994 - val_loss: 1040133.6250 - val_mae: 976.1079\n",
            "Epoch 35/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 846599.6250 - mae: 870.8915 - val_loss: 1036696.8125 - val_mae: 974.2186\n",
            "Epoch 36/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 824203.9375 - mae: 860.1107 - val_loss: 1032952.6250 - val_mae: 972.1579\n",
            "Epoch 37/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 838413.5625 - mae: 866.4876 - val_loss: 1028972.0000 - val_mae: 969.9575\n",
            "Epoch 38/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 850387.6250 - mae: 873.3638 - val_loss: 1024832.8125 - val_mae: 967.6637\n",
            "Epoch 39/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 838788.6250 - mae: 865.4850 - val_loss: 1020555.1250 - val_mae: 965.2858\n",
            "Epoch 40/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 820785.6875 - mae: 856.3707 - val_loss: 1016049.6250 - val_mae: 962.7734\n",
            "Epoch 41/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 825510.8750 - mae: 860.1350 - val_loss: 1011373.5000 - val_mae: 960.1570\n",
            "Epoch 42/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 831911.5625 - mae: 862.1906 - val_loss: 1006635.0000 - val_mae: 957.4966\n",
            "Epoch 43/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 807787.8125 - mae: 849.4657 - val_loss: 1001781.8125 - val_mae: 954.7630\n",
            "Epoch 44/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 801961.1250 - mae: 846.4577 - val_loss: 996780.1250 - val_mae: 951.9359\n",
            "Epoch 45/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 807104.6250 - mae: 849.2721 - val_loss: 991571.1875 - val_mae: 948.9807\n",
            "Epoch 46/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 792904.1875 - mae: 840.3542 - val_loss: 986142.0000 - val_mae: 945.8884\n",
            "Epoch 47/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 789361.5000 - mae: 837.4292 - val_loss: 980535.3125 - val_mae: 942.6819\n",
            "Epoch 48/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 795426.5625 - mae: 841.6404 - val_loss: 974805.3750 - val_mae: 939.3910\n",
            "Epoch 49/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 810961.5625 - mae: 850.8918 - val_loss: 968885.0000 - val_mae: 935.9759\n",
            "Epoch 50/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 794977.8750 - mae: 839.8898 - val_loss: 962759.8750 - val_mae: 932.4265\n",
            "Epoch 51/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 791762.1875 - mae: 837.8985 - val_loss: 956460.3750 - val_mae: 928.7584\n",
            "Epoch 52/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 783619.8125 - mae: 830.4630 - val_loss: 949999.8750 - val_mae: 924.9781\n",
            "Epoch 53/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 779266.1250 - mae: 827.4287 - val_loss: 943333.5000 - val_mae: 921.0573\n",
            "Epoch 54/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 766589.8125 - mae: 821.9163 - val_loss: 936453.0000 - val_mae: 916.9888\n",
            "Epoch 55/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 773161.1250 - mae: 826.6235 - val_loss: 929382.1250 - val_mae: 912.7843\n",
            "Epoch 56/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 768257.9375 - mae: 821.0648 - val_loss: 922059.8750 - val_mae: 908.4050\n",
            "Epoch 57/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 739561.0000 - mae: 802.8011 - val_loss: 914499.3125 - val_mae: 903.8553\n",
            "Epoch 58/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 719887.9375 - mae: 793.2387 - val_loss: 906792.6875 - val_mae: 899.1887\n",
            "Epoch 59/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 767929.4375 - mae: 819.4509 - val_loss: 898950.5000 - val_mae: 894.4089\n",
            "Epoch 60/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 736089.3750 - mae: 800.5728 - val_loss: 890943.0000 - val_mae: 889.4953\n",
            "Epoch 61/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step - loss: 704548.8125 - mae: 784.0243 - val_loss: 882733.0000 - val_mae: 884.4222\n",
            "Epoch 62/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 705000.4375 - mae: 783.9755 - val_loss: 874330.3125 - val_mae: 879.1921\n",
            "Epoch 63/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 722649.0625 - mae: 787.6723 - val_loss: 865743.1250 - val_mae: 873.8068\n",
            "Epoch 64/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 728814.5000 - mae: 794.9741 - val_loss: 856987.5000 - val_mae: 868.2729\n",
            "Epoch 65/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 720112.3125 - mae: 787.4539 - val_loss: 848171.8125 - val_mae: 862.6561\n",
            "Epoch 66/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 681041.1875 - mae: 764.8867 - val_loss: 839276.8125 - val_mae: 856.9418\n",
            "Epoch 67/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 717535.8750 - mae: 784.3051 - val_loss: 830212.1875 - val_mae: 851.0690\n",
            "Epoch 68/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 695121.4375 - mae: 769.5279 - val_loss: 820895.8750 - val_mae: 844.9793\n",
            "Epoch 69/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 701731.1250 - mae: 774.8433 - val_loss: 811511.5000 - val_mae: 838.7886\n",
            "Epoch 70/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 671952.2500 - mae: 753.6024 - val_loss: 802102.4375 - val_mae: 832.5233\n",
            "Epoch 71/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 668344.8125 - mae: 749.4229 - val_loss: 792527.8750 - val_mae: 826.0858\n",
            "Epoch 72/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 667279.4375 - mae: 748.6888 - val_loss: 782659.6250 - val_mae: 819.3838\n",
            "Epoch 73/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 639131.1875 - mae: 731.9122 - val_loss: 772473.6875 - val_mae: 812.3917\n",
            "Epoch 74/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 662153.5625 - mae: 745.7935 - val_loss: 762052.8750 - val_mae: 805.1580\n",
            "Epoch 75/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 627657.6250 - mae: 720.5674 - val_loss: 751504.8750 - val_mae: 797.7505\n",
            "Epoch 76/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 630070.8125 - mae: 719.6084 - val_loss: 740818.3750 - val_mae: 790.1549\n",
            "Epoch 77/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 608898.5625 - mae: 704.6295 - val_loss: 730037.6875 - val_mae: 782.3962\n",
            "Epoch 78/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 621294.8125 - mae: 714.5309 - val_loss: 719067.8125 - val_mae: 774.3982\n",
            "Epoch 79/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 614658.4375 - mae: 705.6705 - val_loss: 708115.7500 - val_mae: 766.3054\n",
            "Epoch 80/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 599964.0000 - mae: 696.3063 - val_loss: 697187.1875 - val_mae: 758.1182\n",
            "Epoch 81/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 591193.7500 - mae: 689.3058 - val_loss: 686248.1250 - val_mae: 749.8063\n",
            "Epoch 82/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 595268.2500 - mae: 688.0681 - val_loss: 675274.6875 - val_mae: 741.3460\n",
            "Epoch 83/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 570426.9375 - mae: 671.1959 - val_loss: 664372.8750 - val_mae: 732.8139\n",
            "Epoch 84/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 568289.1250 - mae: 666.7756 - val_loss: 653342.4375 - val_mae: 724.0463\n",
            "Epoch 85/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - loss: 594323.2500 - mae: 683.1322 - val_loss: 642208.6250 - val_mae: 715.0519\n",
            "Epoch 86/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 570881.6875 - mae: 664.5787 - val_loss: 631204.1875 - val_mae: 706.0126\n",
            "Epoch 87/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 532773.3750 - mae: 638.1470 - val_loss: 620157.6250 - val_mae: 696.7809\n",
            "Epoch 88/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 560556.1875 - mae: 652.9227 - val_loss: 609269.6875 - val_mae: 687.5197\n",
            "Epoch 89/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 538172.9375 - mae: 635.8596 - val_loss: 598476.5000 - val_mae: 678.1709\n",
            "Epoch 90/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 515712.3438 - mae: 619.7899 - val_loss: 587729.1875 - val_mae: 668.6862\n",
            "Epoch 91/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 513144.4375 - mae: 607.5862 - val_loss: 577020.8750 - val_mae: 659.0516\n",
            "Epoch 92/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 547493.9375 - mae: 636.8275 - val_loss: 566370.1250 - val_mae: 649.2745\n",
            "Epoch 93/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 514890.2812 - mae: 607.4257 - val_loss: 555967.1250 - val_mae: 639.5262\n",
            "Epoch 94/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 473972.2812 - mae: 575.3154 - val_loss: 545599.8125 - val_mae: 629.6040\n",
            "Epoch 95/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 500868.0625 - mae: 595.5775 - val_loss: 535219.6875 - val_mae: 619.4479\n",
            "Epoch 96/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 495081.7812 - mae: 583.8359 - val_loss: 525085.8750 - val_mae: 609.3041\n",
            "Epoch 97/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 482647.9688 - mae: 573.1011 - val_loss: 514971.6875 - val_mae: 598.9380\n",
            "Epoch 98/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 489972.5625 - mae: 584.8644 - val_loss: 504891.6875 - val_mae: 589.0485\n",
            "Epoch 99/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 461283.6562 - mae: 558.0325 - val_loss: 495095.4062 - val_mae: 581.9022\n",
            "Epoch 100/100\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 432756.4688 - mae: 541.8276 - val_loss: 485519.8125 - val_mae: 574.7313\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
            "Top 3 taxistas con más carreras:\n",
            "    Taxista_Label  Total_Carreras\n",
            "17             17            1500\n",
            "0               0            1490\n",
            "23             23            1448\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 485519.8125 - mae: 574.7313\n",
            "Pérdida: 485519.8125, MAE: 574.7312622070312\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# #12.Añadir distancias: Agrega una columna para la distancia de la carrera (generada aleatoriamente). Luego, encuentra al taxista que ha recorrido más kilómetros en total"
      ],
      "metadata": {
        "id": "LEj5huw0Epu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "date_column_name = 'Fecha_Carrera'\n",
        "\n",
        "# Convertir la columna de fecha a un tipo de dato de fecha\n",
        "df[date_column_name] = pd.to_datetime(df[date_column_name])\n",
        "\n",
        "# Extraer el año de la fecha\n",
        "df['Año'] = df[date_column_name].dt.year\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "np.random.seed(42)\n",
        "df['Distancia'] = np.random.uniform(1, 50, size=len(df))\n",
        "\n",
        "# Contar el número de carreras por taxista y por año\n",
        "carreras_por_taxista_año = df.groupby(['Año', 'Taxista_Label']).size().reset_index(name='Total_Carreras')\n",
        "\n",
        "# Obtener los 3 taxistas con más carreras para cada año\n",
        "top_3_taxistas_por_año = carreras_por_taxista_año.groupby('Año').apply(lambda x: x.nlargest(3, 'Total_Carreras')).reset_index(drop=True)\n",
        "\n",
        "# Sumar las distancias por taxista\n",
        "distancia_por_taxista = df.groupby('Taxista_Label')['Distancia'].sum().reset_index(name='Total_Distancia')\n",
        "\n",
        "# Encontrar el taxista que ha recorrido más kilómetros\n",
        "taxista_mas_km = distancia_por_taxista.loc[distancia_por_taxista['Total_Distancia'].idxmax()]\n",
        "X = carreras_por_taxista_año[['Taxista_Label']].values\n",
        "y = carreras_por_taxista_año['Total_Carreras'].values\n",
        "\n",
        "# Dividimos en conjuntos de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predecir las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "\n",
        "print(\"Top 3 taxistas con más carreras por año:\")\n",
        "print(top_3_taxistas_por_año)\n",
        "\n",
        "# Mostrar el taxista que ha recorrido más kilómetros\n",
        "print(f\"Taxista que ha recorrido más kilómetros: {taxista_mas_km}\")\n",
        "\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oznj4MS0EsI2",
        "outputId": "95e297be-cd86-4eab-c43e-0f09a6edfa95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-43f01d9089b2>:26: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  top_3_taxistas_por_año = carreras_por_taxista_año.groupby('Año').apply(lambda x: x.nlargest(3, 'Total_Carreras')).reset_index(drop=True)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 1826.1400 - mae: 40.4830 - val_loss: 1107.1083 - val_mae: 30.0697\n",
            "Epoch 2/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1183.3164 - mae: 30.4351 - val_loss: 710.7502 - val_mae: 21.3416\n",
            "Epoch 3/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 746.4583 - mae: 21.8454 - val_loss: 560.2279 - val_mae: 19.5523\n",
            "Epoch 4/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 643.7120 - mae: 20.2756 - val_loss: 557.7318 - val_mae: 19.5909\n",
            "Epoch 5/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 633.2889 - mae: 19.8786 - val_loss: 552.6672 - val_mae: 19.4741\n",
            "Epoch 6/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 659.3811 - mae: 20.2405 - val_loss: 547.0408 - val_mae: 19.3348\n",
            "Epoch 7/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 659.6345 - mae: 20.5452 - val_loss: 542.0975 - val_mae: 19.2522\n",
            "Epoch 8/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 601.6642 - mae: 19.4100 - val_loss: 537.3916 - val_mae: 19.2055\n",
            "Epoch 9/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 673.1002 - mae: 20.4656 - val_loss: 529.9554 - val_mae: 19.0148\n",
            "Epoch 10/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 622.4076 - mae: 19.8800 - val_loss: 525.5637 - val_mae: 19.0279\n",
            "Epoch 11/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 631.2301 - mae: 19.8233 - val_loss: 513.7614 - val_mae: 18.6290\n",
            "Epoch 12/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 616.6353 - mae: 19.9890 - val_loss: 504.6122 - val_mae: 18.4655\n",
            "Epoch 13/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 602.7246 - mae: 19.4956 - val_loss: 492.1118 - val_mae: 18.0754\n",
            "Epoch 14/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 586.3243 - mae: 19.0874 - val_loss: 480.6610 - val_mae: 17.9763\n",
            "Epoch 15/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 576.9560 - mae: 19.1063 - val_loss: 464.9256 - val_mae: 17.6175\n",
            "Epoch 16/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 563.5497 - mae: 18.6732 - val_loss: 447.8779 - val_mae: 17.2857\n",
            "Epoch 17/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 542.2538 - mae: 18.3327 - val_loss: 426.1327 - val_mae: 16.6332\n",
            "Epoch 18/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 480.0056 - mae: 16.9073 - val_loss: 405.3965 - val_mae: 16.3913\n",
            "Epoch 19/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 481.2030 - mae: 17.1405 - val_loss: 369.9357 - val_mae: 15.2301\n",
            "Epoch 20/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 430.9145 - mae: 16.1826 - val_loss: 338.5020 - val_mae: 14.4917\n",
            "Epoch 21/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 415.0907 - mae: 15.7572 - val_loss: 305.6702 - val_mae: 13.8611\n",
            "Epoch 22/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 381.7958 - mae: 15.0848 - val_loss: 276.1926 - val_mae: 13.3024\n",
            "Epoch 23/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 343.3524 - mae: 14.3902 - val_loss: 239.1507 - val_mae: 12.0729\n",
            "Epoch 24/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 276.1352 - mae: 13.1293 - val_loss: 212.8572 - val_mae: 11.6691\n",
            "Epoch 25/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 257.3336 - mae: 12.5746 - val_loss: 193.7135 - val_mae: 11.0881\n",
            "Epoch 26/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 225.5719 - mae: 11.9722 - val_loss: 188.4784 - val_mae: 11.4881\n",
            "Epoch 27/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 209.7684 - mae: 11.8282 - val_loss: 170.1853 - val_mae: 10.7075\n",
            "Epoch 28/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 201.3554 - mae: 11.6097 - val_loss: 165.5567 - val_mae: 10.6739\n",
            "Epoch 29/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 186.7176 - mae: 11.2284 - val_loss: 163.8419 - val_mae: 10.6954\n",
            "Epoch 30/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 171.2020 - mae: 10.7827 - val_loss: 169.6562 - val_mae: 11.0268\n",
            "Epoch 31/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 184.7865 - mae: 11.4579 - val_loss: 163.8092 - val_mae: 10.7020\n",
            "Epoch 32/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 191.3363 - mae: 11.6949 - val_loss: 164.5896 - val_mae: 10.7336\n",
            "Epoch 33/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 190.8919 - mae: 11.6228 - val_loss: 166.0918 - val_mae: 10.7952\n",
            "Epoch 34/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 176.1207 - mae: 10.9412 - val_loss: 163.3610 - val_mae: 10.4577\n",
            "Epoch 35/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 171.8049 - mae: 10.9484 - val_loss: 163.2557 - val_mae: 10.5589\n",
            "Epoch 36/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 188.5053 - mae: 11.3691 - val_loss: 163.5706 - val_mae: 10.4615\n",
            "Epoch 37/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 187.2870 - mae: 11.2674 - val_loss: 165.2872 - val_mae: 10.7247\n",
            "Epoch 38/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 171.4186 - mae: 10.7650 - val_loss: 165.5960 - val_mae: 10.7379\n",
            "Epoch 39/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 181.4185 - mae: 11.3171 - val_loss: 163.2630 - val_mae: 10.4383\n",
            "Epoch 40/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 190.6809 - mae: 11.3659 - val_loss: 164.1169 - val_mae: 10.6213\n",
            "Epoch 41/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 181.8271 - mae: 11.1416 - val_loss: 164.3835 - val_mae: 10.6396\n",
            "Epoch 42/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 186.7146 - mae: 11.2432 - val_loss: 163.4465 - val_mae: 10.5545\n",
            "Epoch 43/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - loss: 180.5232 - mae: 11.1415 - val_loss: 162.7320 - val_mae: 10.5284\n",
            "Epoch 44/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 194.9375 - mae: 11.4035 - val_loss: 165.9464 - val_mae: 10.7780\n",
            "Epoch 45/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 191.9099 - mae: 11.3443 - val_loss: 177.5216 - val_mae: 11.2695\n",
            "Epoch 46/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - loss: 188.5768 - mae: 11.4035 - val_loss: 163.4866 - val_mae: 10.4377\n",
            "Epoch 47/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 176.6545 - mae: 11.0265 - val_loss: 163.1969 - val_mae: 10.5467\n",
            "Epoch 48/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - loss: 175.9360 - mae: 10.8516 - val_loss: 170.2263 - val_mae: 10.9682\n",
            "Epoch 49/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 176.3102 - mae: 10.8018 - val_loss: 162.8041 - val_mae: 10.4024\n",
            "Epoch 50/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 181.1919 - mae: 11.0385 - val_loss: 164.6400 - val_mae: 10.6945\n",
            "Epoch 51/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 177.3874 - mae: 11.1252 - val_loss: 165.4876 - val_mae: 10.7550\n",
            "Epoch 52/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 196.3434 - mae: 11.4883 - val_loss: 162.5672 - val_mae: 10.5254\n",
            "Epoch 53/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 180.7414 - mae: 11.0905 - val_loss: 163.0745 - val_mae: 10.5605\n",
            "Epoch 54/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 184.7060 - mae: 11.1345 - val_loss: 163.9481 - val_mae: 10.6487\n",
            "Epoch 55/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 182.9310 - mae: 11.2443 - val_loss: 164.4655 - val_mae: 10.6783\n",
            "Epoch 56/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 179.6673 - mae: 11.0715 - val_loss: 165.7231 - val_mae: 10.7629\n",
            "Epoch 57/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 182.2197 - mae: 11.2075 - val_loss: 163.0025 - val_mae: 10.5446\n",
            "Epoch 58/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 189.3877 - mae: 11.3335 - val_loss: 164.9364 - val_mae: 10.7117\n",
            "Epoch 59/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 177.9513 - mae: 11.0072 - val_loss: 162.6117 - val_mae: 10.5333\n",
            "Epoch 60/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 181.3749 - mae: 10.9588 - val_loss: 165.4217 - val_mae: 10.7547\n",
            "Epoch 61/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 173.9277 - mae: 10.8988 - val_loss: 168.7194 - val_mae: 10.9046\n",
            "Epoch 62/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 180.4306 - mae: 11.2031 - val_loss: 162.1138 - val_mae: 10.5116\n",
            "Epoch 63/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 183.0323 - mae: 11.0999 - val_loss: 166.0850 - val_mae: 10.7938\n",
            "Epoch 64/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 186.1479 - mae: 11.3209 - val_loss: 167.4344 - val_mae: 10.8700\n",
            "Epoch 65/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 172.4080 - mae: 10.8483 - val_loss: 169.5359 - val_mae: 10.9491\n",
            "Epoch 66/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 173.3868 - mae: 10.9328 - val_loss: 167.6716 - val_mae: 10.8602\n",
            "Epoch 67/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 186.7951 - mae: 11.3792 - val_loss: 161.9814 - val_mae: 10.4724\n",
            "Epoch 68/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 179.2653 - mae: 11.1626 - val_loss: 163.1750 - val_mae: 10.6006\n",
            "Epoch 69/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 172.1746 - mae: 10.7232 - val_loss: 161.9547 - val_mae: 10.4551\n",
            "Epoch 70/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 174.5782 - mae: 11.0087 - val_loss: 163.0139 - val_mae: 10.6019\n",
            "Epoch 71/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 177.0660 - mae: 11.1285 - val_loss: 161.9313 - val_mae: 10.4802\n",
            "Epoch 72/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 169.2070 - mae: 10.6823 - val_loss: 162.8882 - val_mae: 10.5927\n",
            "Epoch 73/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 178.3442 - mae: 11.0907 - val_loss: 163.0885 - val_mae: 10.6190\n",
            "Epoch 74/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 185.3257 - mae: 11.4325 - val_loss: 162.1138 - val_mae: 10.5144\n",
            "Epoch 75/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 173.7089 - mae: 10.9389 - val_loss: 166.9233 - val_mae: 10.8207\n",
            "Epoch 76/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 192.9826 - mae: 11.5543 - val_loss: 162.5542 - val_mae: 10.5644\n",
            "Epoch 77/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 184.3786 - mae: 11.3293 - val_loss: 163.0844 - val_mae: 10.6129\n",
            "Epoch 78/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 174.1901 - mae: 10.9841 - val_loss: 165.6387 - val_mae: 10.7536\n",
            "Epoch 79/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 185.9368 - mae: 11.5529 - val_loss: 163.9326 - val_mae: 10.6734\n",
            "Epoch 80/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 176.2663 - mae: 10.9506 - val_loss: 164.9826 - val_mae: 10.7340\n",
            "Epoch 81/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 191.0772 - mae: 11.5285 - val_loss: 162.9127 - val_mae: 10.4016\n",
            "Epoch 82/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 180.8773 - mae: 10.8455 - val_loss: 163.9966 - val_mae: 10.3255\n",
            "Epoch 83/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 177.6923 - mae: 10.9555 - val_loss: 163.9804 - val_mae: 10.6782\n",
            "Epoch 84/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 188.8936 - mae: 11.4509 - val_loss: 166.3263 - val_mae: 10.8122\n",
            "Epoch 85/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 176.9901 - mae: 11.0881 - val_loss: 163.5293 - val_mae: 10.6475\n",
            "Epoch 86/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 186.0867 - mae: 11.4094 - val_loss: 161.7502 - val_mae: 10.4202\n",
            "Epoch 87/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 190.5991 - mae: 11.3982 - val_loss: 161.9693 - val_mae: 10.3822\n",
            "Epoch 88/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 182.9236 - mae: 11.0915 - val_loss: 164.9759 - val_mae: 10.7401\n",
            "Epoch 89/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 172.9000 - mae: 10.9403 - val_loss: 166.1627 - val_mae: 10.8013\n",
            "Epoch 90/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 186.2863 - mae: 11.3419 - val_loss: 162.7385 - val_mae: 10.5842\n",
            "Epoch 91/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 179.4488 - mae: 11.0333 - val_loss: 175.6948 - val_mae: 11.2108\n",
            "Epoch 92/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 176.0153 - mae: 11.1588 - val_loss: 162.6908 - val_mae: 10.5609\n",
            "Epoch 93/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 174.9477 - mae: 10.8785 - val_loss: 164.6792 - val_mae: 10.7258\n",
            "Epoch 94/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 181.8745 - mae: 11.2314 - val_loss: 162.6278 - val_mae: 10.5861\n",
            "Epoch 95/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 173.4111 - mae: 10.8839 - val_loss: 163.3106 - val_mae: 10.3684\n",
            "Epoch 96/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 175.4599 - mae: 10.9238 - val_loss: 162.8137 - val_mae: 10.6065\n",
            "Epoch 97/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 177.8986 - mae: 10.9320 - val_loss: 163.0543 - val_mae: 10.6121\n",
            "Epoch 98/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 175.9086 - mae: 10.9769 - val_loss: 163.1195 - val_mae: 10.6341\n",
            "Epoch 99/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 179.6916 - mae: 11.1981 - val_loss: 161.7645 - val_mae: 10.4506\n",
            "Epoch 100/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 176.7558 - mae: 10.8687 - val_loss: 161.8138 - val_mae: 10.5306\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
            "Top 3 taxistas con más carreras por año:\n",
            "     Año  Taxista_Label  Total_Carreras\n",
            "0   2001              1              65\n",
            "1   2001             25              62\n",
            "2   2001             17              61\n",
            "3   2002             23              66\n",
            "4   2002             17              65\n",
            "..   ...            ...             ...\n",
            "67  2023              1              67\n",
            "68  2023             17              66\n",
            "69  2024             23              74\n",
            "70  2024              0              70\n",
            "71  2024             17              68\n",
            "\n",
            "[72 rows x 3 columns]\n",
            "Taxista que ha recorrido más kilómetros: Taxista_Label         17.00000\n",
            "Total_Distancia    39073.97343\n",
            "Name: 17, dtype: float64\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 148.8895 - mae: 10.0582\n",
            "Pérdida: 161.81382751464844, MAE: 10.530627250671387\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#13.Agrupar por mes: Modifica el código para agrupar las carreras por mes en lugar de por año, y encuentra el mes en el que se realizaron más carreras en total."
      ],
      "metadata": {
        "id": "iUhr5YySXCKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "date_column_name = 'Fecha_Carrera'\n",
        "\n",
        "# Extraer el mes y el año de la fecha (año-mes)\n",
        "df[date_column_name] = pd.to_datetime(df[date_column_name])\n",
        "df['Mes'] = df[date_column_name].dt.to_period('M')\n",
        "np.random.seed(42)\n",
        "df['Distancia'] = np.random.uniform(1, 50, size=len(df))\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "\n",
        "# Contar el número de carreras por taxista y por mes\n",
        "carreras_por_taxista_mes = df.groupby(['Mes', 'Taxista_Label']).size().reset_index(name='Total_Carreras')\n",
        "\n",
        "# Obtener los 3 taxistas con más carreras para cada mes\n",
        "top_3_taxistas_por_mes = carreras_por_taxista_mes.groupby('Mes').apply(lambda x: x.nlargest(3, 'Total_Carreras')).reset_index(drop=True)\n",
        "carreras_por_mes = df.groupby('Mes').size().reset_index(name='Total_Carreras')\n",
        "mes_mas_carreras = carreras_por_mes.loc[carreras_por_mes['Total_Carreras'].idxmax()]\n",
        "\n",
        "# Sumar las distancias por taxista\n",
        "distancia_por_taxista = df.groupby('Taxista_Label')['Distancia'].sum().reset_index(name='Total_Distancia')\n",
        "\n",
        "# Encontrar el taxista que ha recorrido más kilómetros\n",
        "taxista_mas_km = distancia_por_taxista.loc[distancia_por_taxista['Total_Distancia'].idxmax()]\n",
        "\n",
        "# Datos de entrada (Taxista_Label) y de salida (Total_Carreras)\n",
        "X = carreras_por_taxista_mes[['Taxista_Label']].values\n",
        "y = carreras_por_taxista_mes['Total_Carreras'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Construir el modelo de deep learning\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Predecir las carreras de los taxistas\n",
        "predicciones = model.predict(X_test)\n",
        "print(\"Top 3 taxistas con más carreras por mes:\")\n",
        "print(top_3_taxistas_por_mes)\n",
        "\n",
        "# Mostrar el mes con más carreras\n",
        "print(f\"Mes con más carreras: {mes_mas_carreras}\")\n",
        "\n",
        "# Mostrar el rendimiento del modelo\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLZOfvcmW8c6",
        "outputId": "db8cf8cf-3dab-45a3-8642-c7d77b164df3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-92c8839c2ed9>:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  top_3_taxistas_por_mes = carreras_por_taxista_mes.groupby('Mes').apply(lambda x: x.nlargest(3, 'Total_Carreras')).reset_index(drop=True)\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 6.5706 - mae: 1.9598 - val_loss: 3.9458 - val_mae: 1.5839\n",
            "Epoch 2/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.1641 - mae: 1.6165 - val_loss: 3.8201 - val_mae: 1.5312\n",
            "Epoch 3/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9266 - mae: 1.5870 - val_loss: 3.9756 - val_mae: 1.6256\n",
            "Epoch 4/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 4.0152 - mae: 1.5992 - val_loss: 3.7558 - val_mae: 1.5686\n",
            "Epoch 5/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9052 - mae: 1.5799 - val_loss: 3.7103 - val_mae: 1.5498\n",
            "Epoch 6/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8624 - mae: 1.5642 - val_loss: 3.7202 - val_mae: 1.5157\n",
            "Epoch 7/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8380 - mae: 1.5566 - val_loss: 3.6932 - val_mae: 1.5134\n",
            "Epoch 8/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.8557 - mae: 1.5667 - val_loss: 3.6781 - val_mae: 1.5510\n",
            "Epoch 9/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.8148 - mae: 1.5556 - val_loss: 3.7605 - val_mae: 1.5851\n",
            "Epoch 10/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7026 - mae: 1.5369 - val_loss: 3.7322 - val_mae: 1.5832\n",
            "Epoch 11/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7909 - mae: 1.5637 - val_loss: 3.6059 - val_mae: 1.5135\n",
            "Epoch 12/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7658 - mae: 1.5521 - val_loss: 3.6251 - val_mae: 1.4927\n",
            "Epoch 13/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7018 - mae: 1.5391 - val_loss: 3.6317 - val_mae: 1.4896\n",
            "Epoch 14/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8140 - mae: 1.5621 - val_loss: 3.6473 - val_mae: 1.4884\n",
            "Epoch 15/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7563 - mae: 1.5430 - val_loss: 3.6281 - val_mae: 1.4904\n",
            "Epoch 16/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7177 - mae: 1.5369 - val_loss: 3.5999 - val_mae: 1.4921\n",
            "Epoch 17/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.8669 - mae: 1.5731 - val_loss: 3.6060 - val_mae: 1.4876\n",
            "Epoch 18/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7479 - mae: 1.5498 - val_loss: 3.7305 - val_mae: 1.4904\n",
            "Epoch 19/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.7984 - mae: 1.5496 - val_loss: 3.5847 - val_mae: 1.4884\n",
            "Epoch 20/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.7621 - mae: 1.5502 - val_loss: 3.5784 - val_mae: 1.5231\n",
            "Epoch 21/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6200 - mae: 1.5264 - val_loss: 3.7165 - val_mae: 1.4955\n",
            "Epoch 22/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7071 - mae: 1.5408 - val_loss: 3.5767 - val_mae: 1.5298\n",
            "Epoch 23/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.9058 - mae: 1.5739 - val_loss: 3.6311 - val_mae: 1.5579\n",
            "Epoch 24/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6399 - mae: 1.5194 - val_loss: 3.5584 - val_mae: 1.5234\n",
            "Epoch 25/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7110 - mae: 1.5430 - val_loss: 3.5672 - val_mae: 1.5280\n",
            "Epoch 26/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.7580 - mae: 1.5494 - val_loss: 3.5907 - val_mae: 1.5435\n",
            "Epoch 27/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6912 - mae: 1.5315 - val_loss: 3.5970 - val_mae: 1.4835\n",
            "Epoch 28/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 3.6741 - mae: 1.5330 - val_loss: 3.5707 - val_mae: 1.4831\n",
            "Epoch 29/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6748 - mae: 1.5308 - val_loss: 3.5820 - val_mae: 1.5327\n",
            "Epoch 30/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.7780 - mae: 1.5522 - val_loss: 3.5573 - val_mae: 1.5263\n",
            "Epoch 31/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 3.6803 - mae: 1.5255 - val_loss: 3.5736 - val_mae: 1.5364\n",
            "Epoch 32/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6416 - mae: 1.5236 - val_loss: 3.6211 - val_mae: 1.4952\n",
            "Epoch 33/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5832 - mae: 1.5097 - val_loss: 3.5579 - val_mae: 1.5290\n",
            "Epoch 34/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7064 - mae: 1.5397 - val_loss: 3.5906 - val_mae: 1.4905\n",
            "Epoch 35/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6057 - mae: 1.5204 - val_loss: 3.5484 - val_mae: 1.4802\n",
            "Epoch 36/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7668 - mae: 1.5551 - val_loss: 3.5397 - val_mae: 1.4847\n",
            "Epoch 37/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6786 - mae: 1.5388 - val_loss: 3.5826 - val_mae: 1.4848\n",
            "Epoch 38/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7667 - mae: 1.5533 - val_loss: 3.5465 - val_mae: 1.5158\n",
            "Epoch 39/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6304 - mae: 1.5283 - val_loss: 3.5326 - val_mae: 1.4891\n",
            "Epoch 40/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.7431 - mae: 1.5440 - val_loss: 3.5447 - val_mae: 1.4906\n",
            "Epoch 41/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6358 - mae: 1.5147 - val_loss: 3.6613 - val_mae: 1.5645\n",
            "Epoch 42/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.6219 - mae: 1.5303 - val_loss: 3.5634 - val_mae: 1.4835\n",
            "Epoch 43/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.7094 - mae: 1.5345 - val_loss: 3.5375 - val_mae: 1.5238\n",
            "Epoch 44/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6204 - mae: 1.5196 - val_loss: 3.6074 - val_mae: 1.4793\n",
            "Epoch 45/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6805 - mae: 1.5236 - val_loss: 3.5067 - val_mae: 1.5047\n",
            "Epoch 46/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6982 - mae: 1.5369 - val_loss: 3.5430 - val_mae: 1.5257\n",
            "Epoch 47/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6543 - mae: 1.5270 - val_loss: 3.4960 - val_mae: 1.4919\n",
            "Epoch 48/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6846 - mae: 1.5328 - val_loss: 3.5025 - val_mae: 1.4837\n",
            "Epoch 49/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6447 - mae: 1.5232 - val_loss: 3.5300 - val_mae: 1.4687\n",
            "Epoch 50/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6770 - mae: 1.5236 - val_loss: 3.5546 - val_mae: 1.5335\n",
            "Epoch 51/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6953 - mae: 1.5300 - val_loss: 3.5210 - val_mae: 1.4739\n",
            "Epoch 52/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4881 - mae: 1.4908 - val_loss: 3.5536 - val_mae: 1.5195\n",
            "Epoch 53/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.6806 - mae: 1.5267 - val_loss: 3.5076 - val_mae: 1.4955\n",
            "Epoch 54/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.6308 - mae: 1.5132 - val_loss: 3.5181 - val_mae: 1.5183\n",
            "Epoch 55/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.6389 - mae: 1.5207 - val_loss: 3.4788 - val_mae: 1.4728\n",
            "Epoch 56/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5868 - mae: 1.5062 - val_loss: 3.5525 - val_mae: 1.4630\n",
            "Epoch 57/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5804 - mae: 1.5024 - val_loss: 3.4852 - val_mae: 1.4669\n",
            "Epoch 58/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6304 - mae: 1.5162 - val_loss: 3.4657 - val_mae: 1.4836\n",
            "Epoch 59/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6372 - mae: 1.5193 - val_loss: 3.4885 - val_mae: 1.4987\n",
            "Epoch 60/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5151 - mae: 1.4954 - val_loss: 3.4618 - val_mae: 1.4673\n",
            "Epoch 61/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5197 - mae: 1.4997 - val_loss: 3.4666 - val_mae: 1.4666\n",
            "Epoch 62/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5943 - mae: 1.5121 - val_loss: 3.5263 - val_mae: 1.5112\n",
            "Epoch 63/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6115 - mae: 1.5165 - val_loss: 3.5117 - val_mae: 1.4637\n",
            "Epoch 64/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 3.6336 - mae: 1.5143 - val_loss: 3.4700 - val_mae: 1.4892\n",
            "Epoch 65/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.6055 - mae: 1.5102 - val_loss: 3.4542 - val_mae: 1.4876\n",
            "Epoch 66/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5440 - mae: 1.4994 - val_loss: 3.5213 - val_mae: 1.5167\n",
            "Epoch 67/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6553 - mae: 1.5302 - val_loss: 3.4837 - val_mae: 1.4904\n",
            "Epoch 68/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5256 - mae: 1.5017 - val_loss: 3.5460 - val_mae: 1.5316\n",
            "Epoch 69/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5462 - mae: 1.5020 - val_loss: 3.4810 - val_mae: 1.4681\n",
            "Epoch 70/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6389 - mae: 1.5217 - val_loss: 3.4311 - val_mae: 1.4683\n",
            "Epoch 71/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5843 - mae: 1.5052 - val_loss: 3.4514 - val_mae: 1.4956\n",
            "Epoch 72/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6182 - mae: 1.5136 - val_loss: 3.4219 - val_mae: 1.4604\n",
            "Epoch 73/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6058 - mae: 1.5092 - val_loss: 3.4221 - val_mae: 1.4602\n",
            "Epoch 74/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5262 - mae: 1.4904 - val_loss: 3.4610 - val_mae: 1.4970\n",
            "Epoch 75/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5474 - mae: 1.4945 - val_loss: 3.4655 - val_mae: 1.4632\n",
            "Epoch 76/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.4377 - mae: 1.4776 - val_loss: 3.4713 - val_mae: 1.4590\n",
            "Epoch 77/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.6267 - mae: 1.5112 - val_loss: 3.4320 - val_mae: 1.4671\n",
            "Epoch 78/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.4934 - mae: 1.4876 - val_loss: 3.4272 - val_mae: 1.4587\n",
            "Epoch 79/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5350 - mae: 1.4810 - val_loss: 3.4706 - val_mae: 1.4580\n",
            "Epoch 80/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5403 - mae: 1.5004 - val_loss: 3.4011 - val_mae: 1.4454\n",
            "Epoch 81/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3192 - mae: 1.4507 - val_loss: 3.4089 - val_mae: 1.4660\n",
            "Epoch 82/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4998 - mae: 1.4848 - val_loss: 3.4151 - val_mae: 1.4612\n",
            "Epoch 83/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5674 - mae: 1.5015 - val_loss: 3.4160 - val_mae: 1.4561\n",
            "Epoch 84/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5640 - mae: 1.4940 - val_loss: 3.4243 - val_mae: 1.4518\n",
            "Epoch 85/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5216 - mae: 1.4962 - val_loss: 3.4826 - val_mae: 1.5069\n",
            "Epoch 86/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.6297 - mae: 1.5094 - val_loss: 3.4437 - val_mae: 1.4545\n",
            "Epoch 87/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5740 - mae: 1.5011 - val_loss: 3.4146 - val_mae: 1.4546\n",
            "Epoch 88/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5087 - mae: 1.4903 - val_loss: 3.4130 - val_mae: 1.4532\n",
            "Epoch 89/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.5130 - mae: 1.4909 - val_loss: 3.4634 - val_mae: 1.4525\n",
            "Epoch 90/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.4717 - mae: 1.4806 - val_loss: 3.4441 - val_mae: 1.4613\n",
            "Epoch 91/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.4716 - mae: 1.4846 - val_loss: 3.4722 - val_mae: 1.4502\n",
            "Epoch 92/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5514 - mae: 1.5002 - val_loss: 3.4540 - val_mae: 1.4557\n",
            "Epoch 93/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5180 - mae: 1.4886 - val_loss: 3.4330 - val_mae: 1.4878\n",
            "Epoch 94/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5190 - mae: 1.4941 - val_loss: 3.4182 - val_mae: 1.4513\n",
            "Epoch 95/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5509 - mae: 1.5044 - val_loss: 3.3804 - val_mae: 1.4538\n",
            "Epoch 96/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5049 - mae: 1.4902 - val_loss: 3.4065 - val_mae: 1.4489\n",
            "Epoch 97/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5072 - mae: 1.4934 - val_loss: 3.4309 - val_mae: 1.4705\n",
            "Epoch 98/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.3698 - mae: 1.4596 - val_loss: 3.3856 - val_mae: 1.4619\n",
            "Epoch 99/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 3.5083 - mae: 1.4954 - val_loss: 3.4728 - val_mae: 1.4617\n",
            "Epoch 100/100\n",
            "\u001b[1m336/336\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 3.3941 - mae: 1.4616 - val_loss: 3.4210 - val_mae: 1.4566\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "Top 3 taxistas con más carreras por mes:\n",
            "         Mes  Taxista_Label  Total_Carreras\n",
            "0    2001-01             27               9\n",
            "1    2001-01              0               6\n",
            "2    2001-01              1               6\n",
            "3    2001-02              1              11\n",
            "4    2001-02             23              10\n",
            "..       ...            ...             ...\n",
            "859  2024-11             27               7\n",
            "860  2024-11             45               7\n",
            "861  2024-12             17               9\n",
            "862  2024-12              7               7\n",
            "863  2024-12             13               6\n",
            "\n",
            "[864 rows x 3 columns]\n",
            "Mes con más carreras: Mes               2013-04\n",
            "Total_Carreras        197\n",
            "Name: 147, dtype: object\n",
            "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 3.2870 - mae: 1.4293\n",
            "Pérdida: 3.4210405349731445, MAE: 1.456627368927002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#14. Predicción de carreras futuras: Cambia el modelo de deep learning para predecir el número de\n",
        "carreras que un taxista podría realizar en 2025, usando los datos de los años anteriores."
      ],
      "metadata": {
        "id": "WX_6_Bc_bJaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "# Convertir la columna 'Fecha' a un tipo de dato de fecha\n",
        "df['Fecha_Carrera'] = pd.to_datetime(df['Fecha_Carrera'])\n",
        "df['Año'] = df['Fecha_Carrera'].dt.year\n",
        "\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "\n",
        "# Contar el número de carreras por taxista y por año\n",
        "carreras_por_taxista_año = df.groupby(['Año', 'Taxista_Label']).size().reset_index(name='Total_Carreras')\n",
        "\n",
        "# Crear un dataset que contenga sólo los años anteriores a 2025\n",
        "df_previos = carreras_por_taxista_año[carreras_por_taxista_año['Año'] < 2025]\n",
        "\n",
        "# Crear datos de entrada (Taxista_Label y Año) y datos de salida (Total_Carreras)\n",
        "X = df_previos[['Taxista_Label', 'Año']].values\n",
        "y = df_previos['Total_Carreras'].values\n",
        "\n",
        "# Dividir en conjunto de entrenamiento y prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(2,)),  # Dos entradas: Taxista_Label y Año\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compilar el modelo\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))\n",
        "\n",
        "# Crear los datos para predecir carreras en 2025 para cada taxista\n",
        "taxistas_unicos = df_previos['Taxista_Label'].unique()\n",
        "X_2025 = np.array([[taxista, 2025] for taxista in taxistas_unicos])\n",
        "\n",
        "# Predecir el número de carreras en 2025\n",
        "predicciones_2025 = model.predict(X_2025)\n",
        "predicciones_2025_df = pd.DataFrame({'Taxista_Label': taxistas_unicos, 'Pred_Carreras_2025': predicciones_2025.flatten()})\n",
        "print(\"Predicciones del número de carreras en 2025:\")\n",
        "print(predicciones_2025_df)\n",
        "\n",
        "# Mostrar el rendimiento del modelo en los datos de prueba\n",
        "loss, mae = model.evaluate(X_test, y_test)\n",
        "print(f'Pérdida: {loss}, MAE: {mae}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6kOadoHbnN2",
        "outputId": "1b474a39-9ed6-445d-f8e2-ffe7156308cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - loss: 18657.3223 - mae: 110.9446 - val_loss: 212.9847 - val_mae: 11.1198\n",
            "Epoch 2/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 595.9406 - mae: 20.4185 - val_loss: 170.3503 - val_mae: 10.4306\n",
            "Epoch 3/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 222.5029 - mae: 12.2626 - val_loss: 161.5711 - val_mae: 10.7108\n",
            "Epoch 4/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 178.9075 - mae: 10.9674 - val_loss: 173.4464 - val_mae: 11.1902\n",
            "Epoch 5/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 178.6944 - mae: 11.2422 - val_loss: 165.2425 - val_mae: 10.8748\n",
            "Epoch 6/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 180.7709 - mae: 11.3827 - val_loss: 159.5749 - val_mae: 10.5947\n",
            "Epoch 7/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 185.6199 - mae: 11.2938 - val_loss: 170.8335 - val_mae: 11.0820\n",
            "Epoch 8/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 177.9730 - mae: 11.0563 - val_loss: 158.5723 - val_mae: 10.4635\n",
            "Epoch 9/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 178.8969 - mae: 11.0265 - val_loss: 158.4653 - val_mae: 10.4956\n",
            "Epoch 10/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 189.0669 - mae: 11.4801 - val_loss: 158.3811 - val_mae: 10.4220\n",
            "Epoch 11/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 183.2165 - mae: 11.3199 - val_loss: 159.5968 - val_mae: 10.6113\n",
            "Epoch 12/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 179.0641 - mae: 11.1821 - val_loss: 157.9592 - val_mae: 10.4488\n",
            "Epoch 13/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 177.0938 - mae: 11.0650 - val_loss: 162.5065 - val_mae: 10.7417\n",
            "Epoch 14/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 180.5956 - mae: 11.2720 - val_loss: 157.9287 - val_mae: 10.3805\n",
            "Epoch 15/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 186.3223 - mae: 11.4889 - val_loss: 162.6084 - val_mae: 10.7406\n",
            "Epoch 16/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 177.6950 - mae: 11.0279 - val_loss: 160.6511 - val_mae: 10.6554\n",
            "Epoch 17/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 168.9145 - mae: 10.8267 - val_loss: 161.7378 - val_mae: 10.6982\n",
            "Epoch 18/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 176.0437 - mae: 11.0736 - val_loss: 180.5383 - val_mae: 11.3557\n",
            "Epoch 19/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 179.7386 - mae: 11.2671 - val_loss: 162.3765 - val_mae: 10.2581\n",
            "Epoch 20/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 185.3168 - mae: 11.1667 - val_loss: 195.5688 - val_mae: 11.8340\n",
            "Epoch 21/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 188.8598 - mae: 11.4977 - val_loss: 157.9960 - val_mae: 10.3052\n",
            "Epoch 22/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 165.3263 - mae: 10.7191 - val_loss: 173.4128 - val_mae: 11.0958\n",
            "Epoch 23/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 213.0727 - mae: 12.0267 - val_loss: 159.7784 - val_mae: 10.5951\n",
            "Epoch 24/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 170.3667 - mae: 10.8118 - val_loss: 168.0388 - val_mae: 10.9063\n",
            "Epoch 25/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 191.7067 - mae: 11.6571 - val_loss: 173.6965 - val_mae: 11.0917\n",
            "Epoch 26/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 189.3233 - mae: 11.4196 - val_loss: 160.5986 - val_mae: 10.6264\n",
            "Epoch 27/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 176.2192 - mae: 11.1581 - val_loss: 158.1562 - val_mae: 10.2659\n",
            "Epoch 28/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 170.0976 - mae: 10.8013 - val_loss: 235.6987 - val_mae: 12.9894\n",
            "Epoch 29/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 201.5049 - mae: 11.8310 - val_loss: 188.4292 - val_mae: 11.5762\n",
            "Epoch 30/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 178.9278 - mae: 11.0585 - val_loss: 175.3651 - val_mae: 11.1335\n",
            "Epoch 31/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 186.4675 - mae: 11.3959 - val_loss: 164.0985 - val_mae: 10.7466\n",
            "Epoch 32/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 185.7541 - mae: 11.3444 - val_loss: 160.2109 - val_mae: 10.5932\n",
            "Epoch 33/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 173.0146 - mae: 10.7777 - val_loss: 247.1248 - val_mae: 13.3110\n",
            "Epoch 34/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 222.0253 - mae: 12.0990 - val_loss: 160.2912 - val_mae: 10.5952\n",
            "Epoch 35/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 193.7909 - mae: 11.6347 - val_loss: 162.3111 - val_mae: 10.2080\n",
            "Epoch 36/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 187.8723 - mae: 11.2837 - val_loss: 172.8338 - val_mae: 11.0354\n",
            "Epoch 37/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 207.8004 - mae: 11.7942 - val_loss: 163.0840 - val_mae: 10.2074\n",
            "Epoch 38/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 191.4205 - mae: 11.3858 - val_loss: 164.0214 - val_mae: 10.2137\n",
            "Epoch 39/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 190.3257 - mae: 11.5006 - val_loss: 157.0000 - val_mae: 10.2972\n",
            "Epoch 40/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 236.4622 - mae: 12.5509 - val_loss: 169.5912 - val_mae: 10.9224\n",
            "Epoch 41/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 241.0052 - mae: 12.6759 - val_loss: 169.8385 - val_mae: 10.2961\n",
            "Epoch 42/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 193.3362 - mae: 11.3073 - val_loss: 187.0982 - val_mae: 10.6221\n",
            "Epoch 43/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 219.3282 - mae: 12.0907 - val_loss: 217.7093 - val_mae: 12.4215\n",
            "Epoch 44/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 203.0940 - mae: 11.7751 - val_loss: 278.3218 - val_mae: 14.1664\n",
            "Epoch 45/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 228.6267 - mae: 12.2417 - val_loss: 158.7891 - val_mae: 10.2213\n",
            "Epoch 46/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 187.1371 - mae: 11.3021 - val_loss: 187.4099 - val_mae: 10.6291\n",
            "Epoch 47/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 194.1665 - mae: 11.3588 - val_loss: 264.4878 - val_mae: 13.7797\n",
            "Epoch 48/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 217.5999 - mae: 12.0019 - val_loss: 158.8983 - val_mae: 10.2158\n",
            "Epoch 49/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 192.0498 - mae: 11.5181 - val_loss: 158.4846 - val_mae: 10.2247\n",
            "Epoch 50/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 183.0732 - mae: 11.1354 - val_loss: 198.3682 - val_mae: 11.8403\n",
            "Epoch 51/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 203.0522 - mae: 11.8054 - val_loss: 159.7406 - val_mae: 10.5539\n",
            "Epoch 52/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 174.9032 - mae: 11.1457 - val_loss: 188.5513 - val_mae: 11.5477\n",
            "Epoch 53/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 211.5376 - mae: 11.8675 - val_loss: 159.0173 - val_mae: 10.2083\n",
            "Epoch 54/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 194.1499 - mae: 11.5520 - val_loss: 159.8992 - val_mae: 10.5586\n",
            "Epoch 55/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 193.7073 - mae: 11.6079 - val_loss: 162.8421 - val_mae: 10.6782\n",
            "Epoch 56/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 191.9869 - mae: 11.2827 - val_loss: 158.8244 - val_mae: 10.2133\n",
            "Epoch 57/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 208.6811 - mae: 11.7483 - val_loss: 166.0031 - val_mae: 10.7924\n",
            "Epoch 58/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 201.0130 - mae: 11.5992 - val_loss: 159.1582 - val_mae: 10.2098\n",
            "Epoch 59/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 180.7722 - mae: 11.1043 - val_loss: 156.9374 - val_mae: 10.3505\n",
            "Epoch 60/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 210.8847 - mae: 11.6614 - val_loss: 194.1493 - val_mae: 11.7122\n",
            "Epoch 61/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 198.7710 - mae: 11.7579 - val_loss: 159.1287 - val_mae: 10.5201\n",
            "Epoch 62/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 183.1934 - mae: 11.2526 - val_loss: 174.3392 - val_mae: 11.0731\n",
            "Epoch 63/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 179.6984 - mae: 10.9490 - val_loss: 168.7969 - val_mae: 10.8878\n",
            "Epoch 64/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 242.1032 - mae: 12.7085 - val_loss: 160.2979 - val_mae: 10.1980\n",
            "Epoch 65/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 175.7640 - mae: 10.8872 - val_loss: 163.7478 - val_mae: 10.7122\n",
            "Epoch 66/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 203.6818 - mae: 11.7403 - val_loss: 169.0891 - val_mae: 10.8947\n",
            "Epoch 67/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 209.5811 - mae: 11.8594 - val_loss: 197.7657 - val_mae: 11.8212\n",
            "Epoch 68/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 185.8307 - mae: 11.2489 - val_loss: 172.5670 - val_mae: 10.3380\n",
            "Epoch 69/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 224.6773 - mae: 12.2420 - val_loss: 157.5676 - val_mae: 10.2558\n",
            "Epoch 70/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 203.0555 - mae: 11.5521 - val_loss: 172.1602 - val_mae: 11.0097\n",
            "Epoch 71/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 191.1285 - mae: 11.4846 - val_loss: 157.4892 - val_mae: 10.2561\n",
            "Epoch 72/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 194.5068 - mae: 11.5067 - val_loss: 159.5076 - val_mae: 10.5413\n",
            "Epoch 73/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 178.2478 - mae: 11.0572 - val_loss: 157.2885 - val_mae: 10.2672\n",
            "Epoch 74/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 173.7913 - mae: 10.7287 - val_loss: 177.2408 - val_mae: 11.1776\n",
            "Epoch 75/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 185.0322 - mae: 11.3161 - val_loss: 160.7496 - val_mae: 10.5989\n",
            "Epoch 76/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 189.0078 - mae: 11.2885 - val_loss: 196.1482 - val_mae: 11.7730\n",
            "Epoch 77/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 187.6954 - mae: 11.2967 - val_loss: 162.9425 - val_mae: 10.2029\n",
            "Epoch 78/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 202.8537 - mae: 11.4406 - val_loss: 191.0500 - val_mae: 11.6259\n",
            "Epoch 79/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 210.9215 - mae: 12.1260 - val_loss: 166.0375 - val_mae: 10.2334\n",
            "Epoch 80/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 204.5064 - mae: 11.6129 - val_loss: 156.9296 - val_mae: 10.3467\n",
            "Epoch 81/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 181.8726 - mae: 11.1543 - val_loss: 157.3555 - val_mae: 10.4017\n",
            "Epoch 82/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 187.5755 - mae: 11.2476 - val_loss: 158.0986 - val_mae: 10.2304\n",
            "Epoch 83/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 182.5233 - mae: 11.0433 - val_loss: 345.5973 - val_mae: 15.9356\n",
            "Epoch 84/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 243.6553 - mae: 12.6194 - val_loss: 277.9575 - val_mae: 14.1492\n",
            "Epoch 85/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 226.6796 - mae: 12.1373 - val_loss: 229.4951 - val_mae: 11.6037\n",
            "Epoch 86/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 205.8440 - mae: 11.5368 - val_loss: 216.9524 - val_mae: 12.3942\n",
            "Epoch 87/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 203.6597 - mae: 11.8202 - val_loss: 216.6512 - val_mae: 12.3832\n",
            "Epoch 88/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 217.4317 - mae: 12.0975 - val_loss: 159.1388 - val_mae: 10.5210\n",
            "Epoch 89/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 186.1316 - mae: 11.2973 - val_loss: 212.4532 - val_mae: 12.2648\n",
            "Epoch 90/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 181.4571 - mae: 11.1106 - val_loss: 157.0059 - val_mae: 10.3649\n",
            "Epoch 91/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 189.8360 - mae: 11.2198 - val_loss: 158.2899 - val_mae: 10.2293\n",
            "Epoch 92/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 182.3084 - mae: 11.0439 - val_loss: 159.8919 - val_mae: 10.5601\n",
            "Epoch 93/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 179.7572 - mae: 11.0733 - val_loss: 161.3956 - val_mae: 10.1965\n",
            "Epoch 94/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 208.8421 - mae: 11.8079 - val_loss: 165.7564 - val_mae: 10.2308\n",
            "Epoch 95/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 180.7513 - mae: 11.1461 - val_loss: 214.6028 - val_mae: 11.2410\n",
            "Epoch 96/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 182.8537 - mae: 10.9725 - val_loss: 225.0033 - val_mae: 11.4896\n",
            "Epoch 97/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 227.4264 - mae: 12.1525 - val_loss: 157.3932 - val_mae: 10.2596\n",
            "Epoch 98/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 200.2511 - mae: 11.7579 - val_loss: 180.4990 - val_mae: 10.4863\n",
            "Epoch 99/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 195.9374 - mae: 11.4109 - val_loss: 163.4249 - val_mae: 10.6980\n",
            "Epoch 100/100\n",
            "\u001b[1m30/30\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 197.8283 - mae: 11.5896 - val_loss: 158.6294 - val_mae: 10.2175\n",
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "Predicciones del número de carreras en 2025:\n",
            "    Taxista_Label  Pred_Carreras_2025\n",
            "0               0           41.886223\n",
            "1               1           41.641472\n",
            "2               2           41.396721\n",
            "3               3           41.151962\n",
            "4               4           40.907219\n",
            "5               5           40.662476\n",
            "6               6           40.417686\n",
            "7               7           40.172905\n",
            "8               8           39.928146\n",
            "9               9           39.683395\n",
            "10             10           39.438629\n",
            "11             11           39.193878\n",
            "12             12           38.949089\n",
            "13             13           38.704308\n",
            "14             14           38.459541\n",
            "15             15           38.214867\n",
            "16             16           37.969948\n",
            "17             17           37.725296\n",
            "18             18           37.480476\n",
            "19             19           37.235657\n",
            "20             20           36.990891\n",
            "21             21           36.746147\n",
            "22             22           36.501373\n",
            "23             23           36.256592\n",
            "24             24           36.011848\n",
            "25             25           35.767143\n",
            "26             26           35.522377\n",
            "27             27           35.277550\n",
            "28             28           35.032722\n",
            "29             29           34.787994\n",
            "30             30           34.543304\n",
            "31             31           34.298553\n",
            "32             32           34.053726\n",
            "33             33           33.808968\n",
            "34             34           33.564163\n",
            "35             35           33.319420\n",
            "36             36           33.074745\n",
            "37             37           32.829933\n",
            "38             38           32.585045\n",
            "39             39           32.340431\n",
            "40             40           32.102119\n",
            "41             41           31.863785\n",
            "42             42           31.625496\n",
            "43             43           31.387199\n",
            "44             44           31.148941\n",
            "45             45           30.910553\n",
            "46             46           30.672325\n",
            "47             47           30.433998\n",
            "48             48           30.195686\n",
            "49             49           29.957329\n",
            "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 150.9306 - mae: 10.0070 \n",
            "Pérdida: 158.62939453125, MAE: 10.21751880645752\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#15. Taxistas con más carreras nocturnas: Añade una columna que clasifique las carreras como\n",
        "\"diurnas\" o \"nocturnas\" según la hora del día. Luego, encuentra el taxista que ha realizado más\n",
        "carreras nocturnas."
      ],
      "metadata": {
        "id": "apnIYw__dhe0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Cargar los datos\n",
        "df = pd.read_csv('radio_taxi_carreras.csv')\n",
        "df['Fecha_Carrera'] = pd.to_datetime(df['Fecha_Carrera'])\n",
        "#  Extraer la hora de la columna 'Fecha_Carrera' si es necesario.\n",
        "df['Hora_Carrera'] = df['Fecha_Carrera'].dt.time\n",
        "\n",
        "# Convertimos el código del taxista a un valor numérico usando LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df['Taxista_Label'] = label_encoder.fit_transform(df['Codigo_Taxista'])\n",
        "\n",
        "# Clasificar las carreras como \"diurnas\" o \"nocturnas\" según la hora\n",
        "def clasificar_turno(hora):\n",
        "    if hora >= pd.to_datetime('20:00').time() or hora < pd.to_datetime('06:00').time():\n",
        "        return 'nocturna'\n",
        "    else:\n",
        "        return 'diurna'\n",
        "\n",
        "df['Turno'] = df['Hora_Carrera'].apply(clasificar_turno)\n",
        "\n",
        "# Contar el número de carreras por taxista y por turno\n",
        "carreras_nocturnas_por_taxista = df[df['Turno'] == 'nocturna'].groupby('Taxista_Label').size().reset_index(name='Total_Nocturnas')\n",
        "\n",
        "# Encontrar el taxista con más carreras nocturnas\n",
        "taxista_mas_nocturnas = carreras_nocturnas_por_taxista.loc[carreras_nocturnas_por_taxista['Total_Nocturnas'].idxmax()]\n",
        "print(\"Taxista con más carreras nocturnas:\")\n",
        "print(taxista_mas_nocturnas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3HgWHp_eYUd",
        "outputId": "68e473ce-2bdd-4cef-d1fc-5fe892e4692d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taxista con más carreras nocturnas:\n",
            "Taxista_Label        17\n",
            "Total_Nocturnas    1500\n",
            "Name: 17, dtype: int64\n"
          ]
        }
      ]
    }
  ]
}